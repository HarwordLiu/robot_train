#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†è´¨é‡åˆ†æè„šæœ¬

ç”¨äºåˆ†æä»»åŠ¡1æ•°æ®é›†çš„è´¨é‡ï¼Œå¸®åŠ©è¯Šæ–­è®­ç»ƒé—®é¢˜ï¼š
1. ç‰©å“ä½ç½®åˆ†å¸ƒåˆ†æ
2. åŠ¨ä½œåºåˆ—ç»Ÿè®¡
3. è½¨è¿¹å¯è§†åŒ–
4. æˆåŠŸç‡ä¼°è®¡

ä½¿ç”¨æ–¹æ³•:
    python analyze_dataset_quality.py --data_root /root/robot/data/task-1/1-2000/lerobot/ --episodes 0-50
"""

import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from tqdm import tqdm

# å¯¼å…¥lerobotæ•°æ®é›†åŠ è½½å·¥å…·
from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata


def analyze_action_distribution(dataset, num_episodes=50):
    """åˆ†æåŠ¨ä½œåˆ†å¸ƒ"""
    print("\n" + "="*70)
    print("ğŸ“Š åŠ¨ä½œåˆ†å¸ƒåˆ†æ")
    print("="*70)

    actions = []
    for i in tqdm(range(min(num_episodes * 100, len(dataset))), desc="åŠ è½½åŠ¨ä½œæ•°æ®"):
        try:
            sample = dataset[i]
            action = sample['action'].numpy()
            actions.append(action)
        except:
            continue

    actions = np.array(actions)

    print(f"\nåŠ¨ä½œç»´åº¦: {actions.shape}")
    print(f"\nå„ç»´åº¦ç»Ÿè®¡:")
    print(f"{'ç»´åº¦':<6} {'å‡å€¼':<12} {'æ ‡å‡†å·®':<12} {'æœ€å°å€¼':<12} {'æœ€å¤§å€¼':<12}")
    print("-" * 60)

    for i in range(min(16, actions.shape[1])):  # åªæ˜¾ç¤ºå‰16ç»´ï¼ˆKuavoçš„å®é™…ç»´åº¦ï¼‰
        mean = np.mean(actions[:, i])
        std = np.std(actions[:, i])
        min_val = np.min(actions[:, i])
        max_val = np.max(actions[:, i])
        print(f"{i:<6} {mean:>11.4f} {std:>11.4f} {min_val:>11.4f} {max_val:>11.4f}")

    return actions


def analyze_state_distribution(dataset, num_episodes=50):
    """åˆ†æçŠ¶æ€åˆ†å¸ƒï¼ˆåŒ…æ‹¬æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼‰"""
    print("\n" + "="*70)
    print("ğŸ¤– çŠ¶æ€åˆ†å¸ƒåˆ†æ")
    print("="*70)

    states = []
    for i in tqdm(range(min(num_episodes * 100, len(dataset))), desc="åŠ è½½çŠ¶æ€æ•°æ®"):
        try:
            sample = dataset[i]
            state = sample['observation.state'].numpy()
            states.append(state)
        except:
            continue

    states = np.array(states)

    print(f"\nçŠ¶æ€ç»´åº¦: {states.shape}")
    print(f"\nå„ç»´åº¦ç»Ÿè®¡:")
    print(f"{'ç»´åº¦':<6} {'å‡å€¼':<12} {'æ ‡å‡†å·®':<12} {'æœ€å°å€¼':<12} {'æœ€å¤§å€¼':<12}")
    print("-" * 60)

    for i in range(min(16, states.shape[1])):  # åªæ˜¾ç¤ºå‰16ç»´
        mean = np.mean(states[:, i])
        std = np.std(states[:, i])
        min_val = np.min(states[:, i])
        max_val = np.max(states[:, i])
        print(f"{i:<6} {mean:>11.4f} {std:>11.4f} {min_val:>11.4f} {max_val:>11.4f}")

    # é‡ç‚¹åˆ†ææœ«ç«¯ä½ç½®ï¼ˆå‡è®¾å‰3ç»´æ˜¯xyzä½ç½®ï¼‰
    if states.shape[1] >= 3:
        print("\nğŸ¯ æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®åˆ†æï¼ˆå‡è®¾å‰3ç»´ä¸ºxyzï¼‰:")
        print(
            f"Xè½´èŒƒå›´: [{np.min(states[:, 0]):.4f}, {np.max(states[:, 0]):.4f}], å‡å€¼: {np.mean(states[:, 0]):.4f}")
        print(
            f"Yè½´èŒƒå›´: [{np.min(states[:, 1]):.4f}, {np.max(states[:, 1]):.4f}], å‡å€¼: {np.mean(states[:, 1]):.4f}")
        print(
            f"Zè½´èŒƒå›´: [{np.min(states[:, 2]):.4f}, {np.max(states[:, 2]):.4f}], å‡å€¼: {np.mean(states[:, 2]):.4f}")

        # æ£€æŸ¥ä½ç½®åˆ†å¸ƒæ˜¯å¦é›†ä¸­
        x_std = np.std(states[:, 0])
        y_std = np.std(states[:, 1])
        z_std = np.std(states[:, 2])

        print(f"\nä½ç½®æ ‡å‡†å·®:")
        print(f"X: {x_std:.4f}, Y: {y_std:.4f}, Z: {z_std:.4f}")

        if x_std < 0.05 or y_std < 0.05:
            print("âš ï¸  è­¦å‘Š: ä½ç½®åˆ†å¸ƒè¿‡äºé›†ä¸­ï¼Œå¯èƒ½å¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ï¼")
            print("   å»ºè®®: å¢åŠ æ›´å¤šä¸åŒä½ç½®çš„æ¼”ç¤ºæ•°æ®")

    return states


def analyze_episode_statistics(dataset, metadata):
    """åˆ†æepisodeç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*70)
    print("ğŸ“ˆ Episodeç»Ÿè®¡åˆ†æ")
    print("="*70)

    # è·å–episodeä¿¡æ¯
    episode_data_index = dataset.episode_data_index
    num_episodes = len(episode_data_index)

    print(f"\nEpisodeæ€»æ•°: {num_episodes}")
    print(f"æ€»å¸§æ•°: {len(dataset)}")

    # åˆ†ææ¯ä¸ªepisodeçš„é•¿åº¦
    episode_lengths = []
    for episode_id in range(num_episodes):
        episode_indices = episode_data_index[episode_id]
        episode_length = len(episode_indices)
        episode_lengths.append(episode_length)

    episode_lengths = np.array(episode_lengths)

    print(f"\nEpisodeé•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f} å¸§")
    print(f"  æ ‡å‡†å·®: {np.std(episode_lengths):.1f} å¸§")
    print(f"  æœ€çŸ­: {np.min(episode_lengths)} å¸§")
    print(f"  æœ€é•¿: {np.max(episode_lengths)} å¸§")

    # åˆ†æepisodeé•¿åº¦åˆ†å¸ƒ
    short_episodes = np.sum(episode_lengths < 50)
    medium_episodes = np.sum((episode_lengths >= 50) & (episode_lengths < 100))
    long_episodes = np.sum(episode_lengths >= 100)

    print(f"\nEpisodeé•¿åº¦åˆ†å¸ƒ:")
    print(
        f"  çŸ­(<50å¸§): {short_episodes} ({short_episodes/num_episodes*100:.1f}%)")
    print(
        f"  ä¸­(50-100å¸§): {medium_episodes} ({medium_episodes/num_episodes*100:.1f}%)")
    print(
        f"  é•¿(>100å¸§): {long_episodes} ({long_episodes/num_episodes*100:.1f}%)")

    if short_episodes > num_episodes * 0.3:
        print("âš ï¸  è­¦å‘Š: çŸ­episodeå æ¯”è¿‡é«˜ï¼Œå¯èƒ½åŒ…å«å¤±è´¥çš„æ¼”ç¤º")
        print("   å»ºè®®: æ£€æŸ¥çŸ­episodeçš„è´¨é‡ï¼Œè€ƒè™‘è¿‡æ»¤æ‰å¤±è´¥çš„æ¼”ç¤º")

    return episode_lengths


def visualize_trajectory(dataset, episode_id=0, save_path=None):
    """å¯è§†åŒ–å•ä¸ªepisodeçš„è½¨è¿¹"""
    print(f"\nğŸ“ å¯è§†åŒ–Episode {episode_id}çš„è½¨è¿¹...")

    # è·å–episodeæ•°æ®
    episode_indices = dataset.episode_data_index[episode_id]

    states = []
    actions = []
    for idx in episode_indices:
        sample = dataset[idx]
        states.append(sample['observation.state'].numpy())
        actions.append(sample['action'].numpy())

    states = np.array(states)
    actions = np.array(actions)

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'Episode {episode_id} è½¨è¿¹åˆ†æ', fontsize=16)

    # 1. XYå¹³é¢è½¨è¿¹ï¼ˆå‡è®¾å‰2ç»´æ˜¯XYä½ç½®ï¼‰
    ax = axes[0, 0]
    if states.shape[1] >= 2:
        ax.plot(states[:, 0], states[:, 1], 'b-', linewidth=2, label='è½¨è¿¹')
        ax.scatter(states[0, 0], states[0, 1], c='green',
                   s=100, marker='o', label='èµ·ç‚¹')
        ax.scatter(states[-1, 0], states[-1, 1], c='red',
                   s=100, marker='x', label='ç»ˆç‚¹')
        ax.set_xlabel('Xä½ç½®')
        ax.set_ylabel('Yä½ç½®')
        ax.set_title('XYå¹³é¢è½¨è¿¹')
        ax.legend()
        ax.grid(True)

    # 2. Zé«˜åº¦éšæ—¶é—´å˜åŒ–
    ax = axes[0, 1]
    if states.shape[1] >= 3:
        ax.plot(states[:, 2], 'b-', linewidth=2)
        ax.set_xlabel('æ—¶é—´æ­¥')
        ax.set_ylabel('Zä½ç½®ï¼ˆé«˜åº¦ï¼‰')
        ax.set_title('Zé«˜åº¦å˜åŒ–')
        ax.grid(True)

    # 3. åŠ¨ä½œå¹…åº¦ï¼ˆå‰3ç»´ï¼‰
    ax = axes[1, 0]
    if actions.shape[1] >= 3:
        ax.plot(actions[:, 0], label='åŠ¨ä½œç»´åº¦0', alpha=0.7)
        ax.plot(actions[:, 1], label='åŠ¨ä½œç»´åº¦1', alpha=0.7)
        ax.plot(actions[:, 2], label='åŠ¨ä½œç»´åº¦2', alpha=0.7)
        ax.set_xlabel('æ—¶é—´æ­¥')
        ax.set_ylabel('åŠ¨ä½œå€¼')
        ax.set_title('åŠ¨ä½œåºåˆ—ï¼ˆå‰3ç»´ï¼‰')
        ax.legend()
        ax.grid(True)

    # 4. çŠ¶æ€å˜åŒ–ç‡
    ax = axes[1, 1]
    if states.shape[1] >= 3:
        velocity = np.diff(states[:, :3], axis=0)
        speed = np.linalg.norm(velocity, axis=1)
        ax.plot(speed, 'r-', linewidth=2)
        ax.set_xlabel('æ—¶é—´æ­¥')
        ax.set_ylabel('é€Ÿåº¦')
        ax.set_title('æœ«ç«¯æ‰§è¡Œå™¨é€Ÿåº¦')
        ax.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… è½¨è¿¹å›¾å·²ä¿å­˜åˆ°: {save_path}")
    else:
        plt.savefig(f'episode_{episode_id}_trajectory.png',
                    dpi=150, bbox_inches='tight')
        print(f"âœ… è½¨è¿¹å›¾å·²ä¿å­˜åˆ°: episode_{episode_id}_trajectory.png")

    plt.close()


def detect_potential_issues(states, actions, episode_lengths):
    """æ£€æµ‹æ½œåœ¨é—®é¢˜"""
    print("\n" + "="*70)
    print("âš ï¸  æ½œåœ¨é—®é¢˜æ£€æµ‹")
    print("="*70)

    issues = []

    # 1. æ£€æŸ¥ä½ç½®åˆ†å¸ƒ
    if states.shape[1] >= 3:
        x_std = np.std(states[:, 0])
        y_std = np.std(states[:, 1])
        z_std = np.std(states[:, 2])

        if x_std < 0.05 or y_std < 0.05:
            issues.append("âŒ é—®é¢˜1: ç‰©å“ä½ç½®åˆ†å¸ƒè¿‡äºé›†ä¸­")
            print("\nâŒ é—®é¢˜1: ç‰©å“ä½ç½®åˆ†å¸ƒè¿‡äºé›†ä¸­")
            print(f"   XYä½ç½®æ ‡å‡†å·®: X={x_std:.4f}, Y={y_std:.4f}")
            print("   è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•æ³›åŒ–åˆ°ä¸åŒä½ç½®")
            print("   å»ºè®®: æ”¶é›†æ›´å¤šä¸åŒä½ç½®çš„æ¼”ç¤ºæ•°æ®")
        else:
            print("\nâœ… ç‰©å“ä½ç½®åˆ†å¸ƒè‰¯å¥½")

    # 2. æ£€æŸ¥åŠ¨ä½œå¹…åº¦
    action_std = np.std(actions, axis=0)
    if np.any(action_std < 0.01):
        issues.append("âŒ é—®é¢˜2: éƒ¨åˆ†åŠ¨ä½œç»´åº¦å‡ ä¹ä¸å˜")
        print("\nâŒ é—®é¢˜2: éƒ¨åˆ†åŠ¨ä½œç»´åº¦å‡ ä¹ä¸å˜")
        print(f"   é™æ­¢ç»´åº¦: {np.where(action_std < 0.01)[0]}")
        print("   è¿™å¯èƒ½è¡¨ç¤ºæ•°æ®ä¸­ç¼ºä¹æŸäº›åŠ¨ä½œ")
    else:
        print("\nâœ… åŠ¨ä½œç»´åº¦åˆ†å¸ƒè‰¯å¥½")

    # 3. æ£€æŸ¥episodeé•¿åº¦
    if np.std(episode_lengths) > np.mean(episode_lengths) * 0.5:
        issues.append("âš ï¸  é—®é¢˜3: Episodeé•¿åº¦å·®å¼‚è¾ƒå¤§")
        print("\nâš ï¸  é—®é¢˜3: Episodeé•¿åº¦å·®å¼‚è¾ƒå¤§")
        print(
            f"   æ ‡å‡†å·®({np.std(episode_lengths):.1f}) > å‡å€¼({np.mean(episode_lengths):.1f}) * 0.5")
        print("   è¿™å¯èƒ½è¡¨ç¤ºæ•°æ®è´¨é‡ä¸ä¸€è‡´æˆ–åŒ…å«å¤±è´¥çš„æ¼”ç¤º")
        print("   å»ºè®®: æ£€æŸ¥å¼‚å¸¸çŸ­æˆ–é•¿çš„episodes")
    else:
        print("\nâœ… Episodeé•¿åº¦åˆ†å¸ƒä¸€è‡´")

    # 4. æ£€æŸ¥åŠ¨ä½œå¹³æ»‘æ€§
    action_diff = np.diff(actions, axis=0)
    action_jerk = np.mean(np.abs(action_diff), axis=0)
    if np.any(action_jerk > 0.5):
        issues.append("âš ï¸  é—®é¢˜4: åŠ¨ä½œåºåˆ—æœ‰è¾ƒå¤§è·³å˜")
        print("\nâš ï¸  é—®é¢˜4: åŠ¨ä½œåºåˆ—æœ‰è¾ƒå¤§è·³å˜")
        print(f"   é«˜è·³å˜ç»´åº¦: {np.where(action_jerk > 0.5)[0]}")
        print("   è¿™å¯èƒ½å½±å“ç­–ç•¥å­¦ä¹ çš„å¹³æ»‘æ€§")
    else:
        print("\nâœ… åŠ¨ä½œåºåˆ—å¹³æ»‘")

    # æ€»ç»“
    print("\n" + "="*70)
    if len(issues) == 0:
        print("âœ… æ•°æ®é›†è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")
    else:
        print(f"âš ï¸  å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
        for i, issue in enumerate(issues, 1):
            print(f"{i}. {issue}")
    print("="*70)

    return issues


def generate_report(dataset, metadata, output_dir):
    """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*70)
    print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    print("="*70)

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1. åŠ¨ä½œåˆ†å¸ƒåˆ†æ
    actions = analyze_action_distribution(dataset, num_episodes=50)

    # 2. çŠ¶æ€åˆ†å¸ƒåˆ†æ
    states = analyze_state_distribution(dataset, num_episodes=50)

    # 3. Episodeç»Ÿè®¡
    episode_lengths = analyze_episode_statistics(dataset, metadata)

    # 4. å¯è§†åŒ–å‡ ä¸ªå…¸å‹è½¨è¿¹
    print("\nğŸ“Š å¯è§†åŒ–è½¨è¿¹...")
    num_episodes_to_viz = min(5, len(dataset.episode_data_index))
    for ep_id in range(num_episodes_to_viz):
        visualize_trajectory(
            dataset,
            episode_id=ep_id,
            save_path=output_dir / f"trajectory_episode_{ep_id}.png"
        )

    # 5. é—®é¢˜æ£€æµ‹
    issues = detect_potential_issues(states, actions, episode_lengths)

    # 6. ä¿å­˜æ•°å€¼ç»Ÿè®¡
    report = {
        "dataset_info": {
            "total_episodes": len(dataset.episode_data_index),
            "total_frames": len(dataset),
            "fps": metadata.fps,
            "state_dim": states.shape[1],
            "action_dim": actions.shape[1],
        },
        "action_statistics": {
            "mean": actions.mean(axis=0).tolist(),
            "std": actions.std(axis=0).tolist(),
            "min": actions.min(axis=0).tolist(),
            "max": actions.max(axis=0).tolist(),
        },
        "state_statistics": {
            "mean": states.mean(axis=0).tolist(),
            "std": states.std(axis=0).tolist(),
            "min": states.min(axis=0).tolist(),
            "max": states.max(axis=0).tolist(),
        },
        "episode_statistics": {
            "mean_length": float(episode_lengths.mean()),
            "std_length": float(episode_lengths.std()),
            "min_length": int(episode_lengths.min()),
            "max_length": int(episode_lengths.max()),
        },
        "detected_issues": issues,
    }

    report_file = output_dir / "dataset_analysis_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")

    return report


def main():
    parser = argparse.ArgumentParser(description='åˆ†æLeRobotæ•°æ®é›†è´¨é‡')
    parser.add_argument('--data_root', type=str,
                        default='/root/robot/data/task-1/1-2000/lerobot/',
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--repo_id', type=str,
                        default='lerobot/task1_moving_grasp',
                        help='æ•°æ®é›†repo ID')
    parser.add_argument('--episodes', type=str,
                        default='0-199',
                        help='è¦åˆ†æçš„episodeèŒƒå›´ï¼Œæ ¼å¼: 0-199')
    parser.add_argument('--output_dir', type=str,
                        default='./dataset_analysis',
                        help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # è§£æepisodeèŒƒå›´
    episode_start, episode_end = map(int, args.episodes.split('-'))
    episodes = list(range(episode_start, episode_end + 1))

    print("="*70)
    print("ğŸ” ä»»åŠ¡1æ•°æ®é›†è´¨é‡åˆ†æ")
    print("="*70)
    print(f"æ•°æ®é›†è·¯å¾„: {args.data_root}")
    print(f"Repo ID: {args.repo_id}")
    print(f"åˆ†æEpisodes: {episode_start} - {episode_end}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*70)

    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    metadata = LeRobotDatasetMetadata(args.repo_id, root=args.data_root)
    dataset = LeRobotDataset(
        args.repo_id,
        root=args.data_root,
        episodes=episodes[:50]  # åªåˆ†æå‰50ä¸ªepisodesï¼Œé¿å…å¤ªæ…¢
    )
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} å¸§")

    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report = generate_report(dataset, metadata, args.output_dir)

    print("\n" + "="*70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*70)
    print(f"\nè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•è·å–è¯¦ç»†æŠ¥å‘Š: {args.output_dir}")
    print("\nå»ºè®®:")
    print("1. æŸ¥çœ‹ dataset_analysis_report.json è·å–æ•°å€¼ç»Ÿè®¡")
    print("2. æŸ¥çœ‹ trajectory_episode_*.png äº†è§£è½¨è¿¹æ¨¡å¼")
    print("3. æ ¹æ®æ£€æµ‹åˆ°çš„é—®é¢˜ä¼˜åŒ–æ•°æ®æ”¶é›†ç­–ç•¥")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
