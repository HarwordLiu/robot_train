# ==================== SmolVLA顺序多任务训练基础配置 ====================
# 这是所有4个任务共享的基础配置
# 任务特定配置在 configs/policy/tasks/ 目录下

defaults:
  - _self_

# Hydra配置
hydra:
  run:
    dir: ./outputs/train_hydra_save/smolvla_sequential/${now:%Y%m%d_%H%M%S}
  sweep:
    dir: ./outputs/train_hydra_save/smolvla_sequential/multirun/${now:%Y%m%d_%H%M%S}
    subdir: ${hydra:job.override_dirname}

timestamp: ${now:%Y%m%d_%H%M%S}
method: 'smolvla_sequential'

# 任务配置字段（Hydra会从task config中读取）
# 格式: task1_moving_grasp, task2_weighing, task3_placement, task4_sorting
task: 'task1_moving_grasp' # 默认任务1，会被任务配置覆盖

# ==================== SmolVLA策略配置 ====================
policy_name: smolvla

policy:
  _target_: kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper

  # VLM Backbone配置
  vlm_model_name: 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct' # 使用镜像源下载
  load_vlm_weights: True # 任务1从HuggingFace预训练开始

  # 冻结策略（防止灾难性遗忘的关键）
  freeze_vision_encoder: True # 冻结SigLIP视觉编码器
  train_expert_only: True # 只训练Action Expert
  train_state_proj: True # 训练state投影层

  # 观测配置
  n_obs_steps: 1 # 使用当前帧（SmolVLA标准）

  # 动作空间配置（Kuavo双臂16关节机器人）
  # 注意：为了使用SmolVLA预训练权重，state/action维度必须与预训练模型一致（32维）
  # Kuavo的16维数据会在数据加载时自动填充到32维（后16维填0）
  max_state_dim: 32 # 使用预训练模型的32维状态空间
  max_action_dim: 32 # 使用预训练模型的32维动作空间
  chunk_size: 50 # 动作序列长度（Flow Matching生成50步）
  n_action_steps: 8 # 每次执行8步动作（与Diffusion策略一致）

  # 图像预处理配置
  resize_imgs_with_padding: [512, 512] # SmolVLA标准输入尺寸
  empty_cameras: 0 # 不使用空相机占位

  # Flow Matching配置（SmolVLA使用Flow Matching而非Diffusion）
  num_steps: 10 # 推理时的去噪步数

  # 归一化配置
  normalization_mapping:
    VISUAL:
      _target_: lerobot.configs.types.NormalizationMode
      value: IDENTITY # SmolVLA内部处理图像归一化
    STATE:
      _target_: lerobot.configs.types.NormalizationMode
      value: MEAN_STD
    ACTION:
      _target_: lerobot.configs.types.NormalizationMode
      value: MEAN_STD

  # 优化器基础配置（各任务会override学习率）
  # 针对batch_size=64优化的参数
  optimizer_betas: [0.9, 0.999] # beta2=0.999对较大batch更稳定
  optimizer_eps: 1.0e-08
  optimizer_weight_decay: 5.0e-7 # 适度降低正则化，避免欠拟合
  optimizer_grad_clip_norm: 1.0 # VLM embedding空间大，需要严格梯度控制

  # 学习率调度器配置（各任务会override）
  scheduler_warmup_steps: 1500 # VLM+Action Expert异构架构需要更长warmup
  scheduler_decay_steps: 25000 # 充分的cosine decay保证收敛
  scheduler_decay_lr: 1e-6 # 最终学习率衰减到很小

# ==================== 训练基础配置 ====================
training:
  output_directory: 'outputs/train/${task}/${method}' # 与其他策略一致的路径格式
  device: 'cuda'
  seed: 42

  # 动作维度适配配置
  target_action_dim: 32 # 目标动作维度（与预训练模型一致）

  # 数据加载（针对batch_size=64优化）
  batch_size: 64
  num_workers: 16 # 降低worker数量，避免CPU资源竞争
  drop_last: True
  prefetch_factor: 2 # 增加预取，提高GPU利用率
  persistent_workers: True # 保持worker进程，减少重启开销

  # 日志和保存
  save_freq_epoch: 3 # 更频繁保存checkpoint
  log_freq: 50 # 减少日志IO开销，每50步记录一次

  # 多任务验证配置（防遗忘的关键）
  validate_all_previous_tasks: True
  validation_freq_epoch: 2 # 每2个epoch验证所有之前的任务
  validation_episodes: 20 # 更多验证episodes，评估更准确

# ==================== 顺序训练策略配置 ====================
sequential:
  # 是否使用Replay Buffer（防遗忘的核心技术）
  use_replay_buffer: True
  replay_strategy: 'proportional' # 比例混合策略

  # Stage 2: 训练任务2时的数据混合比例
  stage2_replay:
    task1: 0.2 # 20% 任务1数据
    task2: 0.8 # 80% 任务2数据

  # Stage 3: 训练任务3时的数据混合比例
  stage3_replay:
    task1: 0.1 # 10% 任务1数据
    task2: 0.2 # 20% 任务2数据
    task3: 0.7 # 70% 任务3数据

  # Stage 4: 训练任务4时的数据混合比例（最终多任务模型）
  stage4_replay:
    task1: 0.1 # 10% 任务1数据
    task2: 0.1 # 10% 任务2数据
    task3: 0.2 # 20% 任务3数据
    task4: 0.6 # 60% 任务4数据
