# ==================== SmolVLA顺序多任务训练基础配置 ====================
# 这是所有4个任务共享的基础配置
# 任务特定配置在 configs/policy/tasks/ 目录下

defaults:
  - _self_

# Hydra配置
hydra:
  run:
    dir: ./outputs/train_hydra_save/smolvla_sequential/${now:%Y%m%d_%H%M%S}
  sweep:
    dir: ./outputs/train_hydra_save/smolvla_sequential/multirun/${now:%Y%m%d_%H%M%S}
    subdir: ${hydra:job.override_dirname}

timestamp: ${now:%Y%m%d_%H%M%S}
method: 'smolvla_sequential'

# ==================== HuggingFace Configuration ====================
# Set HuggingFace download mirror (optional)
# If not set, uses default HuggingFace Hub (https://huggingface.co)
# Common mirrors:
#   - China mirror: https://hf-mirror.com
#   - Leave commented out or set to null to use default
hf_endpoint: 'https://hf-mirror.com' # Set to null to use default HuggingFace Hub

# 任务配置字段（Hydra会从task config中读取）
# 格式: task1_moving_grasp, task2_weighing, task3_placement, task4_sorting
task: 'task1_moving_grasp' # 默认任务1，会被任务配置覆盖

# ==================== SmolVLA策略配置 ====================
policy_name: smolvla

policy:
  _target_: kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper

  # VLM Backbone配置
  vlm_model_name: 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct' # 使用镜像源下载
  load_vlm_weights: True # 任务1从HuggingFace预训练开始

  # 冻结策略（防止灾难性遗忘的关键）
  freeze_vision_encoder: True # 不完全冻结视觉编码器
  unfreeze_vision_layers: 0 # 不解冻视觉编码器
  train_expert_only: False # 训练所有模块（单任务场景）
  train_state_proj: True # 训练state投影层

  # 动作空间配置（Kuavo双臂16关节机器人）
  # 注意：为了使用SmolVLA预训练权重，state/action维度必须与预训练模型一致（32维）
  # Kuavo的16维数据会在数据加载时自动填充到32维（后16维填0）
  max_state_dim: 32 # 使用预训练模型的32维状态空间
  max_action_dim: 32 # 使用预训练模型的32维动作空间

  # 图像预处理配置
  resize_imgs_with_padding: [512, 512] # SmolVLA标准输入尺寸
  empty_cameras: 0 # 不使用空相机占位

  # 观测配置
  n_obs_steps: 1
  # Flow Matching配置
  num_steps: 10 # 采样精度
  chunk_size: 50 # chunk的步数
  n_action_steps: 40 # 重新观测和规划的步数

  # 归一化配置
  normalization_mapping:
    VISUAL:
      _target_: lerobot.configs.types.NormalizationMode
      value: IDENTITY # SmolVLA内部处理图像归一化
    STATE:
      _target_: lerobot.configs.types.NormalizationMode
      value: MEAN_STD
    ACTION:
      _target_: lerobot.configs.types.NormalizationMode
      value: MEAN_STD

  optimizer_betas: [0.9, 0.95]
  optimizer_eps: 1.0e-08
  optimizer_weight_decay: 1.0e-10
  optimizer_grad_clip_norm: 10

  # 学习率调度器配置（各任务会override）
  scheduler_warmup_steps: 1000 # SmolVLA默认值
  scheduler_decay_steps: 30000 # SmolVLA默认值
  scheduler_decay_lr: 2.5e-6

# ==================== 训练基础配置 ====================
training:
  output_directory: 'outputs/train/${task}/${method}' # 与其他策略一致的路径格式
  device: 'cuda'
  seed: 42

  # 动作维度适配配置
  target_action_dim: 32 # 目标动作维度（与预训练模型一致）

  # 数据加载
  batch_size: 64 # 更稳定的梯度更新
  # accumulation_steps: 2 # 等效64
  num_workers: 14
  drop_last: True # 保证每个batch大小一致，避免BN层统计偏差

  use_amp: True # 使用混合精度训练

  # 日志和保存
  save_freq_epoch: 5 # 每3个epoch保存一次checkpoint（方案A：更频繁保存）
  log_freq: 10

  # 多任务验证配置（防遗忘的关键）
  # 方案A：单任务专用模型，不需要验证之前的任务
  validate_all_previous_tasks: False
  validation_freq_epoch: 1 # 从2改为1，更频繁验证及早发现问题
  validation_episodes: 20 # 更多验证episodes，评估更准确

# ==================== 顺序训练策略配置 ====================
# 方案 A：单任务专用模型配置
# 关闭 replay buffer，每个任务独立训练以最大化单任务性能
sequential:
  # 是否使用Replay Buffer（已关闭 - 方案A）
  use_replay_buffer: False # ⚠️ 关闭多任务混合训练
  replay_strategy: 'none' # 不使用replay策略

  # 以下replay配置仅在 use_replay_buffer=True 时生效
  # Stage 2: 训练任务2时的数据混合比例
  stage2_replay:
    task1: 0.2 # 20% 任务1数据
    task2: 0.8 # 80% 任务2数据

  # Stage 3: 训练任务3时的数据混合比例
  stage3_replay:
    task1: 0.1 # 10% 任务1数据
    task2: 0.2 # 20% 任务2数据
    task3: 0.7 # 70% 任务3数据

  # Stage 4: 训练任务4时的数据混合比例（最终多任务模型）
  stage4_replay:
    task1: 0.1 # 10% 任务1数据
    task2: 0.1 # 10% 任务2数据
    task3: 0.2 # 20% 任务3数据
    task4: 0.6 # 60% 任务4数据
