# -*- coding: utf-8 -*-
"""
åŸºç¡€è¯„ä¼°å™¨æ¨¡å—

å®šä¹‰æ‰€æœ‰è¯„ä¼°å™¨çš„é€šç”¨æ¥å£å’ŒåŸºç¡€åŠŸèƒ½
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import lerobot_patches.custom_patches

import torch
import numpy as np
import time
import json
import csv
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from omegaconf import DictConfig, OmegaConf
import logging
from datetime import datetime

from kuavo_train.wrapper.dataset.LeRobotDatasetWrapper import CustomLeRobotDataset
from lerobot.utils.random_utils import set_seed

@dataclass
class EvaluationResults:
    """è¯„ä¼°ç»“æœæ•°æ®ç±»"""
    action_metrics: Dict[str, float]
    performance_metrics: Dict[str, float]
    episode_results: List[Dict[str, Any]]
    summary: Dict[str, Any]
    timing_info: Dict[str, float]

class BaseEvaluator(ABC):
    """
    åŸºç¡€è¯„ä¼°å™¨æŠ½è±¡ç±»

    å®šä¹‰æ‰€æœ‰è¯„ä¼°å™¨å¿…é¡»å®ç°çš„æ¥å£å’Œé€šç”¨åŠŸèƒ½
    """

    def __init__(self, config: DictConfig):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            config: è¯„ä¼°é…ç½®
        """
        self.config = config
        self.model = None
        self.test_dataset = None
        self.device = torch.device(config.common.device)
        self.output_dir = Path(config.common.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®éšæœºç§å­
        set_seed(config.common.seed)

        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()

        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.results = {
            'action_metrics': {},
            'performance_metrics': {},
            'episode_results': [],
            'timing_info': {}
        }

        self.logger.info(f"Initialized {self.__class__.__name__} with config: {config.model.type}")

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger(f"{self.__class__.__name__}")
        logger.setLevel(getattr(logging, self.config.logging.level))

        # æ¸…é™¤ç°æœ‰çš„handlers
        logger.handlers.clear()

        # æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)

        # æ–‡ä»¶handlerï¼ˆå¦‚æœé…ç½®äº†ä¿å­˜æ—¥å¿—ï¼‰
        if self.config.logging.save_logs:
            log_file = self.output_dir / f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            file_handler = logging.FileHandler(log_file)
            file_handler.setFormatter(console_formatter)
            logger.addHandler(file_handler)

        return logger

    @abstractmethod
    def load_model(self) -> None:
        """
        åŠ è½½æ¨¡å‹ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰
        """
        pass

    def prepare_data(self) -> None:
        """
        å‡†å¤‡æµ‹è¯•æ•°æ®
        """
        self.logger.info("Preparing test dataset...")

        # æ„å»ºepisodesåˆ—è¡¨
        start_ep, end_ep = self.config.test_data.episodes_range
        episodes = list(range(start_ep, min(end_ep + 1, start_ep + self.config.test_data.max_episodes)))

        self.logger.info(f"Loading episodes {episodes} from {self.config.test_data.root}")

        # åˆ›å»ºæ•°æ®é›†
        self.test_dataset = CustomLeRobotDataset(
            repo_id=self.config.test_data.repo_id,
            root=self.config.test_data.root,
            episodes=episodes,
        )

        self.logger.info(f"Dataset loaded with {len(self.test_dataset)} samples")

    def _calculate_action_metrics(self, predicted_actions: torch.Tensor,
                                true_actions: torch.Tensor) -> Dict[str, float]:
        """
        è®¡ç®—åŠ¨ä½œç²¾åº¦æŒ‡æ ‡

        Args:
            predicted_actions: é¢„æµ‹åŠ¨ä½œ [batch_size, action_dim]
            true_actions: çœŸå®åŠ¨ä½œ [batch_size, action_dim]

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        metrics = {}

        # ç¡®ä¿åœ¨CPUä¸Šè®¡ç®—
        pred = predicted_actions.detach().cpu()
        true = true_actions.detach().cpu()

        # MSE
        if 'mse' in self.config.evaluation.action_metrics:
            mse = torch.nn.functional.mse_loss(pred, true)
            metrics['mse'] = float(mse.item())

        # MAE
        if 'mae' in self.config.evaluation.action_metrics:
            mae = torch.nn.functional.l1_loss(pred, true)
            metrics['mae'] = float(mae.item())

        # ä½™å¼¦ç›¸ä¼¼åº¦
        if 'cosine_sim' in self.config.evaluation.action_metrics:
            cosine_sim = torch.nn.functional.cosine_similarity(pred, true, dim=-1).mean()
            metrics['cosine_sim'] = float(cosine_sim.item())

        # L2èŒƒæ•°å·®å¼‚
        if 'l2_norm' in self.config.evaluation.action_metrics:
            l2_norm = torch.norm(pred - true, p=2, dim=-1).mean()
            metrics['l2_norm'] = float(l2_norm.item())

        return metrics

    def _calculate_joint_metrics(self, predicted_actions: torch.Tensor,
                               true_actions: torch.Tensor) -> Dict[str, float]:
        """
        è®¡ç®—å…³èŠ‚ç‰¹å®šæŒ‡æ ‡

        Args:
            predicted_actions: é¢„æµ‹åŠ¨ä½œ
            true_actions: çœŸå®åŠ¨ä½œ

        Returns:
            å…³èŠ‚æŒ‡æ ‡å­—å…¸
        """
        if not self.config.evaluation.joint_analysis.enable:
            return {}

        metrics = {}
        pred = predicted_actions.detach().cpu()
        true = true_actions.detach().cpu()

        # æ¯ä¸ªå…³èŠ‚çš„è¯¯å·®
        joint_errors = torch.abs(pred - true)  # [batch_size, num_joints]
        joint_weights = torch.tensor(self.config.evaluation.joint_analysis.joint_weights)

        # åŠ æƒè¯¯å·®
        weighted_errors = joint_errors * joint_weights.unsqueeze(0)
        metrics['weighted_joint_error'] = float(weighted_errors.mean().item())

        # å…³é”®å…³èŠ‚è¯¯å·®
        critical_joints = self.config.evaluation.joint_analysis.critical_joints
        if critical_joints:
            critical_errors = joint_errors[:, critical_joints].mean()
            metrics['critical_joint_error'] = float(critical_errors.item())

        return metrics

    def _calculate_temporal_metrics(self, episode_predictions: List[torch.Tensor],
                                  episode_ground_truth: List[torch.Tensor]) -> Dict[str, float]:
        """
        è®¡ç®—æ—¶åºä¸€è‡´æ€§æŒ‡æ ‡

        Args:
            episode_predictions: episodeä¸­çš„é¢„æµ‹åºåˆ—
            episode_ground_truth: episodeä¸­çš„çœŸå®åºåˆ—

        Returns:
            æ—¶åºæŒ‡æ ‡å­—å…¸
        """
        if not self.config.evaluation.temporal_consistency.enable:
            return {}

        metrics = {}
        window_size = self.config.evaluation.temporal_consistency.window_size

        if len(episode_predictions) < window_size:
            return metrics

        # è®¡ç®—è¿ç»­åŠ¨ä½œçš„å¹³æ»‘åº¦
        pred_actions = torch.stack(episode_predictions)  # [seq_len, action_dim]
        true_actions = torch.stack(episode_ground_truth)

        # é€Ÿåº¦å¹³æ»‘åº¦ï¼ˆç›¸é‚»æ—¶åˆ»åŠ¨ä½œå·®å¼‚ï¼‰
        pred_velocities = pred_actions[1:] - pred_actions[:-1]
        true_velocities = true_actions[1:] - true_actions[:-1]

        velocity_error = torch.nn.functional.mse_loss(pred_velocities, true_velocities)
        metrics['velocity_smoothness'] = float(velocity_error.item())

        # åŠ é€Ÿåº¦å¹³æ»‘åº¦
        if len(pred_velocities) > 1:
            pred_accelerations = pred_velocities[1:] - pred_velocities[:-1]
            true_accelerations = true_velocities[1:] - true_velocities[:-1]

            acceleration_error = torch.nn.functional.mse_loss(pred_accelerations, true_accelerations)
            metrics['acceleration_smoothness'] = float(acceleration_error.item())

        return metrics

    @abstractmethod
    def _model_inference(self, observation: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        æ¨¡å‹æ¨ç†ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            observation: è§‚æµ‹æ•°æ®

        Returns:
            Tuple[é¢„æµ‹åŠ¨ä½œ, æ¨ç†ä¿¡æ¯]
        """
        pass

    def evaluate_single_episode(self, episode_data: List[Dict]) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªepisode

        Args:
            episode_data: episodeæ•°æ®åˆ—è¡¨

        Returns:
            episodeè¯„ä¼°ç»“æœ
        """
        episode_predictions = []
        episode_ground_truth = []
        episode_metrics = []
        inference_times = []

        self.logger.debug(f"Evaluating episode with {len(episode_data)} steps")

        for step_idx, step_data in enumerate(episode_data):
            # å‡†å¤‡è§‚æµ‹æ•°æ®
            observation = {}
            for key, value in step_data.items():
                if key.startswith('observation.'):
                    observation[key] = value.unsqueeze(0).to(self.device)

            # è·å–çœŸå®åŠ¨ä½œ
            true_action = step_data['action'].to(self.device)

            # æ¨¡å‹æ¨ç†
            start_time = time.time()
            try:
                predicted_action, inference_info = self._model_inference(observation)
                inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            except Exception as e:
                self.logger.error(f"Inference failed at step {step_idx}: {e}")
                continue

            # è®°å½•ç»“æœ
            episode_predictions.append(predicted_action.squeeze(0))
            episode_ground_truth.append(true_action)
            inference_times.append(inference_time)

            # è®¡ç®—æ­¥éª¤çº§æŒ‡æ ‡
            step_metrics = self._calculate_action_metrics(predicted_action, true_action.unsqueeze(0))
            step_metrics.update(self._calculate_joint_metrics(predicted_action, true_action.unsqueeze(0)))
            step_metrics['inference_time'] = inference_time
            step_metrics.update(inference_info)

            episode_metrics.append(step_metrics)

        # è®¡ç®—episodeçº§æŒ‡æ ‡
        episode_result = {
            'num_steps': len(episode_predictions),
            'average_inference_time': np.mean(inference_times) if inference_times else 0,
            'total_inference_time': np.sum(inference_times) if inference_times else 0,
        }

        # æ±‡æ€»åŠ¨ä½œæŒ‡æ ‡
        if episode_metrics:
            for metric_name in self.config.evaluation.action_metrics:
                if metric_name in episode_metrics[0]:
                    values = [m[metric_name] for m in episode_metrics if metric_name in m]
                    episode_result[f'avg_{metric_name}'] = np.mean(values)
                    episode_result[f'std_{metric_name}'] = np.std(values)

        # è®¡ç®—æ—¶åºæŒ‡æ ‡
        temporal_metrics = self._calculate_temporal_metrics(episode_predictions, episode_ground_truth)
        episode_result.update(temporal_metrics)

        return episode_result

    @abstractmethod
    def evaluate(self) -> EvaluationResults:
        """
        æ‰§è¡Œå®Œæ•´è¯„ä¼°ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰
        """
        pass

    def save_results(self, results: EvaluationResults) -> None:
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            results: è¯„ä¼°ç»“æœ
        """
        if not self.config.common.save_results:
            return

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜JSONæ ¼å¼
        if 'json' in self.config.report.format:
            json_file = self.output_dir / f"evaluation_results_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                serializable_results = self._make_serializable(results.__dict__)
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Results saved to {json_file}")

        # ä¿å­˜CSVæ ¼å¼
        if 'csv' in self.config.report.format:
            csv_file = self.output_dir / f"evaluation_summary_{timestamp}.csv"
            self._save_csv_summary(results, csv_file)
            self.logger.info(f"Summary saved to {csv_file}")

    def _make_serializable(self, obj: Any) -> Any:
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, (np.ndarray, torch.Tensor)):
            return obj.tolist() if hasattr(obj, 'tolist') else list(obj)
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, Path):
            return str(obj)
        else:
            return obj

    def _save_csv_summary(self, results: EvaluationResults, csv_file: Path) -> None:
        """ä¿å­˜CSVæ ¼å¼çš„æ‘˜è¦"""
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow(['Metric', 'Value'])

            # å†™å…¥åŠ¨ä½œæŒ‡æ ‡
            writer.writerow(['=== Action Metrics ===', ''])
            for metric, value in results.action_metrics.items():
                writer.writerow([metric, value])

            # å†™å…¥æ€§èƒ½æŒ‡æ ‡
            writer.writerow(['=== Performance Metrics ===', ''])
            for metric, value in results.performance_metrics.items():
                writer.writerow([metric, value])

            # å†™å…¥æ‘˜è¦
            writer.writerow(['=== Summary ===', ''])
            for metric, value in results.summary.items():
                writer.writerow([metric, value])

    def generate_report(self, results: EvaluationResults) -> None:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

        Args:
            results: è¯„ä¼°ç»“æœ
        """
        self.logger.info("Generating evaluation report...")

        # æ‰“å°æ‘˜è¦åˆ°æ§åˆ¶å°
        self._print_summary(results)

        # ä¿å­˜ç»“æœ
        self.save_results(results)

        # ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if self.config.report.include_plots:
            self._generate_plots(results)

    def _print_summary(self, results: EvaluationResults) -> None:
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print(f"EVALUATION SUMMARY - {self.__class__.__name__}")
        print("="*60)

        print("\nğŸ“Š Action Metrics:")
        for metric, value in results.action_metrics.items():
            print(f"  {metric}: {value:.6f}")

        print("\nâš¡ Performance Metrics:")
        for metric, value in results.performance_metrics.items():
            print(f"  {metric}: {value:.3f}")

        print("\nğŸ“ˆ Summary:")
        for metric, value in results.summary.items():
            print(f"  {metric}: {value}")

        print("="*60 + "\n")

    def _generate_plots(self, results: EvaluationResults) -> None:
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼Œå­ç±»å¯ä»¥è¦†ç›–ï¼‰"""
        try:
            import matplotlib.pyplot as plt

            if self.config.visualization.plot_action_comparison:
                self._plot_action_comparison(results)

            if self.config.visualization.plot_error_distribution:
                self._plot_error_distribution(results)

        except ImportError:
            self.logger.warning("Matplotlib not available, skipping plot generation")
        except Exception as e:
            self.logger.error(f"Error generating plots: {e}")

    def _plot_action_comparison(self, results: EvaluationResults) -> None:
        """ç»˜åˆ¶åŠ¨ä½œå¯¹æ¯”å›¾ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        # å­ç±»å¯ä»¥å®ç°å…·ä½“çš„ç»˜å›¾é€»è¾‘
        pass

    def _plot_error_distribution(self, results: EvaluationResults) -> None:
        """ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒå›¾ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        # å­ç±»å¯ä»¥å®ç°å…·ä½“çš„ç»˜å›¾é€»è¾‘
        pass