#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¦»çº¿è¯„ä¼°ä¸»è„šæœ¬

æ”¯æŒåˆ†å±‚æ¶æ„å’Œä¼ ç»Ÿdiffusionæ¨¡å‹çš„ç¦»çº¿è¯„ä¼°
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import lerobot_patches.custom_patches

import argparse
import logging
from typing import Optional
from omegaconf import DictConfig, OmegaConf

# å¯¼å…¥è¯„ä¼°å™¨
from kuavo_eval.core.hierarchical_evaluator import HierarchicalEvaluator
from kuavo_eval.core.diffusion_evaluator import DiffusionEvaluator
from kuavo_eval.utils.report_generator import EvaluationReportGenerator

def load_config(config_path: str) -> DictConfig:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å¯¹è±¡
    """
    config_path = Path(config_path)

    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    # åŠ è½½ä¸»é…ç½®
    config = OmegaConf.load(config_path)

    # å¤„ç†defaultsï¼ˆç»§æ‰¿åŸºç¡€é…ç½®ï¼‰
    if 'defaults' in config:
        for default_config in config.defaults:
            if isinstance(default_config, str):
                default_path = config_path.parent / f"{default_config}.yaml"
                if default_path.exists():
                    base_config = OmegaConf.load(default_path)
                    # åˆå¹¶é…ç½®ï¼Œå½“å‰é…ç½®ä¼˜å…ˆ
                    config = OmegaConf.merge(base_config, config)

    return config

def create_evaluator(config: DictConfig):
    """
    æ ¹æ®é…ç½®åˆ›å»ºç›¸åº”çš„è¯„ä¼°å™¨

    Args:
        config: é…ç½®å¯¹è±¡

    Returns:
        è¯„ä¼°å™¨å®ä¾‹
    """
    model_type = config.model.type

    if model_type == 'humanoid_diffusion':
        print(f"ğŸ¤– Creating HierarchicalEvaluator for {model_type}")
        return HierarchicalEvaluator(config)
    elif model_type == 'diffusion':
        print(f"ğŸ“ Creating DiffusionEvaluator for {model_type}")
        return DiffusionEvaluator(config)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

def validate_config(config: DictConfig) -> None:
    """
    éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§

    Args:
        config: é…ç½®å¯¹è±¡
    """
    required_fields = [
        'model.type',
        'model.checkpoint_path',
        'test_data.root',
        'test_data.repo_id',
        'common.device'
    ]

    for field in required_fields:
        if not OmegaConf.select(config, field):
            raise ValueError(f"Missing required config field: {field}")

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    checkpoint_path = Path(config.model.checkpoint_path)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Model checkpoint not found: {checkpoint_path}")

    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_root = Path(config.test_data.root)
    if not data_root.exists():
        raise FileNotFoundError(f"Data root directory not found: {data_root}")

    print("âœ… Configuration validation passed")

def setup_logging(config: DictConfig) -> None:
    """
    è®¾ç½®æ—¥å¿—

    Args:
        config: é…ç½®å¯¹è±¡
    """
    log_level = getattr(logging, config.logging.level.upper(), logging.INFO)

    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # è®¾ç½®ç‰¹å®šåº“çš„æ—¥å¿—çº§åˆ«
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    logging.getLogger('PIL').setLevel(logging.WARNING)

def print_evaluation_info(config: DictConfig) -> None:
    """
    æ‰“å°è¯„ä¼°ä¿¡æ¯

    Args:
        config: é…ç½®å¯¹è±¡
    """
    print("\n" + "="*60)
    print("ğŸš€ KUAVO OFFLINE EVALUATION")
    print("="*60)
    print(f"ğŸ“‹ Model Type: {config.model.type}")
    print(f"ğŸ“‚ Checkpoint: {config.model.checkpoint_path}")
    print(f"ğŸ“Š Data Root: {config.test_data.root}")
    print(f"ğŸ¯ Repository: {config.test_data.repo_id}")
    print(f"ğŸ“ˆ Episodes Range: {config.test_data.episodes_range}")
    print(f"ğŸ”¢ Max Episodes: {config.test_data.max_episodes}")
    print(f"ğŸ’» Device: {config.common.device}")
    print("="*60 + "\n")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Kuavo Offline Model Evaluation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # è¯„ä¼°åˆ†å±‚æ¶æ„æ¨¡å‹
  python run_offline_eval.py --config ../../configs/eval/offline_hierarchical_eval.yaml

  # è¯„ä¼°ä¼ ç»Ÿdiffusionæ¨¡å‹
  python run_offline_eval.py --config ../../configs/eval/offline_diffusion_eval.yaml

  # ä½¿ç”¨è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python run_offline_eval.py --config ../../configs/eval/offline_hierarchical_eval.yaml --output-dir ./my_results

  # è¯¦ç»†æ¨¡å¼
  python run_offline_eval.py --config ../../configs/eval/offline_hierarchical_eval.yaml --verbose
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='Path to evaluation configuration file'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        help='Output directory for results (overrides config)'
    )

    parser.add_argument(
        '--checkpoint',
        type=str,
        help='Model checkpoint path (overrides config)'
    )

    parser.add_argument(
        '--episodes',
        type=int,
        help='Maximum number of episodes to evaluate (overrides config)'
    )

    parser.add_argument(
        '--device',
        type=str,
        choices=['cpu', 'cuda'],
        help='Device to use for evaluation (overrides config)'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='Enable quick validation mode (reduced episodes and steps)'
    )

    parser.add_argument(
        '--no-plots',
        action='store_true',
        help='Disable plot generation'
    )

    args = parser.parse_args()

    try:
        # åŠ è½½é…ç½®
        print(f"ğŸ“– Loading configuration from {args.config}")
        config = load_config(args.config)

        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        if args.output_dir:
            config.common.output_dir = args.output_dir

        if args.checkpoint:
            config.model.checkpoint_path = args.checkpoint

        if args.episodes:
            config.test_data.max_episodes = args.episodes

        if args.device:
            config.common.device = args.device

        if args.verbose:
            config.logging.level = 'DEBUG'

        if args.quick:
            # å¯ç”¨å¿«é€ŸéªŒè¯æ¨¡å¼
            config.test_data.max_episodes = min(3, config.test_data.max_episodes)
            config.test_data.max_steps_per_episode = min(20, config.test_data.max_steps_per_episode)
            print("âš¡ Quick validation mode enabled")

        if args.no_plots:
            config.report.include_plots = False

        # è®¾ç½®æ—¥å¿—
        setup_logging(config)

        # éªŒè¯é…ç½®
        validate_config(config)

        # æ‰“å°è¯„ä¼°ä¿¡æ¯
        print_evaluation_info(config)

        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = create_evaluator(config)

        # æ‰§è¡Œè¯„ä¼°
        print("ğŸ”„ Starting evaluation...")
        results = evaluator.evaluate()

        # ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“Š Generating evaluation report...")
        report_generator = EvaluationReportGenerator(
            output_dir=config.common.output_dir,
            config=OmegaConf.to_container(config, resolve=True)
        )

        generated_files = report_generator.generate_comprehensive_report(
            results.__dict__,
            model_type=config.model.type
        )

        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("âœ… EVALUATION COMPLETED")
        print("="*60)
        print("ğŸ“ˆ Results Summary:")
        for key, value in results.summary.items():
            print(f"  {key}: {value}")

        print(f"\nğŸ“ Generated Files:")
        for file_type, file_path in generated_files.items():
            print(f"  {file_type}: {file_path}")

        print("\nğŸ‰ Evaluation completed successfully!")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  Evaluation interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Evaluation failed: {e}")
        logging.exception("Detailed error information:")
        sys.exit(1)

if __name__ == "__main__":
    main()