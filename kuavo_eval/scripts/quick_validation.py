#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€ŸéªŒè¯è„šæœ¬

ä¸“é—¨ç”¨äº1-epochæ¨¡å‹çš„å¿«é€ŸéªŒè¯ï¼Œæä¾›ç²¾ç®€ä½†å…³é”®çš„è¯„ä¼°æŒ‡æ ‡
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import lerobot_patches.custom_patches

import argparse
import time
import torch
from typing import Dict, Any
from omegaconf import DictConfig, OmegaConf

from kuavo_eval.scripts.run_offline_eval import load_config, create_evaluator

class QuickValidator:
    """å¿«é€ŸéªŒè¯å™¨"""

    def __init__(self, config: DictConfig):
        self.config = config
        self.results = {}

    def run_quick_validation(self) -> Dict[str, Any]:
        """
        è¿è¡Œå¿«é€ŸéªŒè¯

        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        print("âš¡ Starting Quick Validation...")
        start_time = time.time()

        # ä¿®æ”¹é…ç½®ä¸ºå¿«é€Ÿæ¨¡å¼
        self._setup_quick_mode()

        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = create_evaluator(self.config)

        # æ‰§è¡Œæœ€å°åŒ–è¯„ä¼°
        try:
            print("ğŸ”„ Running minimal evaluation...")
            results = evaluator.evaluate()

            # æå–å…³é”®æŒ‡æ ‡
            key_metrics = self._extract_key_metrics(results)

            # éªŒè¯æ¨¡å‹å¥åº·çŠ¶å†µ
            health_check = self._check_model_health(key_metrics)

            total_time = time.time() - start_time

            validation_results = {
                'status': 'success',
                'total_time': total_time,
                'key_metrics': key_metrics,
                'health_check': health_check,
                'model_type': self.config.model.type,
                'checkpoint_path': self.config.model.checkpoint_path,
                'validation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }

            return validation_results

        except Exception as e:
            error_time = time.time() - start_time
            return {
                'status': 'failed',
                'error': str(e),
                'total_time': error_time,
                'model_type': self.config.model.type,
                'checkpoint_path': self.config.model.checkpoint_path,
                'validation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }

    def _setup_quick_mode(self) -> None:
        """è®¾ç½®å¿«é€ŸéªŒè¯æ¨¡å¼"""
        # å‡å°‘æµ‹è¯•æ•°æ®é‡
        self.config.test_data.max_episodes = 2
        self.config.test_data.max_steps_per_episode = 10

        # ç¦ç”¨å¤æ‚åˆ†æ
        self.config.evaluation.temporal_consistency.enable = False
        self.config.report.include_plots = False
        self.config.report.detailed_analysis = False

        # åªä¿ç•™æ ¸å¿ƒæŒ‡æ ‡
        self.config.evaluation.action_metrics = ['mse', 'mae']

        # æ¨¡å‹ç‰¹å®šå¿«é€Ÿè®¾ç½®
        if self.config.model.type == 'humanoid_diffusion':
            # ç¦ç”¨åˆ†å±‚æ¶æ„çš„å¤æ‚åˆ†æ
            if hasattr(self.config, 'hierarchical_evaluation'):
                self.config.hierarchical_evaluation.layer_consistency_check.enable = False
                self.config.hierarchical_evaluation.layer_weight_analysis.enable = False

        elif self.config.model.type == 'diffusion':
            # ç¦ç”¨diffusionçš„å¤æ‚åˆ†æ
            if hasattr(self.config, 'diffusion_evaluation'):
                self.config.diffusion_evaluation.denoising_analysis.enable = False
                self.config.diffusion_evaluation.inference_steps_analysis.enable = False

        print("âš™ï¸  Quick mode configured: 2 episodes, 10 steps each")

    def _extract_key_metrics(self, results) -> Dict[str, Any]:
        """æå–å…³é”®æŒ‡æ ‡"""
        key_metrics = {}

        # åŸºç¡€åŠ¨ä½œæŒ‡æ ‡
        action_metrics = results.action_metrics
        if 'overall_avg_mse' in action_metrics:
            key_metrics['mse'] = action_metrics['overall_avg_mse']
        if 'overall_avg_mae' in action_metrics:
            key_metrics['mae'] = action_metrics['overall_avg_mae']

        # æ€§èƒ½æŒ‡æ ‡
        performance_metrics = results.performance_metrics
        if 'overall_avg_inference_time' in performance_metrics:
            key_metrics['avg_inference_time'] = performance_metrics['overall_avg_inference_time']

        # æ¨¡å‹ç‰¹å®šå…³é”®æŒ‡æ ‡
        if self.config.model.type == 'humanoid_diffusion':
            # åˆ†å±‚æ¶æ„å…³é”®æŒ‡æ ‡
            if 'budget_compliance_rate' in performance_metrics:
                key_metrics['budget_compliance'] = performance_metrics['budget_compliance_rate']

            # å®‰å…¨å±‚æ¿€æ´»ç‡
            if 'safety_activation_rate' in performance_metrics:
                key_metrics['safety_activation'] = performance_metrics['safety_activation_rate']

        elif self.config.model.type == 'diffusion':
            # Diffusionå…³é”®æŒ‡æ ‡
            if 'overall_steps_per_second' in performance_metrics:
                key_metrics['inference_speed'] = performance_metrics['overall_steps_per_second']

        # æ€»ä½“æŒ‡æ ‡
        key_metrics['total_episodes'] = results.summary.get('total_episodes_evaluated', 0)
        key_metrics['total_steps'] = results.summary.get('total_inference_steps', 0)

        return key_metrics

    def _check_model_health(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶å†µ"""
        health_check = {
            'overall_status': 'unknown',
            'checks': {}
        }

        checks_passed = 0
        total_checks = 0

        # æ£€æŸ¥MSE
        if 'mse' in metrics:
            total_checks += 1
            mse = metrics['mse']
            if mse < 0.1:
                health_check['checks']['mse'] = {'status': 'pass', 'value': mse, 'threshold': 0.1}
                checks_passed += 1
            elif mse < 0.5:
                health_check['checks']['mse'] = {'status': 'warning', 'value': mse, 'threshold': 0.1}
            else:
                health_check['checks']['mse'] = {'status': 'fail', 'value': mse, 'threshold': 0.1}

        # æ£€æŸ¥æ¨ç†æ—¶é—´
        if 'avg_inference_time' in metrics:
            total_checks += 1
            inference_time = metrics['avg_inference_time']
            threshold = 100.0  # 100ms
            if inference_time < threshold:
                health_check['checks']['inference_time'] = {'status': 'pass', 'value': inference_time, 'threshold': threshold}
                checks_passed += 1
            elif inference_time < threshold * 2:
                health_check['checks']['inference_time'] = {'status': 'warning', 'value': inference_time, 'threshold': threshold}
            else:
                health_check['checks']['inference_time'] = {'status': 'fail', 'value': inference_time, 'threshold': threshold}

        # åˆ†å±‚æ¶æ„ç‰¹å®šæ£€æŸ¥
        if self.config.model.type == 'humanoid_diffusion':
            if 'budget_compliance' in metrics:
                total_checks += 1
                compliance = metrics['budget_compliance']
                if compliance > 0.8:
                    health_check['checks']['budget_compliance'] = {'status': 'pass', 'value': compliance, 'threshold': 0.8}
                    checks_passed += 1
                elif compliance > 0.5:
                    health_check['checks']['budget_compliance'] = {'status': 'warning', 'value': compliance, 'threshold': 0.8}
                else:
                    health_check['checks']['budget_compliance'] = {'status': 'fail', 'value': compliance, 'threshold': 0.8}

            if 'safety_activation' in metrics:
                total_checks += 1
                safety_rate = metrics['safety_activation']
                if safety_rate > 0.8:  # å®‰å…¨å±‚åº”è¯¥ç»å¸¸æ¿€æ´»
                    health_check['checks']['safety_activation'] = {'status': 'pass', 'value': safety_rate, 'threshold': 0.8}
                    checks_passed += 1
                elif safety_rate > 0.5:
                    health_check['checks']['safety_activation'] = {'status': 'warning', 'value': safety_rate, 'threshold': 0.8}
                else:
                    health_check['checks']['safety_activation'] = {'status': 'fail', 'value': safety_rate, 'threshold': 0.8}

        # è®¡ç®—æ•´ä½“çŠ¶æ€
        if total_checks == 0:
            health_check['overall_status'] = 'unknown'
        else:
            pass_rate = checks_passed / total_checks
            if pass_rate >= 0.8:
                health_check['overall_status'] = 'healthy'
            elif pass_rate >= 0.5:
                health_check['overall_status'] = 'warning'
            else:
                health_check['overall_status'] = 'unhealthy'

        health_check['pass_rate'] = pass_rate if total_checks > 0 else 0.0
        health_check['checks_passed'] = checks_passed
        health_check['total_checks'] = total_checks

        return health_check

def print_validation_results(results: Dict[str, Any]) -> None:
    """æ‰“å°éªŒè¯ç»“æœ"""
    print("\n" + "="*50)
    print("âš¡ QUICK VALIDATION RESULTS")
    print("="*50)

    if results['status'] == 'success':
        print(f"âœ… Status: {results['status'].upper()}")
        print(f"â±ï¸  Total Time: {results['total_time']:.2f}s")
        print(f"ğŸ¤– Model Type: {results['model_type']}")

        # å…³é”®æŒ‡æ ‡
        print("\nğŸ“Š Key Metrics:")
        key_metrics = results['key_metrics']
        for metric, value in key_metrics.items():
            if isinstance(value, float):
                print(f"  {metric}: {value:.4f}")
            else:
                print(f"  {metric}: {value}")

        # å¥åº·æ£€æŸ¥
        print("\nğŸ¥ Health Check:")
        health = results['health_check']
        status_emoji = {
            'healthy': 'âœ…',
            'warning': 'âš ï¸',
            'unhealthy': 'âŒ',
            'unknown': 'â“'
        }

        overall_status = health['overall_status']
        print(f"  Overall Status: {status_emoji.get(overall_status, 'â“')} {overall_status.upper()}")
        print(f"  Pass Rate: {health['pass_rate']:.1%} ({health['checks_passed']}/{health['total_checks']})")

        # è¯¦ç»†æ£€æŸ¥ç»“æœ
        if health['checks']:
            print("\n  Detailed Checks:")
            for check_name, check_result in health['checks'].items():
                status = check_result['status']
                value = check_result['value']
                threshold = check_result['threshold']

                check_emoji = {'pass': 'âœ…', 'warning': 'âš ï¸', 'fail': 'âŒ'}

                if isinstance(value, float):
                    print(f"    {check_emoji.get(status, 'â“')} {check_name}: {value:.4f} (threshold: {threshold})")
                else:
                    print(f"    {check_emoji.get(status, 'â“')} {check_name}: {value} (threshold: {threshold})")

        # å»ºè®®
        print("\nğŸ’¡ Recommendations:")
        if overall_status == 'healthy':
            print("  ğŸ‰ Model appears to be working well!")
            print("  âœ¨ Ready for full evaluation or deployment testing")
        elif overall_status == 'warning':
            print("  âš ï¸  Model shows some performance issues")
            print("  ğŸ”§ Consider tuning hyperparameters or training longer")
        elif overall_status == 'unhealthy':
            print("  âŒ Model has significant issues")
            print("  ğŸ”¨ Check training process and data quality")
            print("  ğŸ“Š Run full evaluation for detailed analysis")
        else:
            print("  â“ Unable to determine model health")
            print("  ğŸ” Run full evaluation for more information")

    else:
        print(f"âŒ Status: {results['status'].upper()}")
        print(f"â±ï¸  Time to Failure: {results['total_time']:.2f}s")
        print(f"ğŸ¤– Model Type: {results['model_type']}")
        print(f"âŒ Error: {results['error']}")

        print("\nğŸ”§ Troubleshooting:")
        print("  1. Check model checkpoint path exists")
        print("  2. Verify data directory and format")
        print("  3. Ensure sufficient GPU memory")
        print("  4. Check configuration file syntax")

    print("="*50 + "\n")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Quick validation for 1-epoch models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # å¿«é€ŸéªŒè¯åˆ†å±‚æ¶æ„æ¨¡å‹
  python quick_validation.py --config ../../configs/eval/offline_hierarchical_eval.yaml

  # å¿«é€ŸéªŒè¯å¹¶æŒ‡å®šæ£€æŸ¥ç‚¹
  python quick_validation.py --config ../../configs/eval/offline_diffusion_eval.yaml --checkpoint ./outputs/train/task/method/run_xxx/epoch1

  # ä½¿ç”¨CPUè¿›è¡ŒéªŒè¯
  python quick_validation.py --config ../../configs/eval/offline_hierarchical_eval.yaml --device cpu
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='Path to evaluation configuration file'
    )

    parser.add_argument(
        '--checkpoint',
        type=str,
        help='Model checkpoint path (overrides config)'
    )

    parser.add_argument(
        '--device',
        type=str,
        choices=['cpu', 'cuda'],
        help='Device to use for validation'
    )

    args = parser.parse_args()

    try:
        # åŠ è½½é…ç½®
        print(f"ğŸ“– Loading configuration: {args.config}")
        config = load_config(args.config)

        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        if args.checkpoint:
            config.model.checkpoint_path = args.checkpoint
            print(f"ğŸ”„ Using checkpoint: {args.checkpoint}")

        if args.device:
            config.common.device = args.device
            print(f"ğŸ’» Using device: {args.device}")

        # éªŒè¯æ£€æŸ¥ç‚¹å­˜åœ¨
        checkpoint_path = Path(config.model.checkpoint_path)
        if not checkpoint_path.exists():
            print(f"âŒ Checkpoint not found: {checkpoint_path}")
            print("ğŸ’¡ Make sure the model has been trained and saved")
            sys.exit(1)

        # åˆ›å»ºå¿«é€ŸéªŒè¯å™¨
        validator = QuickValidator(config)

        # è¿è¡ŒéªŒè¯
        results = validator.run_quick_validation()

        # æ‰“å°ç»“æœ
        print_validation_results(results)

        # è®¾ç½®é€€å‡ºç 
        if results['status'] == 'success':
            if results['health_check']['overall_status'] in ['healthy', 'warning']:
                sys.exit(0)
            else:
                sys.exit(2)  # æ¨¡å‹ä¸å¥åº·
        else:
            sys.exit(1)  # éªŒè¯å¤±è´¥

    except KeyboardInterrupt:
        print("\nâ¹ï¸  Validation interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Validation failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()