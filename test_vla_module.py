#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLAæ¨¡å—æµ‹è¯•è„šæœ¬

éªŒè¯TokenåŒ–VLAç­–ç•¥çš„å„ä¸ªç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

from kuavo_train.wrapper.policy.vla.decoders.DiffusionDecoder import DiffusionDecoder
from kuavo_train.wrapper.policy.vla.tokenizers.ActionTokenizer import ActionTokenizer
from kuavo_train.wrapper.policy.vla.tokenizers.StateTokenizer import StateTokenizer
from kuavo_train.wrapper.policy.vla.tokenizers.VisionTokenizer import VisionTokenizer
import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))


def test_vision_tokenizer():
    """æµ‹è¯•VisionTokenizer"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•VisionTokenizer")
    print("=" * 70)

    tokenizer = VisionTokenizer(patch_size=16, embed_dim=512, image_size=224)

    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    num_cameras = 3
    rgb_images = torch.randn(batch_size, num_cameras, 3, 224, 224)
    depth_images = torch.randn(batch_size, num_cameras, 1, 224, 224)

    # TokenåŒ–
    tokens = tokenizer(rgb_images, depth_images)

    print(f"âœ… è¾“å…¥: RGB {rgb_images.shape}, Depth {depth_images.shape}")
    print(f"âœ… è¾“å‡º: Tokens {tokens.shape}")
    print(f"âœ… é¢„æœŸ: [2, 1176, 512] (3ç›¸æœºRGB + 3ç›¸æœºDepth, æ¯ç›¸æœº196patches)")

    assert tokens.shape == (
        batch_size, 1176, 512), f"Shape mismatch: {tokens.shape}"
    print("âœ… VisionTokenizeræµ‹è¯•é€šè¿‡!")

    return True


def test_state_tokenizer():
    """æµ‹è¯•StateTokenizer"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•StateTokenizer")
    print("=" * 70)

    tokenizer = StateTokenizer(embed_dim=512)

    # æ¨¡æ‹Ÿ16ç»´çŠ¶æ€
    batch_size = 2
    state_dim = 16
    state = torch.randn(batch_size, state_dim)

    # å…³èŠ‚é…ç½®ï¼ˆ16ç»´æ‰‹è‡‚é…ç½®ï¼‰
    joint_configs = [
        {'idx': i, 'type': ['shoulder', 'shoulder', 'shoulder', 'elbow', 'wrist', 'wrist', 'wrist', 'gripper',
                            'shoulder', 'shoulder', 'shoulder', 'elbow', 'wrist', 'wrist', 'wrist', 'gripper'][i],
         'side': 0 if i < 8 else 1, 'id': i, 'name': f'joint_{i}'}
        for i in range(state_dim)
    ]

    # TokenåŒ–
    tokens = tokenizer(state, joint_configs)

    print(f"âœ… è¾“å…¥: State {state.shape}")
    print(f"âœ… è¾“å‡º: Tokens {tokens.shape}")
    print(f"âœ… é¢„æœŸ: [2, 16, 512]")

    assert tokens.shape == (batch_size, state_dim,
                            512), f"Shape mismatch: {tokens.shape}"
    print("âœ… StateTokenizeræµ‹è¯•é€šè¿‡!")

    return True


def test_action_tokenizer():
    """æµ‹è¯•ActionTokenizer"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•ActionTokenizer")
    print("=" * 70)

    action_dim = 16
    horizon = 8
    tokenizer = ActionTokenizer(
        action_dim=action_dim, horizon=horizon, embed_dim=512)

    # æ¨¡æ‹ŸåŠ¨ä½œåºåˆ—
    batch_size = 2
    actions = torch.randn(batch_size, horizon, action_dim)

    # TokenåŒ–
    action_tokens = tokenizer.tokenize(actions)
    print(f"âœ… è¾“å…¥: Actions {actions.shape}")
    print(f"âœ… tokenizeè¾“å‡º: Tokens {action_tokens.shape}")
    print(f"âœ… é¢„æœŸ: [2, 8, 512]")

    assert action_tokens.shape == (
        batch_size, horizon, 512), f"Shape mismatch: {action_tokens.shape}"

    # åTokenåŒ–
    decoded_actions = tokenizer.detokenize(action_tokens)
    print(f"âœ… detokenizeè¾“å‡º: Actions {decoded_actions.shape}")
    print(f"âœ… é¢„æœŸ: [2, 8, 16]")

    assert decoded_actions.shape == (
        batch_size, horizon, action_dim), f"Shape mismatch: {decoded_actions.shape}"
    print("âœ… ActionTokenizeræµ‹è¯•é€šè¿‡!")

    return True


def test_diffusion_decoder():
    """æµ‹è¯•DiffusionDecoder"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•DiffusionDecoder")
    print("=" * 70)

    action_dim = 16
    horizon = 8
    decoder = DiffusionDecoder(
        action_dim=action_dim,
        horizon=horizon,
        context_dim=512,
        num_train_timesteps=100
    )

    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    target_actions = torch.randn(batch_size, horizon, action_dim)
    context_tokens = torch.randn(batch_size, 100, 512)  # å‡è®¾100ä¸ªcontext tokens

    # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—loss
    loss = decoder.compute_loss(target_actions, context_tokens)
    print(f"âœ… è®­ç»ƒloss: {loss.item():.4f}")
    assert loss.ndim == 0, "Loss should be a scalar"

    # æ¨ç†æ¨¡å¼ï¼šé‡‡æ ·
    sampled_actions = decoder.sample(context_tokens, num_inference_steps=10)
    print(f"âœ… é‡‡æ ·è¾“å‡º: Actions {sampled_actions.shape}")
    print(f"âœ… é¢„æœŸ: [2, 8, 16]")

    assert sampled_actions.shape == (
        batch_size, horizon, action_dim), f"Shape mismatch: {sampled_actions.shape}"
    print("âœ… DiffusionDecoderæµ‹è¯•é€šè¿‡!")

    return True


def test_integration():
    """é›†æˆæµ‹è¯•ï¼šå®Œæ•´çš„forwardæµç¨‹"""
    print("\n" + "=" * 70)
    print("é›†æˆæµ‹è¯•ï¼šå®Œæ•´Forwardæµç¨‹")
    print("=" * 70)

    # åˆ›å»ºæ‰€æœ‰ç»„ä»¶
    vision_tokenizer = VisionTokenizer(
        patch_size=16, embed_dim=512, image_size=224)
    state_tokenizer = StateTokenizer(embed_dim=512)

    # Transformer Encoder
    import torch.nn as nn
    encoder_layer = nn.TransformerEncoderLayer(
        d_model=512, nhead=8, dim_feedforward=2048, batch_first=True
    )
    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

    # Diffusion Decoder
    diffusion_decoder = DiffusionDecoder(
        action_dim=16, horizon=8, context_dim=512, num_train_timesteps=100
    )

    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    rgb_images = torch.randn(batch_size, 1, 3, 224, 224)
    state = torch.randn(batch_size, 16)
    target_actions = torch.randn(batch_size, 8, 16)

    joint_configs = [
        {'idx': i, 'type': ['shoulder', 'elbow', 'wrist', 'gripper'] * 4,
         'side': 0 if i < 8 else 1, 'id': i, 'name': f'joint_{i}'}
        for i in range(16)
    ]

    # Forwardæµç¨‹
    print("1ï¸âƒ£  Vision tokenization...")
    vision_tokens = vision_tokenizer(rgb_images, None)
    print(f"   Vision tokens: {vision_tokens.shape}")

    print("2ï¸âƒ£  State tokenization...")
    state_tokens = state_tokenizer(state, joint_configs)
    print(f"   State tokens: {state_tokens.shape}")

    print("3ï¸âƒ£  Concatenate tokens...")
    all_tokens = torch.cat([vision_tokens, state_tokens], dim=1)
    print(f"   All tokens: {all_tokens.shape}")

    print("4ï¸âƒ£  Transformer encoding...")
    context_tokens = transformer_encoder(all_tokens)
    print(f"   Context tokens: {context_tokens.shape}")

    print("5ï¸âƒ£  Diffusion loss computation...")
    loss = diffusion_decoder.compute_loss(target_actions, context_tokens)
    print(f"   Loss: {loss.item():.4f}")

    print("6ï¸âƒ£  Diffusion sampling...")
    sampled_actions = diffusion_decoder.sample(
        context_tokens, num_inference_steps=5)
    print(f"   Sampled actions: {sampled_actions.shape}")

    print("âœ… é›†æˆæµ‹è¯•é€šè¿‡!")

    return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("ğŸ§ª VLAæ¨¡å—æµ‹è¯•å¥—ä»¶")
    print("=" * 70)

    tests = [
        ("VisionTokenizer", test_vision_tokenizer),
        ("StateTokenizer", test_state_tokenizer),
        ("ActionTokenizer", test_action_tokenizer),
        ("DiffusionDecoder", test_diffusion_decoder),
        ("Integration", test_integration),
    ]

    results = []
    for name, test_func in tests:
        try:
            success = test_func()
            results.append((name, success))
        except Exception as e:
            print(f"âŒ {name}æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)

    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name:20s} {status}")

    all_passed = all(success for _, success in results)

    print("=" * 70)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    print("=" * 70)

    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
