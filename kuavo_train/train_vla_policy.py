#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLA Transformer Policyè®­ç»ƒè„šæœ¬

ä½¿ç”¨TokenåŒ–æ¶æ„è®­ç»ƒç°ä»£åŒ–çš„VLAç­–ç•¥
æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶çµæ´»å®šä¹‰è¾“å…¥ç»´åº¦

ä½¿ç”¨æ–¹æ³•ï¼š
python kuavo_train/train_vla_policy.py --config-name=vla_config
"""

# Ensure custom patches are applied
import lerobot_patches.custom_patches

import os
import hydra
from omegaconf import DictConfig
from pathlib import Path
from functools import partial

import torch
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader
from tqdm import tqdm
import shutil
from hydra.utils import instantiate
from diffusers.optimization import get_scheduler
import logging

from lerobot.configs.types import FeatureType
from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.utils.random_utils import set_seed

# å¯¼å…¥VLAæ¨¡å—
from kuavo_train.wrapper.policy.vla.VLAPolicyWrapper import VLAPolicyWrapper
from kuavo_train.wrapper.policy.vla.VLAConfigWrapper import VLAConfigWrapper

# å¯¼å…¥å·¥å…·å‡½æ•°
from kuavo_train.utils.augmenter import crop_image, resize_image
from kuavo_train.utils.utils import save_rng_state, load_rng_state
from kuavo_train.utils.transforms import ImageTransforms, ImageTransformsConfig, ImageTransformConfig


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('vla_training.log', encoding='utf-8')
        ]
    )
    return logging.getLogger("VLATraining")


def build_augmenter(cfg):
    """æ„å»ºå›¾åƒå¢å¼ºå™¨"""
    img_tf_cfg = ImageTransformsConfig(
        enable=cfg.get("enable", False),
        max_num_transforms=cfg.get("max_num_transforms", 3),
        random_order=cfg.get("random_order", False),
        tfs={}
    )

    if "tfs" in cfg:
        for name, tf_dict in cfg["tfs"].items():
            img_tf_cfg.tfs[name] = ImageTransformConfig(
                weight=tf_dict.get("weight", 1.0),
                type=tf_dict.get("type", "Identity"),
                kwargs=tf_dict.get("kwargs", {}),
            )
    return ImageTransforms(img_tf_cfg)


def build_delta_timestamps(dataset_metadata, policy_cfg):
    """æ„å»ºdelta timestamps"""
    obs_indices = getattr(policy_cfg, "observation_delta_indices", None)
    act_indices = getattr(policy_cfg, "action_delta_indices", None)
    if obs_indices is None and act_indices is None:
        return None

    delta_timestamps = {}
    for key in dataset_metadata.info["features"]:
        if "observation" in key and obs_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in obs_indices]
        elif "action" in key and act_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in act_indices]

    return delta_timestamps if delta_timestamps else None


def build_policy_config(cfg, input_features, output_features):
    """æ„å»ºpolicyé…ç½®"""
    from lerobot.configs.policies import PolicyFeature

    def _normalize_feature_dict(d):
        if isinstance(d, DictConfig):
            from omegaconf import OmegaConf
            d = OmegaConf.to_container(d, resolve=True)
        if not isinstance(d, dict):
            raise TypeError(f"Expected dict or DictConfig, got {type(d)}")

        return {
            k: PolicyFeature(**v) if isinstance(v,
                                                dict) and not isinstance(v, PolicyFeature) else v
            for k, v in d.items()
        }

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=cfg.training.device,
    )

    policy_cfg.input_features = _normalize_feature_dict(
        policy_cfg.input_features)
    policy_cfg.output_features = _normalize_feature_dict(
        policy_cfg.output_features)

    return policy_cfg


def build_optimizer_and_scheduler(policy, cfg, total_frames):
    """æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())

    if cfg.training.max_training_step is None:
        updates_per_epoch = (
            total_frames // (cfg.training.batch_size * cfg.training.accumulation_steps)) + 1
        num_training_steps = cfg.training.max_epoch * updates_per_epoch
    else:
        num_training_steps = cfg.training.max_training_step

    lr_scheduler = policy.config.get_scheduler_preset()
    if lr_scheduler is not None:
        lr_scheduler = lr_scheduler.build(optimizer, num_training_steps)
    else:
        lr_scheduler = get_scheduler(
            name=cfg.training.scheduler_name,
            optimizer=optimizer,
            num_warmup_steps=cfg.training.scheduler_warmup_steps,
            num_training_steps=num_training_steps,
        )

    return optimizer, lr_scheduler


@hydra.main(config_path="../configs/policy/", config_name="vla_config", version_base=None)
def main(cfg: DictConfig):
    """VLAè®­ç»ƒä¸»å‡½æ•°"""
    logger = setup_logging()
    set_seed(cfg.training.seed)

    print("ğŸ¤– VLA Transformer Policyè®­ç»ƒ")
    print("=" * 70)
    print(f"ä»»åŠ¡: {cfg.task}")
    print(f"æ–¹æ³•: {cfg.method}")
    print(f"Tokenç»´åº¦: {cfg.policy.token_embed_dim}")
    print(f"Transformeræ·±åº¦: {cfg.policy.transformer_depth}")

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_directory = Path(
        cfg.training.output_directory) / f"run_{cfg.timestamp}"
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    device = torch.device(cfg.training.device)

    # ==================== åŠ è½½æ•°æ®é›† ====================
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset_metadata = LeRobotDatasetMetadata(cfg.repoid, root=cfg.root)
    print(f"Camera keys: {dataset_metadata.camera_keys}")
    print(f"Dataset features: {list(dataset_metadata.features.keys())}")

    # æ„å»ºç‰¹å¾
    features = dataset_to_policy_features(dataset_metadata.features)
    input_features = {k: ft for k, ft in features.items(
    ) if ft.type is not FeatureType.ACTION}
    output_features = {k: ft for k,
                       ft in features.items() if ft.type is FeatureType.ACTION}

    # Episodeé™åˆ¶
    episodes_to_use = getattr(cfg, 'episodes_to_use', None)
    if episodes_to_use is not None:
        if isinstance(episodes_to_use, int):
            episodes_to_use = list(range(episodes_to_use))
        elif hasattr(episodes_to_use, '__len__') and len(episodes_to_use) == 2:
            start, end = int(episodes_to_use[0]), int(episodes_to_use[1])
            episodes_to_use = list(range(start, end + 1))
            print(
                f"ä½¿ç”¨episodesèŒƒå›´ [{start}, {end}]: {len(episodes_to_use)}ä¸ªepisodes")
        elif hasattr(episodes_to_use, '__iter__'):
            episodes_to_use = list(episodes_to_use)
        print(f"ä½¿ç”¨é™å®šepisodes: {len(episodes_to_use)}ä¸ª")
    else:
        print("ä½¿ç”¨å…¨éƒ¨episodes")

    # æ„å»ºpolicyé…ç½®
    policy_cfg = build_policy_config(cfg, input_features, output_features)

    # æ„å»ºå›¾åƒå¢å¼ºå™¨
    image_transforms = build_augmenter(cfg.training.RGB_Augmenter)

    # åŠ è½½æ•°æ®é›†
    delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)
    dataset = LeRobotDataset(
        cfg.repoid,
        delta_timestamps=delta_timestamps,
        root=cfg.root,
        episodes=episodes_to_use,
        image_transforms=image_transforms,
    )

    total_frames = dataset_metadata.info["total_frames"]
    dataset_stats = dataset_metadata.stats

    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {total_frames} frames")

    # ==================== æ„å»ºPolicy ====================
    print("ğŸ—ï¸  æ„å»ºVLA Policy...")
    policy = VLAPolicyWrapper(policy_cfg, dataset_stats)
    policy.train().to(device)

    print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in policy.parameters()):,}")

    # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, lr_scheduler = build_optimizer_and_scheduler(
        policy, cfg, total_frames)

    # AMPæ”¯æŒ
    amp_requested = bool(getattr(cfg.policy, "use_amp", False))
    amp_enabled = amp_requested and device.type == "cuda"

    if hasattr(torch, "amp"):
        scaler = torch.amp.GradScaler(device=device.type, enabled=amp_enabled)
    else:
        scaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)

    print(f"ä½¿ç”¨AMP: {amp_enabled}")

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    steps = 0
    best_loss = float('inf')

    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
    if cfg.training.resume and cfg.training.resume_timestamp:
        resume_path = Path(cfg.training.output_directory) / \
            cfg.training.resume_timestamp
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_path}")
        try:
            load_rng_state(resume_path / "rng_state.pth")
            policy = policy.from_pretrained(resume_path, strict=True)
            policy = policy.to(device)

            optimizer, lr_scheduler = build_optimizer_and_scheduler(
                policy, cfg, total_frames)

            checkpoint_file = resume_path / "learning_state.pth"
            if checkpoint_file.exists():
                checkpoint = torch.load(checkpoint_file, map_location=device)
                optimizer.load_state_dict(checkpoint["optimizer"])

                if "lr_scheduler" in checkpoint:
                    lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
                if "scaler" in checkpoint and amp_enabled:
                    scaler.load_state_dict(checkpoint["scaler"])
                if "steps" in checkpoint:
                    steps = checkpoint["steps"]
                if "epoch" in checkpoint:
                    start_epoch = checkpoint["epoch"]
                if "best_loss" in checkpoint:
                    best_loss = checkpoint["best_loss"]

            for file in resume_path.glob("events.*"):
                shutil.copy(file, output_directory)

            print(f"å·²æ¢å¤è®­ç»ƒä»epoch {start_epoch}, step {steps}")
        except Exception as e:
            print(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return
    else:
        print("ä»å¤´å¼€å§‹è®­ç»ƒ!")

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

    from contextlib import nullcontext
    has_torch_autocast = hasattr(torch, "autocast")

    def make_autocast(enabled: bool):
        if not enabled:
            return nullcontext()
        if device.type == "cuda":
            if has_torch_autocast:
                return torch.autocast(device_type="cuda")
            else:
                from torch.cuda.amp import autocast as cuda_autocast
                return cuda_autocast()
        return nullcontext()

    for epoch in range(start_epoch, cfg.training.max_epoch):
        dataloader = DataLoader(
            dataset,
            num_workers=cfg.training.num_workers,
            batch_size=cfg.training.batch_size,
            shuffle=True,
            pin_memory=(device.type != "cpu"),
            drop_last=cfg.training.drop_last,
            prefetch_factor=1,
        )

        epoch_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch+1}/{cfg.training.max_epoch}",
            dynamic_ncols=True,
            leave=False
        )

        total_loss = 0.0
        for batch in epoch_bar:
            # å°†batchç§»åˆ°device
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            # å‰å‘ä¼ æ’­
            with make_autocast(amp_enabled):
                loss, _ = policy.forward(batch)

            scaled_loss = loss / cfg.training.accumulation_steps

            # åå‘ä¼ æ’­
            if amp_enabled:
                scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            steps += 1  # å…ˆå¢åŠ æ­¥æ•°è®¡æ•°å™¨

            # ä¼˜åŒ–å™¨æ›´æ–°
            if steps % cfg.training.accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
                if amp_enabled:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)
                    optimizer.step()
                optimizer.zero_grad()
                lr_scheduler.step()

            # è®°å½•æ—¥å¿—
            if steps % cfg.training.log_freq == 0:
                writer.add_scalar("train/loss", scaled_loss.item(), steps)
                writer.add_scalar(
                    "train/lr", lr_scheduler.get_last_lr()[0], steps)

                epoch_bar.set_postfix(
                    loss=f"{scaled_loss.item():.4f}",
                    step=steps,
                    lr=f"{lr_scheduler.get_last_lr()[0]:.2e}"
                )

            total_loss += scaled_loss.item()

        # Epochç»“æŸç»Ÿè®¡
        avg_loss = total_loss / len(dataloader)
        print(f"\nğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

        # æ›´æ–°æœ€ä½³æŸå¤±
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            save_rng_state(best_path / "rng_state.pth")
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: loss={best_loss:.4f}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            save_rng_state(epoch_path / "rng_state.pth")

            # ä¿å­˜è®­ç»ƒçŠ¶æ€
            checkpoint = {
                "optimizer": optimizer.state_dict(),
                "lr_scheduler": lr_scheduler.state_dict(),
                "scaler": scaler.state_dict() if amp_enabled else None,
                "steps": steps,
                "epoch": epoch + 1,
                "best_loss": best_loss,
            }
            torch.save(checkpoint, epoch_path / "learning_state.pth")

            print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: epoch {epoch+1}")

    writer.close()

    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 70)
    print(f"æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {output_directory}")


if __name__ == "__main__":
    main()
