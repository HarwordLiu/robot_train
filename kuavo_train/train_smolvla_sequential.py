#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SmolVLAé¡ºåºå¤šä»»åŠ¡è®­ç»ƒè„šæœ¬

å®ç°SmolVLAçš„é¡ºåºFine-tuningç­–ç•¥ï¼š
- Stage 1: é¢„è®­ç»ƒ â†’ ä»»åŠ¡1æ¨¡å‹
- Stage 2: ä»»åŠ¡1æ¨¡å‹ â†’ ä»»åŠ¡2æ¨¡å‹
- Stage 3: ä»»åŠ¡2æ¨¡å‹ â†’ ä»»åŠ¡3æ¨¡å‹
- Stage 4: ä»»åŠ¡3æ¨¡å‹ â†’ ä»»åŠ¡4æ¨¡å‹ï¼ˆæœ€ç»ˆå¤šä»»åŠ¡æ¨¡å‹ï¼‰

é˜²é—å¿˜æŠ€æœ¯ï¼š
- Replay Buffer: æ··åˆä¹‹å‰ä»»åŠ¡çš„æ•°æ®
- Lower Learning Rate: é€æ­¥é™ä½å­¦ä¹ ç‡
- Freeze Layers: å†»ç»“VLMéƒ¨åˆ†å±‚
- Multi-task Validation: éªŒè¯æ‰€æœ‰ä¹‹å‰ä»»åŠ¡

ä½¿ç”¨æ–¹æ³•ï¼š
    # è®­ç»ƒä»»åŠ¡1
    python kuavo_train/train_smolvla_sequential.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task1_moving_grasp

    # è®­ç»ƒä»»åŠ¡2ï¼ˆè‡ªåŠ¨ä»ä»»åŠ¡1ç»§ç»­ï¼‰
    python kuavo_train/train_smolvla_sequential.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task2_weighing
"""

# Ensure custom patches are applied
import lerobot_patches.custom_patches

import os
import hydra
from omegaconf import DictConfig, OmegaConf
from pathlib import Path
from functools import partial
import json
from typing import Optional, Dict, Any

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.utils.random_utils import set_seed
from lerobot.configs.types import FeatureType

# å¯¼å…¥SmolVLAæ¨¡å—
from kuavo_train.wrapper.policy.smolvla.SmolVLAPolicyWrapper import SmolVLAPolicyWrapper
from kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper import SmolVLAConfigWrapper


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(
                'smolvla_sequential_training.log', encoding='utf-8')
        ]
    )
    return logging.getLogger("SmolVLASequentialTraining")


def load_task_config(cfg_root: Path, task_id: int) -> DictConfig:
    """
    åŠ è½½æŒ‡å®šä»»åŠ¡çš„é…ç½®

    Args:
        cfg_root: é…ç½®æ–‡ä»¶æ ¹ç›®å½•
        task_id: ä»»åŠ¡ID (1-4)

    Returns:
        ä»»åŠ¡é…ç½®å¯¹è±¡
    """
    task_files = {
        1: "task1_moving_grasp.yaml",
        2: "task2_weighing.yaml",
        3: "task3_placement.yaml",
        4: "task4_sorting.yaml",
    }

    task_file = cfg_root / "tasks" / task_files[task_id]
    if not task_file.exists():
        raise FileNotFoundError(f"Task config file not found: {task_file}")

    task_cfg = OmegaConf.load(task_file)
    return task_cfg


class ReplayDatasetManager:
    """
    ç®¡ç†Replay Bufferçš„ç±»

    åœ¨è®­ç»ƒä»»åŠ¡Næ—¶ï¼Œæ··åˆä¹‹å‰ä»»åŠ¡1åˆ°N-1çš„æ•°æ®ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜
    """

    def __init__(self, cfg: DictConfig, current_task_id: int, cfg_root: Path):
        self.cfg = cfg
        self.current_task_id = current_task_id
        self.cfg_root = cfg_root
        self.replay_datasets = {}  # task_id -> dataset
        self.replay_weights = {}   # task_id -> weight

    def load_replay_tasks(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦replayçš„ä»»åŠ¡æ•°æ®"""
        if self.current_task_id == 1:
            # ä»»åŠ¡1ä¸éœ€è¦replay
            return {}, {}

        # è·å–å½“å‰stageçš„replayé…ç½®
        stage_key = f"stage{self.current_task_id}_replay"
        replay_config = self.cfg.sequential.get(stage_key, {})

        if not replay_config:
            print(
                f"âš ï¸  No replay config found for stage {self.current_task_id}")
            return {}, {}

        print(f"\nğŸ“¦ Loading Replay Buffer for Stage {self.current_task_id}")
        print("="*70)

        for task_key, weight in replay_config.items():
            if 'task' in task_key:
                task_id = int(task_key.replace('task', ''))

                # åªåŠ è½½ä¹‹å‰çš„ä»»åŠ¡
                if task_id < self.current_task_id:
                    print(
                        f"  Loading Task {task_id} (weight: {weight:.1%})...")

                    # åŠ è½½ä»»åŠ¡é…ç½®
                    task_cfg = load_task_config(self.cfg_root, task_id)

                    # åŠ è½½æ•°æ®é›†
                    dataset = LeRobotDataset(
                        task_cfg.task.data.repoid,
                        root=task_cfg.task.data.root,
                        episodes=list(range(
                            task_cfg.task.data.episodes_to_use[0],
                            task_cfg.task.data.episodes_to_use[1] + 1
                        ))
                    )

                    self.replay_datasets[task_id] = dataset
                    self.replay_weights[task_id] = weight

                    print(
                        f"    âœ… Loaded {len(dataset)} frames from Task {task_id}")

        print("="*70 + "\n")
        return self.replay_datasets, self.replay_weights


def create_dataloader_with_language(
    dataset: LeRobotDataset,
    language_instruction: str,
    batch_size: int,
    num_workers: int,
    pin_memory: bool = True,
    drop_last: bool = False,
    target_action_dim: int = 16
) -> DataLoader:
    """
    åˆ›å»ºåŒ…å«language instructionçš„DataLoader

    Args:
        dataset: LeRobotæ•°æ®é›†
        language_instruction: ä»»åŠ¡çš„language instruction
        batch_size: batchå¤§å°
        num_workers: workeræ•°é‡
        pin_memory: æ˜¯å¦pin memory
        drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªbatch

    Returns:
        DataLoader
    """

    def collate_fn_with_language(batch):
        """ä¸ºbatchæ·»åŠ language instructionå¹¶é€‚é…åŠ¨ä½œç»´åº¦"""
        # ä½¿ç”¨é»˜è®¤collate
        from torch.utils.data._utils.collate import default_collate
        batch_dict = default_collate(batch)

        # æ·»åŠ taskå­—æ®µ
        batch_size = batch_dict[list(batch_dict.keys())[0]].shape[0]
        batch_dict['task'] = [language_instruction] * batch_size

        # é€‚é…åŠ¨ä½œç»´åº¦ï¼šå¦‚æœåŠ¨ä½œæ˜¯16ç»´ä½†éœ€è¦32ç»´ï¼Œè¿›è¡Œå¡«å……
        for key, value in batch_dict.items():
            if isinstance(value, torch.Tensor) and 'action' in key.lower():
                if value.shape[-1] == 16 and target_action_dim > 16:
                    # å¡«å……åŠ¨ä½œç»´åº¦
                    if value.shape[-1] < target_action_dim:
                        padding_size = target_action_dim - value.shape[-1]
                        padding = torch.zeros(
                            *value.shape[:-1], padding_size, dtype=value.dtype)
                        batch_dict[key] = torch.cat([value, padding], dim=-1)

        return batch_dict

    return DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
        pin_memory=pin_memory,
        drop_last=drop_last,
        collate_fn=collate_fn_with_language,
        prefetch_factor=1
    )


def create_mixed_dataloader(
    cfg: DictConfig,
    task_cfg: DictConfig,
    replay_manager: Optional[ReplayDatasetManager] = None
) -> DataLoader:
    """
    åˆ›å»ºæ··åˆäº†replayæ•°æ®çš„DataLoader

    Args:
        cfg: åŸºç¡€é…ç½®
        task_cfg: å½“å‰ä»»åŠ¡é…ç½®
        replay_manager: Replayæ•°æ®ç®¡ç†å™¨

    Returns:
        æ··åˆæ•°æ®çš„DataLoader
    """
    task_id = task_cfg.task.id
    language_instruction = task_cfg.task.language_instruction

    # å½“å‰ä»»åŠ¡æ•°æ®é›†
    current_dataset = LeRobotDataset(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root,
        episodes=list(range(
            task_cfg.task.data.episodes_to_use[0],
            task_cfg.task.data.episodes_to_use[1] + 1
        ))
    )

    print(f"ğŸ“Š Current Task {task_id} Dataset: {len(current_dataset)} frames")

    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡æˆ–ä¸ä½¿ç”¨replayï¼Œç›´æ¥è¿”å›
    if task_id == 1 or not cfg.sequential.use_replay_buffer:
        return create_dataloader_with_language(
            current_dataset,
            language_instruction,
            cfg.training.batch_size,
            cfg.training.num_workers,
            pin_memory=(cfg.training.device != 'cpu'),
            drop_last=cfg.training.drop_last,
            target_action_dim=cfg.training.target_action_dim
        )

    # æ··åˆreplayæ•°æ®
    if replay_manager is None:
        raise ValueError("replay_manager is required for task > 1")

    # åˆ›å»ºæ··åˆæ•°æ®é›†
    # æ³¨æ„ï¼šæ¯ä¸ªä»»åŠ¡éœ€è¦è‡ªå·±çš„language instruction
    all_datasets = [(current_dataset, language_instruction)]

    for replay_task_id, replay_dataset in replay_manager.replay_datasets.items():
        replay_task_cfg = load_task_config(
            Path(cfg.hydra.run.dir).parent.parent.parent / "configs/policy", replay_task_id)
        replay_language = replay_task_cfg.task.language_instruction
        all_datasets.append((replay_dataset, replay_language))
        print(
            f"ğŸ“¦ Adding Task {replay_task_id} replay: {len(replay_dataset)} frames")

    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºå•ç‹¬çš„dataloaderï¼Œç„¶åè½®æµé‡‡æ ·
    # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥concatenate datasets
    # æ³¨æ„ï¼šè¿™é‡Œæ¯ä¸ªdatasetéƒ½éœ€è¦ä¿ç•™è‡ªå·±çš„language instruction

    class MixedDataset(torch.utils.data.Dataset):
        """æ··åˆå¤šä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†ä¿ç•™è‡ªå·±çš„language instruction"""

        def __init__(self, datasets_with_language):
            self.datasets_with_language = datasets_with_language
            self.lengths = [len(ds) for ds, _ in datasets_with_language]
            self.total_length = sum(self.lengths)

            # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„é‡‡æ ·æ¦‚ç‡ï¼ˆåŸºäºreplay weightsï¼‰
            stage_key = f"stage{task_id}_replay"
            replay_config = cfg.sequential.get(stage_key, {})

            self.weights = []
            for i, (ds, _) in enumerate(datasets_with_language):
                if i == 0:
                    # å½“å‰ä»»åŠ¡çš„weight
                    task_key = f"task{task_id}"
                    weight = replay_config.get(task_key, 1.0)
                else:
                    # Replayä»»åŠ¡çš„weight
                    task_key = f"task{i}"  # iå¯¹åº”replay_task_id
                    weight = replay_config.get(task_key, 0.1)
                self.weights.append(weight)

            # å½’ä¸€åŒ–weights
            total_weight = sum(self.weights)
            self.weights = [w / total_weight for w in self.weights]

        def __len__(self):
            return self.total_length

        def __getitem__(self, idx):
            # æ ¹æ®weightséšæœºé€‰æ‹©ä¸€ä¸ªdataset
            import random
            dataset_idx = random.choices(
                range(len(self.datasets_with_language)), weights=self.weights, k=1)[0]
            dataset, language = self.datasets_with_language[dataset_idx]

            # ä»è¯¥datasetéšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
            sample_idx = random.randint(0, len(dataset) - 1)
            sample = dataset[sample_idx]

            # æ·»åŠ language instruction
            sample['task'] = language

            return sample

    mixed_dataset = MixedDataset(all_datasets)

    print(f"ğŸ“Š Mixed Dataset: {len(mixed_dataset)} frames (with replay)")
    print(f"   Weights: {mixed_dataset.weights}")

    return DataLoader(
        mixed_dataset,
        batch_size=cfg.training.batch_size,
        num_workers=cfg.training.num_workers,
        shuffle=True,
        pin_memory=(cfg.training.device != 'cpu'),
        drop_last=cfg.training.drop_last,
        prefetch_factor=1
    )


def validate_all_tasks(
    policy: SmolVLAPolicyWrapper,
    cfg: DictConfig,
    current_task_id: int,
    device: torch.device,
    cfg_root: Path
) -> Dict[int, float]:
    """
    éªŒè¯æ‰€æœ‰ä¹‹å‰çš„ä»»åŠ¡ï¼ˆæ£€æµ‹é—å¿˜ï¼‰

    Args:
        policy: SmolVLAç­–ç•¥
        cfg: é…ç½®
        current_task_id: å½“å‰ä»»åŠ¡ID
        device: è®¾å¤‡
        cfg_root: é…ç½®æ ¹ç›®å½•

    Returns:
        validation_results: {task_id: avg_loss}
    """
    print("\n" + "="*70)
    print(f"ğŸ” Multi-Task Validation (Tasks 1-{current_task_id})")
    print("="*70)

    policy.eval()
    validation_results = {}

    for task_id in range(1, current_task_id + 1):
        print(f"\nğŸ“Š Validating Task {task_id}...")

        # åŠ è½½ä»»åŠ¡é…ç½®
        task_cfg = load_task_config(cfg_root, task_id)

        # åŠ è½½éªŒè¯é›†ï¼ˆä½¿ç”¨æœ€åNä¸ªepisodesï¼‰
        num_val_episodes = cfg.training.validation_episodes
        total_episodes = task_cfg.task.data.episodes_to_use[1] + 1
        val_start = max(0, total_episodes - num_val_episodes)

        val_dataset = LeRobotDataset(
            task_cfg.task.data.repoid,
            root=task_cfg.task.data.root,
            episodes=list(range(val_start, total_episodes))
        )

        val_loader = create_dataloader_with_language(
            val_dataset,
            task_cfg.task.language_instruction,
            batch_size=cfg.training.batch_size,
            num_workers=cfg.training.num_workers // 2,
            pin_memory=(device.type != 'cpu'),
            drop_last=False,
            target_action_dim=cfg.training.target_action_dim
        )

        # éªŒè¯
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Task {task_id} Validation", leave=False):
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                         for k, v in batch.items()}

                loss, _ = policy.forward(batch)
                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / \
            num_batches if num_batches > 0 else float('inf')
        validation_results[task_id] = avg_loss

        print(f"  Task {task_id} Validation Loss: {avg_loss:.4f}")

    # åˆ†æé—å¿˜æƒ…å†µ
    if current_task_id > 1:
        print("\nâš ï¸  Forgetting Analysis:")
        for task_id in range(1, current_task_id):
            loss = validation_results[task_id]
            # ç®€å•çš„é˜ˆå€¼åˆ¤æ–­
            if loss < 0.7:
                status = "âœ… Well Retained"
            elif loss < 1.0:
                status = "âš ï¸  Slight Degradation"
            else:
                status = "âŒ Significant Forgetting"

            print(f"  Task {task_id}: {status} (loss={loss:.4f})")

    print("="*70 + "\n")

    policy.train()
    return validation_results


@hydra.main(config_path="../configs/policy/", config_name="smolvla_sequential_base", version_base=None)
def main(cfg: DictConfig):
    """ä¸»è®­ç»ƒæµç¨‹"""

    # è®¾ç½® HuggingFace é•œåƒæºä»¥æé«˜ä¸‹è½½é€Ÿåº¦
    import os
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

    # å¯é€‰ï¼šè®¾ç½®å…¶ä»–é•œåƒæº
    # os.environ['HF_ENDPOINT'] = 'https://huggingface.co'  # å®˜æ–¹æº
    # os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'   # ä¸­å›½é•œåƒæº

    logger = setup_logging()
    set_seed(cfg.training.seed)

    # åŠ è½½ä»»åŠ¡é…ç½®
    # ä»Hydraé…ç½®è·å–ä»»åŠ¡åç§°
    task_name = cfg.get('task', 'tasks/task1_moving_grasp')
    if task_name.startswith('tasks/'):
        task_name = task_name.replace('tasks/', '')

    # åŠ¨æ€åŠ è½½ä»»åŠ¡é…ç½®
    cfg_root = Path(__file__).parent.parent / "configs/policy"
    task_cfg = load_task_config(cfg_root, int(
        task_name.split('_')[0].replace('task', '')))
    task_id = task_cfg.task.id
    task_name = task_cfg.task.name

    print("\n" + "="*70)
    print(f"ğŸ¤– SmolVLA Sequential Training - Stage {task_id}")
    print("="*70)
    print(f"Task ID: {task_id}")
    print(f"Task Name: {task_name}")
    print(f"Description: {task_cfg.task.description}")
    print(f"Language: {task_cfg.task.language_instruction}")
    print("="*70 + "\n")

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_directory = Path(cfg.training.output_directory) / \
        f"task{task_id}_{task_name}"
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    device = torch.device(cfg.training.device)

    # ==================== åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯ ====================
    print("ğŸ“‚ Loading Dataset Metadata...")
    dataset_metadata = LeRobotDatasetMetadata(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root
    )

    # æ„å»ºfeatures
    features = dataset_to_policy_features(dataset_metadata.features)
    input_features = {k: ft for k, ft in features.items(
    ) if ft.type is not FeatureType.ACTION}
    output_features = {k: ft for k,
                       ft in features.items() if ft.type is FeatureType.ACTION}

    dataset_stats = dataset_metadata.stats

    # ==================== æ„å»ºPolicyé…ç½® ====================
    from hydra.utils import instantiate

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=device,
    )

    # Override learning rate from task config
    if hasattr(task_cfg.task.training, 'policy'):
        policy_cfg.optimizer_lr = task_cfg.task.training.policy.optimizer_lr
        policy_cfg.scheduler_warmup_steps = task_cfg.task.training.policy.scheduler_warmup_steps
        policy_cfg.scheduler_decay_steps = task_cfg.task.training.policy.scheduler_decay_steps

    # ==================== åŠ è½½/åˆ›å»ºæ¨¡å‹ ====================
    if task_cfg.task.training.resume_from == 'pretrained':
        # Stage 1: ä»HuggingFaceé¢„è®­ç»ƒåŠ è½½
        print(
            f"\nğŸ“‚ Loading pretrained SmolVLA from {task_cfg.task.training.pretrained_path}")
        policy = SmolVLAPolicyWrapper.from_pretrained(
            task_cfg.task.training.pretrained_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )

    elif task_cfg.task.training.resume_from == 'task':
        # Stage 2+: ä»ä¸Šä¸€ä¸ªä»»åŠ¡ç»§ç»­
        prev_task_id = task_cfg.task.training.resume_task_id
        resume_path = task_cfg.task.training.resume_path

        print(f"\nğŸ“‚ Loading from Task {prev_task_id}: {resume_path}")
        policy = SmolVLAPolicyWrapper.from_pretrained(
            resume_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )
        print(
            f"âœ… Successfully loaded Task {prev_task_id} model for sequential training")

    else:
        # ä»å¤´è®­ç»ƒï¼ˆä¸æ¨èï¼‰
        print("\nâš ï¸  Training from scratch (not recommended for sequential training)")
        policy = SmolVLAPolicyWrapper(policy_cfg, dataset_stats)

    policy = policy.to(device)

    # é€‚é…åŠ¨ä½œç»´åº¦ï¼šå¦‚æœæ•°æ®æ˜¯16ç»´ä½†æ¨¡å‹æ˜¯32ç»´ï¼Œéœ€è¦å¡«å……æ•°æ®
    if policy.config.max_action_dim > 16:
        print(
            f"\nğŸ”§ Adapting action dimensions: Data=16, Model={policy.config.max_action_dim}")
        print("   Padding action data to match model dimensions...")

    policy.train()

    # ==================== å‡†å¤‡æ•°æ® ====================
    # åŠ è½½replay bufferï¼ˆå¦‚æœéœ€è¦ï¼‰
    replay_manager = None
    if task_id > 1 and cfg.sequential.use_replay_buffer:
        cfg_root = Path(__file__).parent.parent / "configs/policy"
        replay_manager = ReplayDatasetManager(cfg, task_id, cfg_root)
        replay_manager.load_replay_tasks()

    # åˆ›å»ºdataloader
    dataloader = create_mixed_dataloader(cfg, task_cfg, replay_manager)

    # ==================== æ„å»ºä¼˜åŒ–å™¨ ====================
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())
    lr_scheduler = policy.config.get_scheduler_preset().build(
        optimizer,
        num_training_steps=task_cfg.task.training.max_epoch * len(dataloader)
    )

    print(f"\nğŸ¯ Training Configuration:")
    print(f"   Epochs: {task_cfg.task.training.max_epoch}")
    print(f"   Batch Size: {cfg.training.batch_size}")
    print(f"   Learning Rate: {policy_cfg.optimizer_lr}")
    print(f"   Steps per Epoch: {len(dataloader)}")
    print(
        f"   Total Steps: {task_cfg.task.training.max_epoch * len(dataloader)}")

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\nğŸš€ Starting Training...")
    print("="*70 + "\n")

    best_loss = float('inf')

    for epoch in range(task_cfg.task.training.max_epoch):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch + 1}/{task_cfg.task.training.max_epoch}")
        print(f"{'='*70}")

        # è®­ç»ƒ
        policy.train()
        total_loss = 0.0
        num_batches = 0

        epoch_bar = tqdm(
            dataloader,
            desc=f"Training Epoch {epoch+1}",
            dynamic_ncols=True,
            leave=False
        )

        for batch in epoch_bar:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in batch.items()}

            # Forward
            loss, _ = policy.forward(batch)

            # Backward
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                policy.parameters(),
                max_norm=policy_cfg.optimizer_grad_clip_norm
            )

            # Optimizer step
            optimizer.step()
            optimizer.zero_grad()
            lr_scheduler.step()

            # Logging
            total_loss += loss.item()
            num_batches += 1

            epoch_bar.set_postfix(
                loss=f"{loss.item():.4f}",
                lr=f"{lr_scheduler.get_last_lr()[0]:.2e}"
            )

        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

        # TensorBoard logging
        writer.add_scalar("train/loss", avg_loss, epoch)
        writer.add_scalar("train/lr", lr_scheduler.get_last_lr()[0], epoch)

        # å¤šä»»åŠ¡éªŒè¯
        if (epoch + 1) % cfg.training.validation_freq_epoch == 0:
            cfg_root = Path(__file__).parent.parent / "configs/policy"
            validation_results = validate_all_tasks(
                policy, cfg, task_id, device, cfg_root)

            # Log validation results
            for val_task_id, val_loss in validation_results.items():
                writer.add_scalar(
                    f"validation/task{val_task_id}_loss", val_loss, epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            print(f"âœ… Best model saved: loss={best_loss:.4f}")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            print(f"âœ… Checkpoint saved: epoch {epoch+1}")

    writer.close()

    # ==================== æœ€ç»ˆéªŒè¯ ====================
    print("\n" + "="*70)
    print("ğŸ¯ Final Multi-Task Validation")
    print("="*70)

    cfg_root = Path(__file__).parent.parent / "configs/policy"
    final_results = validate_all_tasks(policy, cfg, task_id, device, cfg_root)

    # ä¿å­˜è®­ç»ƒç»“æœ
    results_file = output_directory / "training_results.json"
    with open(results_file, 'w') as f:
        json.dump({
            'task_id': task_id,
            'task_name': task_name,
            'description': task_cfg.task.description,
            'language_instruction': task_cfg.task.language_instruction,
            'best_loss': best_loss,
            'final_validation': {str(k): v for k, v in final_results.items()},
            'training_epochs': task_cfg.task.training.max_epoch,
            'learning_rate': policy_cfg.optimizer_lr,
        }, f, indent=2)

    print(f"\n{'='*70}")
    print(f"âœ… Task {task_id} Training Completed!")
    print(f"{'='*70}")
    print(f"Best Loss: {best_loss:.4f}")
    print(f"Model saved to: {output_directory}")
    print(f"Results saved to: {results_file}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
