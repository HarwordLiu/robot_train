#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SmolVLAé¡ºåºå¤šä»»åŠ¡è®­ç»ƒè„šæœ¬

å®ç°SmolVLAçš„é¡ºåºFine-tuningç­–ç•¥ï¼š
- Stage 1: é¢„è®­ç»ƒ â†’ ä»»åŠ¡1æ¨¡å‹
- Stage 2: ä»»åŠ¡1æ¨¡å‹ â†’ ä»»åŠ¡2æ¨¡å‹
- Stage 3: ä»»åŠ¡2æ¨¡å‹ â†’ ä»»åŠ¡3æ¨¡å‹
- Stage 4: ä»»åŠ¡3æ¨¡å‹ â†’ ä»»åŠ¡4æ¨¡å‹ï¼ˆæœ€ç»ˆå¤šä»»åŠ¡æ¨¡å‹ï¼‰

é˜²é—å¿˜æŠ€æœ¯ï¼š
- Replay Buffer: æ··åˆä¹‹å‰ä»»åŠ¡çš„æ•°æ®
- Lower Learning Rate: é€æ­¥é™ä½å­¦ä¹ ç‡
- Freeze Layers: å†»ç»“VLMéƒ¨åˆ†å±‚
- Multi-task Validation: éªŒè¯æ‰€æœ‰ä¹‹å‰ä»»åŠ¡

ä½¿ç”¨æ–¹æ³•ï¼š
    # è®­ç»ƒä»»åŠ¡1
    python kuavo_train/train_smolvla_sequential.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task1_moving_grasp

    # è®­ç»ƒä»»åŠ¡2ï¼ˆè‡ªåŠ¨ä»ä»»åŠ¡1ç»§ç»­ï¼‰
    python kuavo_train/train_smolvla_sequential.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task2_weighing
"""

# Ensure custom patches are applied FIRST before any lerobot imports
import lerobot_patches.custom_patches

import random
from kuavo_train.utils.augmenter import DeterministicAugmenterColor
from kuavo_train.utils.utils import save_rng_state, load_rng_state
from kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper import SmolVLAConfigWrapper
from kuavo_train.wrapper.policy.smolvla.SmolVLAPolicyWrapper import SmolVLAPolicyWrapper
from lerobot.configs.types import FeatureType
from lerobot.utils.random_utils import set_seed
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler
import torch.nn as nn
import torch
from typing import Optional, Dict, Any
import json
from functools import partial
from pathlib import Path
from omegaconf import DictConfig, OmegaConf
import hydra

import os
# æ¶ˆé™¤tokenizers forkè­¦å‘Š
os.environ['TOKENIZERS_PARALLELISM'] = 'false'


# å¯¼å…¥SmolVLAæ¨¡å—

# å¯¼å…¥è®­ç»ƒçŠ¶æ€ä¿å­˜/åŠ è½½å·¥å…·

# å¯¼å…¥æ•°æ®å¢å¼ºå·¥å…·


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(
                'smolvla_sequential_training.log', encoding='utf-8')
        ]
    )
    return logging.getLogger("SmolVLASequentialTraining")


def load_task_config(cfg_root: Path, task_id: int) -> DictConfig:
    """
    åŠ è½½æŒ‡å®šä»»åŠ¡çš„é…ç½®

    Args:
        cfg_root: é…ç½®æ–‡ä»¶æ ¹ç›®å½•
        task_id: ä»»åŠ¡ID (1-4)

    Returns:
        ä»»åŠ¡é…ç½®å¯¹è±¡
    """
    task_files = {
        1: "task1_moving_grasp.yaml",
        2: "task2_weighing.yaml",
        3: "task3_placement.yaml",
        4: "task4_sorting.yaml",
    }

    task_file = cfg_root / "tasks" / task_files[task_id]
    if not task_file.exists():
        raise FileNotFoundError(f"Task config file not found: {task_file}")

    task_cfg = OmegaConf.load(task_file)
    return task_cfg


class ReplayDatasetManager:
    """
    ç®¡ç†Replay Bufferçš„ç±»

    åœ¨è®­ç»ƒä»»åŠ¡Næ—¶ï¼Œæ··åˆä¹‹å‰ä»»åŠ¡1åˆ°N-1çš„æ•°æ®ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜
    """

    def __init__(self, cfg: DictConfig, current_task_id: int, cfg_root: Path, dataset_fps: int):
        self.cfg = cfg
        self.current_task_id = current_task_id
        self.cfg_root = cfg_root
        self.dataset_fps = dataset_fps
        self.replay_datasets = {}  # task_id -> dataset
        self.replay_weights = {}   # task_id -> weight

    def load_replay_tasks(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦replayçš„ä»»åŠ¡æ•°æ®"""
        if self.current_task_id == 1:
            # ä»»åŠ¡1ä¸éœ€è¦replay
            return {}, {}

        # è·å–å½“å‰stageçš„replayé…ç½®
        stage_key = f"stage{self.current_task_id}_replay"
        replay_config = self.cfg.sequential.get(stage_key, {})

        if not replay_config:
            print(
                f"âš ï¸  No replay config found for stage {self.current_task_id}")
            return {}, {}

        print(f"\nğŸ“¦ Loading Replay Buffer for Stage {self.current_task_id}")
        print("="*70)

        # æ„å»ºdelta_timestampsé…ç½® (ç”¨äºåŠ è½½action chunks)
        chunk_size = self.cfg.policy.chunk_size
        delta_timestamps = {
            "observation.state": [0],  # åªå–å½“å‰å¸§
            # æœªæ¥chunk_sizeå¸§
            "action": [i / self.dataset_fps for i in range(chunk_size)],
        }

        for task_key, weight in replay_config.items():
            if 'task' in task_key:
                task_id = int(task_key.replace('task', ''))

                # åªåŠ è½½ä¹‹å‰çš„ä»»åŠ¡
                if task_id < self.current_task_id:
                    print(
                        f"  Loading Task {task_id} (weight: {weight:.1%})...")

                    # åŠ è½½ä»»åŠ¡é…ç½®
                    task_cfg = load_task_config(self.cfg_root, task_id)

                    # åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨delta_timestampsï¼‰
                    dataset = LeRobotDataset(
                        task_cfg.task.data.repoid,
                        root=task_cfg.task.data.root,
                        episodes=list(range(
                            task_cfg.task.data.episodes_to_use[0],
                            task_cfg.task.data.episodes_to_use[1] + 1
                        )),
                        delta_timestamps=delta_timestamps
                    )

                    self.replay_datasets[task_id] = dataset
                    self.replay_weights[task_id] = weight

                    print(
                        f"    âœ… Loaded {len(dataset)} frames from Task {task_id}")

        print("="*70 + "\n")
        return self.replay_datasets, self.replay_weights


def pad_tensor_to_target_dim(tensor, target_dim: int):
    """
    å°†tensoræˆ–numpy arrayä»å®é™…ç»´åº¦å¡«å……åˆ°ç›®æ ‡ç»´åº¦

    Args:
        tensor: è¾“å…¥tensor (torch.Tensoræˆ–numpy.ndarray)ï¼Œå½¢çŠ¶ä¸º [..., actual_dim]
        target_dim: ç›®æ ‡ç»´åº¦

    Returns:
        å¡«å……åçš„tensorï¼Œç±»å‹ä¸è¾“å…¥ç›¸åŒ
    """
    import numpy as np

    actual_dim = tensor.shape[-1]
    if actual_dim == target_dim:
        return tensor
    elif actual_dim < target_dim:
        # å¡«å……0åˆ°ç›®æ ‡ç»´åº¦
        pad_size = target_dim - actual_dim
        pad_shape = list(tensor.shape[:-1]) + [pad_size]

        if isinstance(tensor, torch.Tensor):
            # torch.Tensor: ä½¿ç”¨torch.zeros
            pad_tensor = torch.zeros(
                pad_shape, dtype=tensor.dtype, device=tensor.device)
            return torch.cat([tensor, pad_tensor], dim=-1)
        elif isinstance(tensor, np.ndarray):
            # numpy.ndarray: ä½¿ç”¨np.zeros
            pad_array = np.zeros(pad_shape, dtype=tensor.dtype)
            return np.concatenate([tensor, pad_array], axis=-1)
        else:
            raise TypeError(f"Unsupported tensor type: {type(tensor)}")
    else:
        # æˆªæ–­åˆ°ç›®æ ‡ç»´åº¦ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
        return tensor[..., :target_dim]


def pad_dataset_stats(dataset_stats: Dict[str, Dict],
                      target_action_dim: int = 32,
                      target_state_dim: int = 32) -> Dict[str, Dict]:
    """
    å°†dataset_statsä¸­çš„actionå’Œstateç»Ÿè®¡ä¿¡æ¯å¡«å……åˆ°ç›®æ ‡ç»´åº¦

    å¯¹äºmeanï¼šå¡«å……0
    å¯¹äºstdï¼šå¡«å……1ï¼ˆè¿™æ ·å½’ä¸€åŒ–æ—¶å¡«å……éƒ¨åˆ†ä¸ä¼šè¢«æ”¹å˜ï¼‰

    Args:
        dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å­—å…¸ (å¯ä»¥æ˜¯torch.Tensoræˆ–numpy.ndarray)
        target_action_dim: ç›®æ ‡actionç»´åº¦
        target_state_dim: ç›®æ ‡stateç»´åº¦

    Returns:
        å¡«å……åçš„dataset_stats
    """
    import numpy as np

    def pad_with_ones(tensor, target_dim):
        """å¡«å……1åˆ°ç›®æ ‡ç»´åº¦ï¼ˆç”¨äºstdï¼‰"""
        actual_dim = tensor.shape[-1]
        if actual_dim >= target_dim:
            return tensor

        pad_size = target_dim - actual_dim
        pad_shape = list(tensor.shape[:-1]) + [pad_size]

        if isinstance(tensor, torch.Tensor):
            pad_tensor = torch.ones(
                pad_shape, dtype=tensor.dtype, device=tensor.device)
            return torch.cat([tensor, pad_tensor], dim=-1)
        elif isinstance(tensor, np.ndarray):
            pad_array = np.ones(pad_shape, dtype=tensor.dtype)
            return np.concatenate([tensor, pad_array], axis=-1)
        else:
            raise TypeError(f"Unsupported tensor type: {type(tensor)}")

    padded_stats = {}

    for key, stats_dict in dataset_stats.items():
        if 'action' in key.lower():
            # å¡«å……actionç›¸å…³ç»Ÿè®¡
            padded_stats[key] = {}
            for stat_name, stat_tensor in stats_dict.items():
                if stat_name == 'mean':
                    # meanå¡«å……0
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_action_dim)
                elif stat_name == 'std':
                    # stdå¡«å……1ï¼ˆé¿å…é™¤0ï¼Œä¸”ä¸æ”¹å˜å¡«å……éƒ¨åˆ†çš„å€¼ï¼‰
                    padded_stats[key][stat_name] = pad_with_ones(
                        stat_tensor, target_action_dim)
                else:
                    # å…¶ä»–ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚min, maxï¼‰ä¹Ÿéœ€è¦å¡«å……
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_action_dim)

        elif 'state' in key.lower() or 'observation.state' in key:
            # å¡«å……stateç›¸å…³ç»Ÿè®¡
            padded_stats[key] = {}
            for stat_name, stat_tensor in stats_dict.items():
                if stat_name == 'mean':
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_state_dim)
                elif stat_name == 'std':
                    padded_stats[key][stat_name] = pad_with_ones(
                        stat_tensor, target_state_dim)
                else:
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_state_dim)
        else:
            # ä¸æ˜¯actionæˆ–stateï¼Œç›´æ¥å¤åˆ¶
            padded_stats[key] = stats_dict

    return padded_stats


def create_lerobot_dataset_with_deltas(
    repo_id: str,
    root: str,
    episodes: list,
    delta_timestamps: Dict[str, list]
) -> LeRobotDataset:
    """
    åˆ›å»ºLeRobotDatasetå¹¶é…ç½®delta_timestampsä»¥åŠ è½½action chunks

    Args:
        repo_id: Dataset repository ID
        root: Dataset root path
        episodes: List of episode indices
        delta_timestamps: Delta timestampsé…ç½®ï¼Œä¾‹å¦‚ï¼š
            {
                "observation.state": [0],  # å½“å‰å¸§
                "action": [i/fps for i in range(50)]  # æœªæ¥50å¸§
            }

    Returns:
        é…ç½®å¥½çš„LeRobotDataset
    """
    return LeRobotDataset(
        repo_id,
        root=root,
        episodes=episodes,
        delta_timestamps=delta_timestamps
    )


def create_dataloader_with_language(
    dataset: LeRobotDataset,
    language_instruction: str,
    batch_size: int,
    num_workers: int,
    pin_memory: bool = True,
    drop_last: bool = False,
    target_action_dim: int = 32,
    target_state_dim: int = 32,
    use_augmentation: bool = True,
    augmentation_prob: float = 0.5
) -> DataLoader:
    """
    åˆ›å»ºåŒ…å«language instructionçš„DataLoaderï¼Œå¹¶è‡ªåŠ¨å¡«å……action/stateç»´åº¦

    Args:
        dataset: LeRobotæ•°æ®é›†
        language_instruction: ä»»åŠ¡çš„language instruction
        batch_size: batchå¤§å°
        num_workers: workeræ•°é‡
        pin_memory: æ˜¯å¦pin memory
        drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªbatch
        target_action_dim: ç›®æ ‡actionç»´åº¦ï¼ˆé»˜è®¤32ï¼Œä¸SmolVLAé¢„è®­ç»ƒä¸€è‡´ï¼‰
        target_state_dim: ç›®æ ‡stateç»´åº¦ï¼ˆé»˜è®¤32ï¼Œä¸SmolVLAé¢„è®­ç»ƒä¸€è‡´ï¼‰
        use_augmentation: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
        augmentation_prob: æ•°æ®å¢å¼ºæ¦‚ç‡

    Returns:
        DataLoader
    """

    # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
    augmenter = DeterministicAugmenterColor() if use_augmentation else None

    def collate_fn_with_language(batch):
        """ä¸ºbatchæ·»åŠ language instructionå¹¶å¡«å……action/stateç»´åº¦"""
        # ä½¿ç”¨é»˜è®¤collate
        from torch.utils.data._utils.collate import default_collate
        batch_dict = default_collate(batch)

        # æ·»åŠ taskå­—æ®µ
        batch_size = batch_dict[list(batch_dict.keys())[0]].shape[0]
        batch_dict['task'] = [language_instruction] * batch_size

        # æ•°æ®å¢å¼ºï¼ˆ50%æ¦‚ç‡åº”ç”¨ï¼‰
        if augmenter is not None and random.random() < augmentation_prob:
            augmenter.set_random_params()
            for key in batch_dict.keys():
                if 'image' in key.lower() and isinstance(batch_dict[key], torch.Tensor):
                    # åº”ç”¨å›¾åƒå¢å¼º
                    batch_dict[key] = augmenter.apply_augment_sequence(
                        batch_dict[key])

        # å¡«å……actionå’Œstateç»´åº¦ï¼ˆä»Kuavoçš„16ç»´åˆ°SmolVLAçš„32ç»´ï¼‰
        for key in batch_dict.keys():
            if isinstance(batch_dict[key], torch.Tensor):
                if 'action' in key.lower():
                    # å¡«å……actionç»´åº¦
                    batch_dict[key] = pad_tensor_to_target_dim(
                        batch_dict[key], target_action_dim)
                elif 'state' in key.lower() or 'observation.state' in key:
                    # å¡«å……stateç»´åº¦
                    batch_dict[key] = pad_tensor_to_target_dim(
                        batch_dict[key], target_state_dim)

        return batch_dict

    return DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
        pin_memory=pin_memory,
        drop_last=drop_last,
        collate_fn=collate_fn_with_language,
        prefetch_factor=2,
        persistent_workers=True,
    )


def create_mixed_dataloader(
    cfg: DictConfig,
    task_cfg: DictConfig,
    replay_manager: Optional[ReplayDatasetManager] = None,
    dataset_fps: int = 10
) -> DataLoader:
    """
    åˆ›å»ºæ··åˆäº†replayæ•°æ®çš„DataLoader

    Args:
        cfg: åŸºç¡€é…ç½®
        task_cfg: å½“å‰ä»»åŠ¡é…ç½®
        replay_manager: Replayæ•°æ®ç®¡ç†å™¨
        dataset_fps: æ•°æ®é›†çš„fpsï¼ˆä»metadataè¯»å–ï¼‰

    Returns:
        æ··åˆæ•°æ®çš„DataLoader
    """
    task_id = task_cfg.task.id
    language_instruction = task_cfg.task.language_instruction

    # æ„å»ºdelta_timestampsé…ç½® (ç”¨äºåŠ è½½action chunks)
    chunk_size = cfg.policy.chunk_size
    delta_timestamps = {
        "observation.state": [0],  # åªå–å½“å‰å¸§
        # æœªæ¥chunk_sizeå¸§
        "action": [i / dataset_fps for i in range(chunk_size)],
    }

    print(f"ğŸ“ Dataset delta_timestamps configuration:")
    print(f"   - Dataset FPS: {dataset_fps}")
    print(f"   - observation.state: current frame only")
    print(
        f"   - action: {chunk_size} future frames ({chunk_size/dataset_fps:.2f}s @ {dataset_fps}fps)")

    # å½“å‰ä»»åŠ¡æ•°æ®é›†ï¼ˆä½¿ç”¨delta_timestampsï¼‰
    current_dataset = LeRobotDataset(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root,
        episodes=list(range(
            task_cfg.task.data.episodes_to_use[0],
            task_cfg.task.data.episodes_to_use[1] + 1
        )),
        delta_timestamps=delta_timestamps
    )

    print(f"ğŸ“Š Current Task {task_id} Dataset: {len(current_dataset)} frames")

    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡æˆ–ä¸ä½¿ç”¨replayï¼Œç›´æ¥è¿”å›
    if task_id == 1 or not cfg.sequential.use_replay_buffer:
        return create_dataloader_with_language(
            current_dataset,
            language_instruction,
            cfg.training.batch_size,
            cfg.training.num_workers,
            pin_memory=(cfg.training.device != 'cpu'),
            drop_last=cfg.training.drop_last
        )

    # æ··åˆreplayæ•°æ®
    if replay_manager is None:
        raise ValueError("replay_manager is required for task > 1")

    # åˆ›å»ºæ··åˆæ•°æ®é›†
    # æ³¨æ„ï¼šæ¯ä¸ªä»»åŠ¡éœ€è¦è‡ªå·±çš„language instruction
    all_datasets = [(current_dataset, language_instruction)]

    for replay_task_id, replay_dataset in replay_manager.replay_datasets.items():
        replay_task_cfg = load_task_config(
            Path(cfg.hydra.run.dir).parent.parent.parent / "configs/policy", replay_task_id)
        replay_language = replay_task_cfg.task.language_instruction
        all_datasets.append((replay_dataset, replay_language))
        print(
            f"ğŸ“¦ Adding Task {replay_task_id} replay: {len(replay_dataset)} frames")

    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºå•ç‹¬çš„dataloaderï¼Œç„¶åè½®æµé‡‡æ ·
    # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥concatenate datasets
    # æ³¨æ„ï¼šè¿™é‡Œæ¯ä¸ªdatasetéƒ½éœ€è¦ä¿ç•™è‡ªå·±çš„language instruction

    class MixedDataset(torch.utils.data.Dataset):
        """æ··åˆå¤šä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†ä¿ç•™è‡ªå·±çš„language instruction"""

        def __init__(self, datasets_with_language):
            self.datasets_with_language = datasets_with_language
            self.lengths = [len(ds) for ds, _ in datasets_with_language]
            self.total_length = sum(self.lengths)

            # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„é‡‡æ ·æ¦‚ç‡ï¼ˆåŸºäºreplay weightsï¼‰
            stage_key = f"stage{task_id}_replay"
            replay_config = cfg.sequential.get(stage_key, {})

            self.weights = []
            for i, (ds, _) in enumerate(datasets_with_language):
                if i == 0:
                    # å½“å‰ä»»åŠ¡çš„weight
                    task_key = f"task{task_id}"
                    weight = replay_config.get(task_key, 1.0)
                else:
                    # Replayä»»åŠ¡çš„weight
                    task_key = f"task{i}"  # iå¯¹åº”replay_task_id
                    weight = replay_config.get(task_key, 0.1)
                self.weights.append(weight)

            # å½’ä¸€åŒ–weights
            total_weight = sum(self.weights)
            self.weights = [w / total_weight for w in self.weights]

        def __len__(self):
            return self.total_length

        def __getitem__(self, idx):
            # æ ¹æ®weightséšæœºé€‰æ‹©ä¸€ä¸ªdataset
            import random
            dataset_idx = random.choices(
                range(len(self.datasets_with_language)), weights=self.weights, k=1)[0]
            dataset, language = self.datasets_with_language[dataset_idx]

            # ä»è¯¥datasetéšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
            sample_idx = random.randint(0, len(dataset) - 1)
            sample = dataset[sample_idx]

            # æ·»åŠ language instruction
            sample['task'] = language

            return sample

    mixed_dataset = MixedDataset(all_datasets)

    print(f"ğŸ“Š Mixed Dataset: {len(mixed_dataset)} frames (with replay)")
    print(f"   Weights: {mixed_dataset.weights}")

    # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
    augmenter = DeterministicAugmenterColor()

    def collate_fn_with_padding(batch):
        """collateå‡½æ•°ï¼šå¤„ç†mixed datasetçš„batchå¹¶å¡«å……ç»´åº¦"""
        from torch.utils.data._utils.collate import default_collate

        # batchä¸­çš„æ¯ä¸ªsampleå·²ç»æœ‰'task'å­—æ®µäº†
        # å…ˆæå–æ‰€æœ‰é'task'å­—æ®µè¿›è¡Œcollate
        tasks = [sample.pop('task') for sample in batch]

        # ä½¿ç”¨é»˜è®¤collateå¤„ç†å…¶ä»–å­—æ®µ
        batch_dict = default_collate(batch)

        # æ·»åŠ taskå­—æ®µå›å»
        batch_dict['task'] = tasks

        # æ•°æ®å¢å¼ºï¼ˆ50%æ¦‚ç‡åº”ç”¨ï¼‰
        if random.random() < 0.5:
            augmenter.set_random_params()
            for key in batch_dict.keys():
                if 'image' in key.lower() and isinstance(batch_dict[key], torch.Tensor):
                    # åº”ç”¨å›¾åƒå¢å¼º
                    batch_dict[key] = augmenter.apply_augment_sequence(
                        batch_dict[key])

        # å¡«å……actionå’Œstateç»´åº¦
        target_action_dim = cfg.policy.max_action_dim
        target_state_dim = cfg.policy.max_state_dim

        for key in batch_dict.keys():
            if isinstance(batch_dict[key], torch.Tensor):
                if 'action' in key.lower():
                    batch_dict[key] = pad_tensor_to_target_dim(
                        batch_dict[key], target_action_dim)
                elif 'state' in key.lower() or 'observation.state' in key:
                    batch_dict[key] = pad_tensor_to_target_dim(
                        batch_dict[key], target_state_dim)

        return batch_dict

    return DataLoader(
        mixed_dataset,
        batch_size=cfg.training.batch_size,
        num_workers=cfg.training.num_workers,
        shuffle=True,
        pin_memory=(cfg.training.device != 'cpu'),
        drop_last=cfg.training.drop_last,
        collate_fn=collate_fn_with_padding,
        prefetch_factor=1
    )


def validate_all_tasks(
    policy: SmolVLAPolicyWrapper,
    cfg: DictConfig,
    current_task_id: int,
    device: torch.device,
    cfg_root: Path,
    dataset_fps: int = 10
) -> Dict[int, float]:
    """
    éªŒè¯æ‰€æœ‰ä¹‹å‰çš„ä»»åŠ¡ï¼ˆæ£€æµ‹é—å¿˜ï¼‰

    Args:
        policy: SmolVLAç­–ç•¥
        cfg: é…ç½®
        current_task_id: å½“å‰ä»»åŠ¡ID
        device: è®¾å¤‡
        cfg_root: é…ç½®æ ¹ç›®å½•

    Returns:
        validation_results: {task_id: avg_loss}
    """
    print("\n" + "="*70)
    print(f"ğŸ” Multi-Task Validation (Tasks 1-{current_task_id})")
    print("="*70)

    policy.eval()
    validation_results = {}

    for task_id in range(1, current_task_id + 1):
        print(f"\nğŸ“Š Validating Task {task_id}...")

        # åŠ è½½ä»»åŠ¡é…ç½®
        task_cfg = load_task_config(cfg_root, task_id)

        # åŠ è½½éªŒè¯é›†ï¼ˆä½¿ç”¨å‰Nä¸ªepisodesä½œä¸ºéªŒè¯ï¼Œé¿å…ä¸è®­ç»ƒæ•°æ®å®Œå…¨åˆ†ç¦»ï¼‰
        # æ³¨æ„ï¼šè¿™æ˜¯å¿«é€ŸéªŒè¯æ–¹æ³•ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„å­é›†
        num_val_episodes = cfg.training.validation_episodes

        # ä»è®­ç»ƒepisodesä¸­é€‰æ‹©å‰Nä¸ªä½œä¸ºéªŒè¯
        train_episode_start = task_cfg.task.data.episodes_to_use[0]
        train_episode_end = task_cfg.task.data.episodes_to_use[1]

        # éªŒè¯ç”¨å‰Nä¸ªepisodes
        val_episode_end = min(train_episode_start +
                              num_val_episodes - 1, train_episode_end)
        val_episodes = list(range(train_episode_start, val_episode_end + 1))

        # ç¡®ä¿ä¸è¶…è¿‡num_val_episodes
        val_episodes = val_episodes[:num_val_episodes]

        # æ„å»ºdelta_timestampsé…ç½®
        chunk_size = cfg.policy.chunk_size
        delta_timestamps = {
            "observation.state": [0],
            "action": [i / dataset_fps for i in range(chunk_size)],
        }

        val_dataset = LeRobotDataset(
            task_cfg.task.data.repoid,
            root=task_cfg.task.data.root,
            episodes=val_episodes,
            delta_timestamps=delta_timestamps
        )

        val_loader = create_dataloader_with_language(
            val_dataset,
            task_cfg.task.language_instruction,
            batch_size=cfg.training.batch_size,
            num_workers=cfg.training.num_workers // 2,
            pin_memory=(device.type != 'cpu'),
            drop_last=False
        )

        # éªŒè¯
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Task {task_id} Validation", leave=False):
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                         for k, v in batch.items()}

                loss, _ = policy.forward(batch)
                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / \
            num_batches if num_batches > 0 else float('inf')
        validation_results[task_id] = avg_loss

        print(f"  Task {task_id} Validation Loss: {avg_loss:.4f}")

    # åˆ†æé—å¿˜æƒ…å†µ
    if current_task_id > 1:
        print("\nâš ï¸  Forgetting Analysis:")
        for task_id in range(1, current_task_id):
            loss = validation_results[task_id]
            # ç®€å•çš„é˜ˆå€¼åˆ¤æ–­
            if loss < 0.7:
                status = "âœ… Well Retained"
            elif loss < 1.0:
                status = "âš ï¸  Slight Degradation"
            else:
                status = "âŒ Significant Forgetting"

            print(f"  Task {task_id}: {status} (loss={loss:.4f})")

    print("="*70 + "\n")

    policy.train()
    return validation_results


@hydra.main(config_path="../configs/policy/", config_name="smolvla_sequential_base", version_base=None)
def main(cfg: DictConfig):
    """ä¸»è®­ç»ƒæµç¨‹"""

    # è®¾ç½® HuggingFace é•œåƒæºä»¥æé«˜ä¸‹è½½é€Ÿåº¦
    import os

    # ä»é…ç½®è¯»å– HF endpointï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤é•œåƒæº
    hf_endpoint = cfg.get('hf_endpoint', 'https://hf-mirror.com')
    if hf_endpoint:
        os.environ['HF_ENDPOINT'] = hf_endpoint
        print(f"âœ… å·²è®¾ç½® HuggingFace ä¸‹è½½æº: {hf_endpoint}")
    else:
        print("â„¹ï¸  ä½¿ç”¨é»˜è®¤ HuggingFace Hub: https://huggingface.co")

    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

    logger = setup_logging()
    set_seed(cfg.training.seed)

    # åŠ è½½ä»»åŠ¡é…ç½®
    # ä»Hydraé…ç½®è·å–ä»»åŠ¡åç§°ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼štasks/task1_moving_grasp æˆ– task1_moving_graspï¼‰
    task_param = cfg.get('task', 'task1_moving_grasp')
    if task_param.startswith('tasks/'):
        task_param = task_param.replace('tasks/', '')

    # åŠ¨æ€åŠ è½½ä»»åŠ¡é…ç½®
    cfg_root = Path(__file__).parent.parent / "configs/policy"
    task_cfg = load_task_config(cfg_root, int(
        task_param.split('_')[0].replace('task', '')))
    task_id = task_cfg.task.id
    task_name = task_cfg.task.name

    # è®¾ç½®taskå­—æ®µç”¨äºè·¯å¾„ï¼ˆæ ¼å¼ï¼štask{id}_{name}ï¼Œå¦‚task1_moving_graspï¼‰
    cfg.task = f"task{task_id}_{task_name}"

    print("\n" + "="*70)
    print(f"ğŸ¤– SmolVLA Sequential Training - Stage {task_id}")
    print("="*70)
    print(f"Task ID: {task_id}")
    print(f"Task Name: {task_name}")
    print(f"Description: {task_cfg.task.description}")
    print(f"Language: {task_cfg.task.language_instruction}")
    print("="*70 + "\n")

    # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–ç­–ç•¥ä¸€è‡´çš„æ ¼å¼ï¼‰
    # æ ¼å¼: outputs/train/{task}/{method}/run_{timestamp}
    # å±•å¼€: outputs/train/task1_moving_grasp/smolvla_sequential/run_20251011_123456
    output_directory = Path(
        cfg.training.output_directory) / f"run_{cfg.timestamp}"
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    print(f"ğŸ“ Output Directory: {output_directory}")
    print(f"ğŸ“… Timestamp: {cfg.timestamp}\n")

    device = torch.device(cfg.training.device)

    # ==================== åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯ ====================
    print("ğŸ“‚ Loading Dataset Metadata...")
    dataset_metadata = LeRobotDatasetMetadata(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root
    )

    # è·å–æ•°æ®é›†fpsï¼ˆç”¨äºé…ç½®delta_timestampsï¼‰
    dataset_fps = dataset_metadata.fps
    print(f"ğŸ“Š Dataset FPS: {dataset_fps}")

    # æ„å»ºfeatures
    features = dataset_to_policy_features(dataset_metadata.features)
    input_features = {k: ft for k, ft in features.items(
    ) if ft.type is not FeatureType.ACTION}
    output_features = {k: ft for k,
                       ft in features.items() if ft.type is FeatureType.ACTION}

    dataset_stats = dataset_metadata.stats

    # å¡«å……dataset_statsåˆ°ç›®æ ‡ç»´åº¦ï¼ˆKuavo 16ç»´ â†’ SmolVLA 32ç»´ï¼‰
    print("ğŸ“ Padding dataset_stats to match SmolVLA dimensions (16D â†’ 32D)...")
    dataset_stats = pad_dataset_stats(
        dataset_stats,
        target_action_dim=cfg.policy.max_action_dim,
        target_state_dim=cfg.policy.max_state_dim
    )
    print("âœ… Dataset stats padded successfully")

    # ==================== æ„å»ºPolicyé…ç½® ====================
    from hydra.utils import instantiate

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=device,
    )

    # Override learning rate from task config
    if hasattr(task_cfg.task.training, 'policy'):
        policy_cfg.optimizer_lr = task_cfg.task.training.policy.optimizer_lr
        policy_cfg.scheduler_warmup_steps = task_cfg.task.training.policy.scheduler_warmup_steps
        policy_cfg.scheduler_decay_steps = task_cfg.task.training.policy.scheduler_decay_steps

    # ==================== åŠ è½½/åˆ›å»ºæ¨¡å‹ ====================
    if task_cfg.task.training.resume_from == 'pretrained':
        # Stage 1: ä»HuggingFaceé¢„è®­ç»ƒåŠ è½½
        print(
            f"\nğŸ“‚ Loading pretrained SmolVLA from {task_cfg.task.training.pretrained_path}")
        policy = SmolVLAPolicyWrapper.from_pretrained(
            task_cfg.task.training.pretrained_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )

    elif task_cfg.task.training.resume_from == 'task':
        # Stage 2+: ä»ä¸Šä¸€ä¸ªä»»åŠ¡ç»§ç»­
        prev_task_id = task_cfg.task.training.resume_task_id
        resume_path = task_cfg.task.training.resume_path

        print(f"\nğŸ“‚ Loading from Task {prev_task_id}: {resume_path}")
        policy = SmolVLAPolicyWrapper.from_pretrained(
            resume_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )
        print(
            f"âœ… Successfully loaded Task {prev_task_id} model for sequential training")

    else:
        # ä»å¤´è®­ç»ƒï¼ˆä¸æ¨èï¼‰
        print("\nâš ï¸  Training from scratch (not recommended for sequential training)")
        policy = SmolVLAPolicyWrapper(policy_cfg, dataset_stats)

    policy = policy.to(device)

    policy.train()

    # ==================== å‡†å¤‡æ•°æ® ====================
    # åŠ è½½replay bufferï¼ˆå¦‚æœéœ€è¦ï¼‰
    replay_manager = None
    if task_id > 1 and cfg.sequential.use_replay_buffer:
        cfg_root = Path(__file__).parent.parent / "configs/policy"
        replay_manager = ReplayDatasetManager(
            cfg, task_id, cfg_root, dataset_fps)
        replay_manager.load_replay_tasks()

    # åˆ›å»ºdataloaderï¼ˆä¼ é€’dataset_fpsï¼‰
    dataloader = create_mixed_dataloader(
        cfg, task_cfg, replay_manager, dataset_fps)

    # ==================== æ„å»ºä¼˜åŒ–å™¨ ====================
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())
    lr_scheduler = policy.config.get_scheduler_preset().build(
        optimizer,
        num_training_steps=task_cfg.task.training.max_epoch * len(dataloader)
    )

    print(f"\nğŸ¯ Training Configuration:")
    print(f"   Epochs: {task_cfg.task.training.max_epoch}")
    print(f"   Batch Size: {cfg.training.batch_size}")
    print(f"   Learning Rate: {policy_cfg.optimizer_lr}")
    print(f"   Steps per Epoch: {len(dataloader)}")
    print(
        f"   Total Steps: {task_cfg.task.training.max_epoch * len(dataloader)}")

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\nğŸš€ Starting Training...")
    print("="*70 + "\n")

    best_loss = float('inf')

    for epoch in range(task_cfg.task.training.max_epoch):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch + 1}/{task_cfg.task.training.max_epoch}")
        print(f"{'='*70}")

        # è®­ç»ƒ
        policy.train()
        total_loss = 0.0
        num_batches = 0

        epoch_bar = tqdm(
            dataloader,
            desc=f"Training Epoch {epoch+1}",
            dynamic_ncols=True,
            leave=False
        )

        for batch in epoch_bar:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in batch.items()}

            # Forward
            loss, _ = policy.forward(batch)

            # Backward
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                policy.parameters(),
                max_norm=policy_cfg.optimizer_grad_clip_norm
            )

            # Optimizer step
            optimizer.step()
            optimizer.zero_grad()
            lr_scheduler.step()

            # Logging
            total_loss += loss.item()
            num_batches += 1

            epoch_bar.set_postfix(
                loss=f"{loss.item():.4f}",
                lr=f"{lr_scheduler.get_last_lr()[0]:.2e}"
            )

        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

        # TensorBoard logging
        writer.add_scalar("train/loss", avg_loss, epoch)
        writer.add_scalar("train/lr", lr_scheduler.get_last_lr()[0], epoch)

        # å¤šä»»åŠ¡éªŒè¯
        if (epoch + 1) % cfg.training.validation_freq_epoch == 0 and cfg.training.get('validate_all_previous_tasks', False):
            cfg_root = Path(__file__).parent.parent / "configs/policy"
            validation_results = validate_all_tasks(
                policy, cfg, task_id, device, cfg_root, dataset_fps)

            # Log validation results
            for val_task_id, val_loss in validation_results.items():
                writer.add_scalar(
                    f"validation/task{val_task_id}_loss", val_loss, epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            save_rng_state(best_path / "rng_state.pth")

            # ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºå®Œç¾æ¢å¤è®­ç»ƒï¼‰
            checkpoint = {
                "optimizer": optimizer.state_dict(),
                "lr_scheduler": lr_scheduler.state_dict(),
                "epoch": epoch + 1,
                "best_loss": best_loss
            }
            torch.save(checkpoint, best_path / "learning_state.pth")

            print(f"âœ… Best model saved: loss={best_loss:.4f}")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            save_rng_state(epoch_path / "rng_state.pth")

            # ä¿å­˜è®­ç»ƒçŠ¶æ€
            checkpoint = {
                "optimizer": optimizer.state_dict(),
                "lr_scheduler": lr_scheduler.state_dict(),
                "epoch": epoch + 1,
                "best_loss": best_loss
            }
            torch.save(checkpoint, epoch_path / "learning_state.pth")

            print(f"âœ… Checkpoint saved: epoch {epoch+1}")

    writer.close()

    # ==================== ä¿å­˜æœ€ç»ˆçŠ¶æ€ ====================
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºå®Œç¾æ¢å¤æˆ–ç»§ç»­è®­ç»ƒï¼‰
    print("\nğŸ’¾ Saving final model and training state...")
    policy.save_pretrained(output_directory)
    save_rng_state(output_directory / "rng_state.pth")

    # ä¿å­˜æœ€ç»ˆè®­ç»ƒçŠ¶æ€
    final_checkpoint = {
        "optimizer": optimizer.state_dict(),
        "lr_scheduler": lr_scheduler.state_dict(),
        "epoch": task_cfg.task.training.max_epoch,
        "best_loss": best_loss
    }
    torch.save(final_checkpoint, output_directory / "learning_state.pth")
    print("âœ… Final model and training state saved")

    # ==================== æœ€ç»ˆéªŒè¯ ====================
    print("\n" + "="*70)
    print("ğŸ¯ Final Multi-Task Validation")
    print("="*70)

    cfg_root = Path(__file__).parent.parent / "configs/policy"
    final_results = validate_all_tasks(
        policy, cfg, task_id, device, cfg_root, dataset_fps)

    # ä¿å­˜è®­ç»ƒç»“æœ
    results_file = output_directory / "training_results.json"
    with open(results_file, 'w') as f:
        json.dump({
            'task_id': task_id,
            'task_name': task_name,
            'description': task_cfg.task.description,
            'language_instruction': task_cfg.task.language_instruction,
            'best_loss': best_loss,
            'final_validation': {str(k): v for k, v in final_results.items()},
            'training_epochs': task_cfg.task.training.max_epoch,
            'learning_rate': policy_cfg.optimizer_lr,
        }, f, indent=2)

    print(f"\n{'='*70}")
    print(f"âœ… Task {task_id} Training Completed!")
    print(f"{'='*70}")
    print(f"Best Loss: {best_loss:.4f}")
    print(f"Model saved to: {output_directory}")
    print(f"Results saved to: {results_file}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
