#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SmolVLAé¡ºåºå¤šä»»åŠ¡è®­ç»ƒè„šæœ¬

å®ç°SmolVLAçš„é¡ºåºFine-tuningç­–ç•¥ï¼š
- Stage 1: é¢„è®­ç»ƒ â†’ ä»»åŠ¡1æ¨¡å‹
- Stage 2: ä»»åŠ¡1æ¨¡å‹ â†’ ä»»åŠ¡2æ¨¡å‹
- Stage 3: ä»»åŠ¡2æ¨¡å‹ â†’ ä»»åŠ¡3æ¨¡å‹
- Stage 4: ä»»åŠ¡3æ¨¡å‹ â†’ ä»»åŠ¡4æ¨¡å‹ï¼ˆæœ€ç»ˆå¤šä»»åŠ¡æ¨¡å‹ï¼‰

é˜²é—å¿˜æŠ€æœ¯ï¼š
- Replay Buffer: æ··åˆä¹‹å‰ä»»åŠ¡çš„æ•°æ®
- Lower Learning Rate: é€æ­¥é™ä½å­¦ä¹ ç‡
- Freeze Layers: å†»ç»“VLMéƒ¨åˆ†å±‚
- Multi-task Validation: éªŒè¯æ‰€æœ‰ä¹‹å‰ä»»åŠ¡

ä½¿ç”¨æ–¹æ³•ï¼š
    # è®­ç»ƒä»»åŠ¡1
    python kuavo_train/train_smolvla_sequential.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task1_moving_grasp

    # è®­ç»ƒä»»åŠ¡2ï¼ˆè‡ªåŠ¨ä»ä»»åŠ¡1ç»§ç»­ï¼‰
    python kuavo_train/train_smolvla_sequential.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task2_weighing
"""

# Ensure custom patches are applied FIRST before any lerobot imports
import lerobot_patches.custom_patches

import random
from kuavo_train.utils.augmenter import DeterministicAugmenterColor
from kuavo_train.utils.utils import save_rng_state, load_rng_state
from kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper import SmolVLAConfigWrapper
from kuavo_train.wrapper.policy.smolvla.SmolVLAPolicyWrapper import SmolVLAPolicyWrapper
from lerobot.configs.types import FeatureType
from lerobot.utils.random_utils import set_seed
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from kuavo_train.wrapper.dataset.SmolVLADatasetWrapper import (
    SmolVLADatasetWrapper,
    SmolVLAMixedDatasetWrapper
)
from tqdm import tqdm
import time
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler
import torch.nn as nn
import torch
from typing import Optional, Dict, Any
import json
from functools import partial
from pathlib import Path
from omegaconf import DictConfig, OmegaConf
import hydra

import os
# æ¶ˆé™¤tokenizers forkè­¦å‘Š
os.environ['TOKENIZERS_PARALLELISM'] = 'false'


# å¯¼å…¥SmolVLAæ¨¡å—

# å¯¼å…¥è®­ç»ƒçŠ¶æ€ä¿å­˜/åŠ è½½å·¥å…·

# å¯¼å…¥æ•°æ®å¢å¼ºå·¥å…·


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(
                'smolvla_sequential_training.log', encoding='utf-8')
        ]
    )
    return logging.getLogger("SmolVLASequentialTraining")


def load_task_config(cfg_root: Path, task_id: int) -> DictConfig:
    """
    åŠ è½½æŒ‡å®šä»»åŠ¡çš„é…ç½®

    Args:
        cfg_root: é…ç½®æ–‡ä»¶æ ¹ç›®å½•
        task_id: ä»»åŠ¡ID (1-4)

    Returns:
        ä»»åŠ¡é…ç½®å¯¹è±¡
    """
    task_files = {
        1: "task1_moving_grasp.yaml",
        2: "task2_weighing.yaml",
        3: "task3_placement.yaml",
        4: "task4_sorting.yaml",
    }

    task_file = cfg_root / "tasks" / task_files[task_id]
    if not task_file.exists():
        raise FileNotFoundError(f"Task config file not found: {task_file}")

    task_cfg = OmegaConf.load(task_file)
    return task_cfg


class ReplayDatasetManager:
    """
    ç®¡ç†Replay Bufferçš„ç±»

    åœ¨è®­ç»ƒä»»åŠ¡Næ—¶ï¼Œæ··åˆä¹‹å‰ä»»åŠ¡1åˆ°N-1çš„æ•°æ®ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜
    """

    def __init__(self, cfg: DictConfig, current_task_id: int, cfg_root: Path, dataset_fps: int):
        self.cfg = cfg
        self.current_task_id = current_task_id
        self.cfg_root = cfg_root
        self.dataset_fps = dataset_fps
        self.replay_datasets = {}  # task_id -> dataset
        self.replay_weights = {}   # task_id -> weight

    def load_replay_tasks(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦replayçš„ä»»åŠ¡æ•°æ®"""
        if self.current_task_id == 1:
            # ä»»åŠ¡1ä¸éœ€è¦replay
            return {}, {}

        # è·å–å½“å‰stageçš„replayé…ç½®
        stage_key = f"stage{self.current_task_id}_replay"
        replay_config = self.cfg.sequential.get(stage_key, {})

        if not replay_config:
            print(
                f"âš ï¸  No replay config found for stage {self.current_task_id}")
            return {}, {}

        print(f"\nğŸ“¦ Loading Replay Buffer for Stage {self.current_task_id}")
        print("="*70)

        # æ„å»ºdelta_timestampsé…ç½® (ç”¨äºåŠ è½½action chunks)
        chunk_size = self.cfg.policy.chunk_size
        delta_timestamps = {
            "observation.state": [0],  # åªå–å½“å‰å¸§
            # æœªæ¥chunk_sizeå¸§
            "action": [i / self.dataset_fps for i in range(chunk_size)],
        }

        for task_key, weight in replay_config.items():
            if 'task' in task_key:
                task_id = int(task_key.replace('task', ''))

                # åªåŠ è½½ä¹‹å‰çš„ä»»åŠ¡
                if task_id < self.current_task_id:
                    print(
                        f"  Loading Task {task_id} (weight: {weight:.1%})...")

                    # åŠ è½½ä»»åŠ¡é…ç½®
                    task_cfg = load_task_config(self.cfg_root, task_id)

                    # åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨delta_timestampså’Œä¼˜åŒ–çš„è§†é¢‘åç«¯ï¼‰
                    dataset = LeRobotDataset(
                        task_cfg.task.data.repoid,
                        root=task_cfg.task.data.root,
                        episodes=list(range(
                            task_cfg.task.data.episodes_to_use[0],
                            task_cfg.task.data.episodes_to_use[1] + 1
                        )),
                        delta_timestamps=delta_timestamps,
                        video_backend=get_optimal_video_backend()  # ä¼˜åŒ–ï¼šä½¿ç”¨æœ€ä¼˜è§†é¢‘åç«¯
                    )

                    self.replay_datasets[task_id] = dataset
                    self.replay_weights[task_id] = weight

                    print(
                        f"    âœ… Loaded {len(dataset)} frames from Task {task_id}")

        print("="*70 + "\n")
        return self.replay_datasets, self.replay_weights


def pad_tensor_to_target_dim(tensor, target_dim: int):
    """
    å°†tensoræˆ–numpy arrayä»å®é™…ç»´åº¦å¡«å……åˆ°ç›®æ ‡ç»´åº¦

    Args:
        tensor: è¾“å…¥tensor (torch.Tensoræˆ–numpy.ndarray)ï¼Œå½¢çŠ¶ä¸º [..., actual_dim]
        target_dim: ç›®æ ‡ç»´åº¦

    Returns:
        å¡«å……åçš„tensorï¼Œç±»å‹ä¸è¾“å…¥ç›¸åŒ
    """
    import numpy as np

    actual_dim = tensor.shape[-1]
    if actual_dim == target_dim:
        return tensor
    elif actual_dim < target_dim:
        # å¡«å……0åˆ°ç›®æ ‡ç»´åº¦
        pad_size = target_dim - actual_dim
        pad_shape = list(tensor.shape[:-1]) + [pad_size]

        if isinstance(tensor, torch.Tensor):
            # torch.Tensor: ä½¿ç”¨torch.zeros
            pad_tensor = torch.zeros(
                pad_shape, dtype=tensor.dtype, device=tensor.device)
            return torch.cat([tensor, pad_tensor], dim=-1)
        elif isinstance(tensor, np.ndarray):
            # numpy.ndarray: ä½¿ç”¨np.zeros
            pad_array = np.zeros(pad_shape, dtype=tensor.dtype)
            return np.concatenate([tensor, pad_array], axis=-1)
        else:
            raise TypeError(f"Unsupported tensor type: {type(tensor)}")
    else:
        # æˆªæ–­åˆ°ç›®æ ‡ç»´åº¦ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
        return tensor[..., :target_dim]


def pad_dataset_stats(dataset_stats: Dict[str, Dict],
                      target_action_dim: int = 32,
                      target_state_dim: int = 32) -> Dict[str, Dict]:
    """
    å°†dataset_statsä¸­çš„actionå’Œstateç»Ÿè®¡ä¿¡æ¯å¡«å……åˆ°ç›®æ ‡ç»´åº¦

    å¯¹äºmeanï¼šå¡«å……0
    å¯¹äºstdï¼šå¡«å……1ï¼ˆè¿™æ ·å½’ä¸€åŒ–æ—¶å¡«å……éƒ¨åˆ†ä¸ä¼šè¢«æ”¹å˜ï¼‰

    Args:
        dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å­—å…¸ (å¯ä»¥æ˜¯torch.Tensoræˆ–numpy.ndarray)
        target_action_dim: ç›®æ ‡actionç»´åº¦
        target_state_dim: ç›®æ ‡stateç»´åº¦

    Returns:
        å¡«å……åçš„dataset_stats
    """
    import numpy as np

    def pad_with_ones(tensor, target_dim):
        """å¡«å……1åˆ°ç›®æ ‡ç»´åº¦ï¼ˆç”¨äºstdï¼‰"""
        actual_dim = tensor.shape[-1]
        if actual_dim >= target_dim:
            return tensor

        pad_size = target_dim - actual_dim
        pad_shape = list(tensor.shape[:-1]) + [pad_size]

        if isinstance(tensor, torch.Tensor):
            pad_tensor = torch.ones(
                pad_shape, dtype=tensor.dtype, device=tensor.device)
            return torch.cat([tensor, pad_tensor], dim=-1)
        elif isinstance(tensor, np.ndarray):
            pad_array = np.ones(pad_shape, dtype=tensor.dtype)
            return np.concatenate([tensor, pad_array], axis=-1)
        else:
            raise TypeError(f"Unsupported tensor type: {type(tensor)}")

    padded_stats = {}

    for key, stats_dict in dataset_stats.items():
        if 'action' in key.lower():
            # å¡«å……actionç›¸å…³ç»Ÿè®¡
            padded_stats[key] = {}
            for stat_name, stat_tensor in stats_dict.items():
                if stat_name == 'mean':
                    # meanå¡«å……0
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_action_dim)
                elif stat_name == 'std':
                    # stdå¡«å……1ï¼ˆé¿å…é™¤0ï¼Œä¸”ä¸æ”¹å˜å¡«å……éƒ¨åˆ†çš„å€¼ï¼‰
                    padded_stats[key][stat_name] = pad_with_ones(
                        stat_tensor, target_action_dim)
                else:
                    # å…¶ä»–ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚min, maxï¼‰ä¹Ÿéœ€è¦å¡«å……
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_action_dim)

        elif 'state' in key.lower() or 'observation.state' in key:
            # å¡«å……stateç›¸å…³ç»Ÿè®¡
            padded_stats[key] = {}
            for stat_name, stat_tensor in stats_dict.items():
                if stat_name == 'mean':
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_state_dim)
                elif stat_name == 'std':
                    padded_stats[key][stat_name] = pad_with_ones(
                        stat_tensor, target_state_dim)
                else:
                    padded_stats[key][stat_name] = pad_tensor_to_target_dim(
                        stat_tensor, target_state_dim)
        else:
            # ä¸æ˜¯actionæˆ–stateï¼Œç›´æ¥å¤åˆ¶
            padded_stats[key] = stats_dict

    return padded_stats


def get_optimal_video_backend():
    """
    è·å–æœ€ä¼˜çš„è§†é¢‘è§£ç åç«¯

    ä¼˜å…ˆä½¿ç”¨torchcodecï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå› ä¸ºå®ƒå¯ä»¥ç²¾ç¡®å®šä½åˆ°ç›®æ ‡å¸§ï¼Œæ¯”pyavå¿«å¾—å¤šã€‚
    å¦‚æœtorchcodecä¸å¯ç”¨ï¼Œfallbackåˆ°pyavã€‚

    Returns:
        str: è§†é¢‘åç«¯åç§° ("torchcodec" æˆ– "pyav")
    """
    try:
        import importlib.util
        if importlib.util.find_spec("torchcodec"):
            return "torchcodec"
        else:
            return "pyav"
    except:
        return "pyav"


def create_lerobot_dataset_with_deltas(
    repo_id: str,
    root: str,
    episodes: list,
    delta_timestamps: Dict[str, list]
) -> LeRobotDataset:
    """
    åˆ›å»ºLeRobotDatasetå¹¶é…ç½®delta_timestampsä»¥åŠ è½½action chunks

    Args:
        repo_id: Dataset repository ID
        root: Dataset root path
        episodes: List of episode indices
        delta_timestamps: Delta timestampsé…ç½®ï¼Œä¾‹å¦‚ï¼š
            {
                "observation.state": [0],  # å½“å‰å¸§
                "action": [i/fps for i in range(50)]  # æœªæ¥50å¸§
            }

    Returns:
        é…ç½®å¥½çš„LeRobotDataset
    """
    return LeRobotDataset(
        repo_id,
        root=root,
        episodes=episodes,
        delta_timestamps=delta_timestamps,
        video_backend=get_optimal_video_backend()  # ä¼˜åŒ–ï¼šä½¿ç”¨æœ€ä¼˜è§†é¢‘åç«¯
    )


def create_dataloader_with_language(
    dataset: LeRobotDataset,
    language_instruction: str,
    batch_size: int,
    num_workers: int,
    pin_memory: bool = True,
    drop_last: bool = False,
    target_action_dim: int = 32,
    target_state_dim: int = 32,
    use_augmentation: bool = True,
    augmentation_prob: float = 0.5
) -> DataLoader:
    """
    åˆ›å»ºåŒ…å«language instructionçš„DataLoaderï¼Œå¹¶è‡ªåŠ¨å¡«å……action/stateç»´åº¦

    ä¼˜åŒ–ï¼šä½¿ç”¨SmolVLADatasetWrapperå°†æ•°æ®å¢å¼ºå’Œå¡«å……æ“ä½œç§»åˆ°workerè¿›ç¨‹

    Args:
        dataset: LeRobotæ•°æ®é›†
        language_instruction: ä»»åŠ¡çš„language instruction
        batch_size: batchå¤§å°
        num_workers: workeræ•°é‡
        pin_memory: æ˜¯å¦pin memory
        drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªbatch
        target_action_dim: ç›®æ ‡actionç»´åº¦ï¼ˆé»˜è®¤32ï¼Œä¸SmolVLAé¢„è®­ç»ƒä¸€è‡´ï¼‰
        target_state_dim: ç›®æ ‡stateç»´åº¦ï¼ˆé»˜è®¤32ï¼Œä¸SmolVLAé¢„è®­ç»ƒä¸€è‡´ï¼‰
        use_augmentation: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
        augmentation_prob: æ•°æ®å¢å¼ºæ¦‚ç‡

    Returns:
        DataLoader
    """
    # ä½¿ç”¨ä¼˜åŒ–çš„DatasetåŒ…è£…å™¨ï¼ˆæ•°æ®å¢å¼ºå’Œå¡«å……åœ¨workerè¿›ç¨‹ä¸­æ‰§è¡Œï¼‰
    wrapped_dataset = SmolVLADatasetWrapper(
        dataset=dataset,
        language_instruction=language_instruction,
        target_action_dim=target_action_dim,
        target_state_dim=target_state_dim,
        use_augmentation=use_augmentation,
        augmentation_prob=augmentation_prob,
    )

    # ç®€åŒ–çš„collateå‡½æ•°ï¼ˆåªéœ€è¦åŸºæœ¬çš„batch collationï¼‰
    def collate_fn_with_language(batch):
        """ç®€åŒ–çš„collateå‡½æ•°ï¼ŒåªåšåŸºæœ¬çš„batch collation"""
        from torch.utils.data._utils.collate import default_collate
        return default_collate(batch)

    return DataLoader(
        wrapped_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
        pin_memory=pin_memory,
        drop_last=drop_last,
        collate_fn=collate_fn_with_language,
        prefetch_factor=2,
        persistent_workers=True,
    )


def create_mixed_dataloader(
    cfg: DictConfig,
    task_cfg: DictConfig,
    replay_manager: Optional[ReplayDatasetManager] = None,
    dataset_fps: int = 10
) -> DataLoader:
    """
    åˆ›å»ºæ··åˆäº†replayæ•°æ®çš„DataLoader

    Args:
        cfg: åŸºç¡€é…ç½®
        task_cfg: å½“å‰ä»»åŠ¡é…ç½®
        replay_manager: Replayæ•°æ®ç®¡ç†å™¨
        dataset_fps: æ•°æ®é›†çš„fpsï¼ˆä»metadataè¯»å–ï¼‰

    Returns:
        æ··åˆæ•°æ®çš„DataLoader
    """
    task_id = task_cfg.task.id
    language_instruction = task_cfg.task.language_instruction

    # æ„å»ºdelta_timestampsé…ç½® (ç”¨äºåŠ è½½action chunks)
    chunk_size = cfg.policy.chunk_size
    delta_timestamps = {
        "observation.state": [0],  # åªå–å½“å‰å¸§
        # æœªæ¥chunk_sizeå¸§
        "action": [i / dataset_fps for i in range(chunk_size)],
    }

    print(f"ğŸ“ Dataset delta_timestamps configuration:")
    print(f"   - Dataset FPS: {dataset_fps}")
    print(f"   - observation.state: current frame only")
    print(
        f"   - action: {chunk_size} future frames ({chunk_size/dataset_fps:.2f}s @ {dataset_fps}fps)")

    # å½“å‰ä»»åŠ¡æ•°æ®é›†ï¼ˆä½¿ç”¨delta_timestampså’Œä¼˜åŒ–çš„è§†é¢‘åç«¯ï¼‰
    video_backend = get_optimal_video_backend()
    print(f"ğŸ¬ Using video backend: {video_backend}")
    if video_backend == "torchcodec":
        print("   âœ… torchcodec available - faster video decoding enabled")
    else:
        print("   âš ï¸  torchcodec not available - using pyav (slower)")
        print("   ğŸ’¡ Install torchcodec to improve video decoding performance")

    current_dataset = LeRobotDataset(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root,
        episodes=list(range(
            task_cfg.task.data.episodes_to_use[0],
            task_cfg.task.data.episodes_to_use[1] + 1
        )),
        delta_timestamps=delta_timestamps,
        video_backend=video_backend  # ä¼˜åŒ–ï¼šä½¿ç”¨æœ€ä¼˜è§†é¢‘åç«¯
    )

    print(f"ğŸ“Š Current Task {task_id} Dataset: {len(current_dataset)} frames")

    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡æˆ–ä¸ä½¿ç”¨replayï¼Œç›´æ¥è¿”å›
    if task_id == 1 or not cfg.sequential.use_replay_buffer:
        return create_dataloader_with_language(
            current_dataset,
            language_instruction,
            cfg.training.batch_size,
            cfg.training.num_workers,
            pin_memory=(cfg.training.device != 'cpu'),
            drop_last=cfg.training.drop_last
        )

    # æ··åˆreplayæ•°æ®
    if replay_manager is None:
        raise ValueError("replay_manager is required for task > 1")

    # åˆ›å»ºæ··åˆæ•°æ®é›†
    # æ³¨æ„ï¼šæ¯ä¸ªä»»åŠ¡éœ€è¦è‡ªå·±çš„language instruction
    all_datasets = [(current_dataset, language_instruction)]

    for replay_task_id, replay_dataset in replay_manager.replay_datasets.items():
        replay_task_cfg = load_task_config(
            Path(cfg.hydra.run.dir).parent.parent.parent / "configs/policy", replay_task_id)
        replay_language = replay_task_cfg.task.language_instruction
        all_datasets.append((replay_dataset, replay_language))
        print(
            f"ğŸ“¦ Adding Task {replay_task_id} replay: {len(replay_dataset)} frames")

    # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„é‡‡æ ·æ¦‚ç‡ï¼ˆåŸºäºreplay weightsï¼‰
    stage_key = f"stage{task_id}_replay"
    replay_config = cfg.sequential.get(stage_key, {})

    weights = []
    for i, (ds, _) in enumerate(all_datasets):
        if i == 0:
            # å½“å‰ä»»åŠ¡çš„weight
            task_key = f"task{task_id}"
            weight = replay_config.get(task_key, 1.0)
        else:
            # Replayä»»åŠ¡çš„weight
            task_key = f"task{i}"  # iå¯¹åº”replay_task_id
            weight = replay_config.get(task_key, 0.1)
        weights.append(weight)

    # å½’ä¸€åŒ–weights
    total_weight = sum(weights)
    normalized_weights = [w / total_weight for w in weights]

    # ä½¿ç”¨ä¼˜åŒ–çš„MixedDatasetåŒ…è£…å™¨ï¼ˆæ•°æ®å¢å¼ºå’Œå¡«å……åœ¨workerè¿›ç¨‹ä¸­æ‰§è¡Œï¼‰
    mixed_dataset_wrapper = SmolVLAMixedDatasetWrapper(
        datasets_with_language=all_datasets,
        weights=normalized_weights,
        target_action_dim=cfg.policy.max_action_dim,
        target_state_dim=cfg.policy.max_state_dim,
        use_augmentation=True,
        augmentation_prob=0.5,
    )

    print(
        f"ğŸ“Š Mixed Dataset: {len(mixed_dataset_wrapper)} frames (with replay)")
    print(f"   Weights: {mixed_dataset_wrapper.weights}")

    # ç®€åŒ–çš„collateå‡½æ•°ï¼ˆåªéœ€è¦åŸºæœ¬çš„batch collationï¼‰
    def collate_fn_with_padding(batch):
        """ç®€åŒ–çš„collateå‡½æ•°ï¼ŒåªåšåŸºæœ¬çš„batch collation"""
        from torch.utils.data._utils.collate import default_collate
        return default_collate(batch)

    # ä¼˜åŒ–DataLoaderé…ç½®ï¼š
    # 1. å¢åŠ prefetch_factoråˆ°2-4ï¼ˆæå‡é¢„å–æ•ˆç‡ï¼‰
    # 2. æ·»åŠ persistent_workers=Trueï¼ˆé¿å…æ¯ä¸ªepoché‡æ–°åˆ›å»ºworkerï¼‰
    # 3. æ ¹æ®CPUæ ¸å¿ƒæ•°åŠ¨æ€è°ƒæ•´num_workersï¼ˆå¦‚æœå¯ç”¨ï¼‰
    import os
    max_workers = cfg.training.num_workers
    try:
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        # å»ºè®®ä½¿ç”¨CPUæ ¸å¿ƒæ•°-1ï¼Œä½†ä¸è¶…è¿‡é…ç½®çš„max_workers
        suggested_workers = min(cpu_count - 1, max(max_workers, 20))
        if suggested_workers > max_workers:
            print(
                f"ğŸ’¡ å»ºè®®å°†num_workersä»{max_workers}å¢åŠ åˆ°{suggested_workers}ä»¥æå‡IOå¯†é›†å‹ä»»åŠ¡æ€§èƒ½")
    except:
        suggested_workers = max_workers

    return DataLoader(
        mixed_dataset_wrapper,
        batch_size=cfg.training.batch_size,
        num_workers=max_workers,
        shuffle=True,
        pin_memory=(cfg.training.device != 'cpu'),
        drop_last=cfg.training.drop_last,
        collate_fn=collate_fn_with_padding,
        prefetch_factor=2,  # ä»1å¢åŠ åˆ°2ï¼Œæå‡é¢„å–æ•ˆç‡
        persistent_workers=True if max_workers > 0 else False,  # æ·»åŠ persistent_workers
    )


def validate_all_tasks(
    policy: SmolVLAPolicyWrapper,
    cfg: DictConfig,
    current_task_id: int,
    device: torch.device,
    cfg_root: Path,
    dataset_fps: int = 10
) -> Dict[int, float]:
    """
    éªŒè¯æ‰€æœ‰ä¹‹å‰çš„ä»»åŠ¡ï¼ˆæ£€æµ‹é—å¿˜ï¼‰

    Args:
        policy: SmolVLAç­–ç•¥
        cfg: é…ç½®
        current_task_id: å½“å‰ä»»åŠ¡ID
        device: è®¾å¤‡
        cfg_root: é…ç½®æ ¹ç›®å½•

    Returns:
        validation_results: {task_id: avg_loss}
    """
    print("\n" + "="*70)
    print(f"ğŸ” Multi-Task Validation (Tasks 1-{current_task_id})")
    print("="*70)

    policy.eval()
    validation_results = {}

    for task_id in range(1, current_task_id + 1):
        print(f"\nğŸ“Š Validating Task {task_id}...")

        # åŠ è½½ä»»åŠ¡é…ç½®
        task_cfg = load_task_config(cfg_root, task_id)

        # åŠ è½½éªŒè¯é›†ï¼ˆä½¿ç”¨å‰Nä¸ªepisodesä½œä¸ºéªŒè¯ï¼Œé¿å…ä¸è®­ç»ƒæ•°æ®å®Œå…¨åˆ†ç¦»ï¼‰
        # æ³¨æ„ï¼šè¿™æ˜¯å¿«é€ŸéªŒè¯æ–¹æ³•ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„å­é›†
        num_val_episodes = cfg.training.validation_episodes

        # ä»è®­ç»ƒepisodesä¸­é€‰æ‹©å‰Nä¸ªä½œä¸ºéªŒè¯
        train_episode_start = task_cfg.task.data.episodes_to_use[0]
        train_episode_end = task_cfg.task.data.episodes_to_use[1]

        # éªŒè¯ç”¨å‰Nä¸ªepisodes
        val_episode_end = min(train_episode_start +
                              num_val_episodes - 1, train_episode_end)
        val_episodes = list(range(train_episode_start, val_episode_end + 1))

        # ç¡®ä¿ä¸è¶…è¿‡num_val_episodes
        val_episodes = val_episodes[:num_val_episodes]

        # æ„å»ºdelta_timestampsé…ç½®
        chunk_size = cfg.policy.chunk_size
        delta_timestamps = {
            "observation.state": [0],
            "action": [i / dataset_fps for i in range(chunk_size)],
        }

        val_dataset = LeRobotDataset(
            task_cfg.task.data.repoid,
            root=task_cfg.task.data.root,
            episodes=val_episodes,
            delta_timestamps=delta_timestamps,
            video_backend=get_optimal_video_backend()  # ä¼˜åŒ–ï¼šä½¿ç”¨æœ€ä¼˜è§†é¢‘åç«¯
        )

        val_loader = create_dataloader_with_language(
            val_dataset,
            task_cfg.task.language_instruction,
            batch_size=cfg.training.batch_size,
            num_workers=cfg.training.num_workers // 2,
            pin_memory=(device.type != 'cpu'),
            drop_last=False
        )

        # éªŒè¯
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Task {task_id} Validation", leave=False):
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                         for k, v in batch.items()}

                loss, _ = policy.forward(batch)
                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / \
            num_batches if num_batches > 0 else float('inf')
        validation_results[task_id] = avg_loss

        print(f"  Task {task_id} Validation Loss: {avg_loss:.4f}")

    # åˆ†æé—å¿˜æƒ…å†µ
    if current_task_id > 1:
        print("\nâš ï¸  Forgetting Analysis:")
        for task_id in range(1, current_task_id):
            loss = validation_results[task_id]
            # ç®€å•çš„é˜ˆå€¼åˆ¤æ–­
            if loss < 0.7:
                status = "âœ… Well Retained"
            elif loss < 1.0:
                status = "âš ï¸  Slight Degradation"
            else:
                status = "âŒ Significant Forgetting"

            print(f"  Task {task_id}: {status} (loss={loss:.4f})")

    print("="*70 + "\n")

    policy.train()
    return validation_results


@hydra.main(config_path="../configs/policy/", config_name="smolvla_sequential_base", version_base=None)
def main(cfg: DictConfig):
    """ä¸»è®­ç»ƒæµç¨‹"""

    # è®¾ç½® HuggingFace é•œåƒæºä»¥æé«˜ä¸‹è½½é€Ÿåº¦
    import os

    # ä»é…ç½®è¯»å– HF endpointï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤é•œåƒæº
    hf_endpoint = cfg.get('hf_endpoint', 'https://hf-mirror.com')
    if hf_endpoint:
        os.environ['HF_ENDPOINT'] = hf_endpoint
        print(f"âœ… å·²è®¾ç½® HuggingFace ä¸‹è½½æº: {hf_endpoint}")
    else:
        print("â„¹ï¸  ä½¿ç”¨é»˜è®¤ HuggingFace Hub: https://huggingface.co")

    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

    logger = setup_logging()
    set_seed(cfg.training.seed)

    # åŠ è½½ä»»åŠ¡é…ç½®
    # ä»Hydraé…ç½®è·å–ä»»åŠ¡åç§°ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼štasks/task1_moving_grasp æˆ– task1_moving_graspï¼‰
    task_param = cfg.get('task', 'task1_moving_grasp')
    if task_param.startswith('tasks/'):
        task_param = task_param.replace('tasks/', '')

    # åŠ¨æ€åŠ è½½ä»»åŠ¡é…ç½®
    cfg_root = Path(__file__).parent.parent / "configs/policy"
    task_cfg = load_task_config(cfg_root, int(
        task_param.split('_')[0].replace('task', '')))
    task_id = task_cfg.task.id
    task_name = task_cfg.task.name

    # è®¾ç½®taskå­—æ®µç”¨äºè·¯å¾„ï¼ˆæ ¼å¼ï¼štask{id}_{name}ï¼Œå¦‚task1_moving_graspï¼‰
    cfg.task = f"task{task_id}_{task_name}"

    print("\n" + "="*70)
    print(f"ğŸ¤– SmolVLA Sequential Training - Stage {task_id}")
    print("="*70)
    print(f"Task ID: {task_id}")
    print(f"Task Name: {task_name}")
    print(f"Description: {task_cfg.task.description}")
    print(f"Language: {task_cfg.task.language_instruction}")
    print("="*70 + "\n")

    # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–ç­–ç•¥ä¸€è‡´çš„æ ¼å¼ï¼‰
    # æ ¼å¼: outputs/train/{task}/{method}/run_{timestamp}
    # å±•å¼€: outputs/train/task1_moving_grasp/smolvla_sequential/run_20251011_123456
    output_directory = Path(
        cfg.training.output_directory) / f"run_{cfg.timestamp}"
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    print(f"ğŸ“ Output Directory: {output_directory}")
    print(f"ğŸ“… Timestamp: {cfg.timestamp}\n")

    device = torch.device(cfg.training.device)

    # ==================== åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯ ====================
    print("ğŸ“‚ Loading Dataset Metadata...")
    dataset_metadata = LeRobotDatasetMetadata(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root
    )

    # è·å–æ•°æ®é›†fpsï¼ˆç”¨äºé…ç½®delta_timestampsï¼‰
    dataset_fps = dataset_metadata.fps
    print(f"ğŸ“Š Dataset FPS: {dataset_fps}")

    # æ„å»ºfeatures
    features = dataset_to_policy_features(dataset_metadata.features)
    input_features = {k: ft for k, ft in features.items(
    ) if ft.type is not FeatureType.ACTION}
    output_features = {k: ft for k,
                       ft in features.items() if ft.type is FeatureType.ACTION}

    dataset_stats = dataset_metadata.stats

    # å¡«å……dataset_statsåˆ°ç›®æ ‡ç»´åº¦ï¼ˆKuavo 16ç»´ â†’ SmolVLA 32ç»´ï¼‰
    print("ğŸ“ Padding dataset_stats to match SmolVLA dimensions (16D â†’ 32D)...")
    dataset_stats = pad_dataset_stats(
        dataset_stats,
        target_action_dim=cfg.policy.max_action_dim,
        target_state_dim=cfg.policy.max_state_dim
    )
    print("âœ… Dataset stats padded successfully")

    # ==================== æ„å»ºPolicyé…ç½® ====================
    from hydra.utils import instantiate

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=device,
    )

    # Override learning rate from task config
    if hasattr(task_cfg.task.training, 'policy'):
        policy_cfg.optimizer_lr = task_cfg.task.training.policy.optimizer_lr
        policy_cfg.scheduler_warmup_steps = task_cfg.task.training.policy.scheduler_warmup_steps
        policy_cfg.scheduler_decay_steps = task_cfg.task.training.policy.scheduler_decay_steps

    # ==================== åŠ è½½/åˆ›å»ºæ¨¡å‹ ====================
    if task_cfg.task.training.resume_from == 'pretrained':
        # Stage 1: ä»HuggingFaceé¢„è®­ç»ƒåŠ è½½
        print(
            f"\nğŸ“‚ Loading pretrained SmolVLA from {task_cfg.task.training.pretrained_path}")
        policy = SmolVLAPolicyWrapper.from_pretrained(
            task_cfg.task.training.pretrained_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )

    elif task_cfg.task.training.resume_from == 'task':
        # Stage 2+: ä»ä¸Šä¸€ä¸ªä»»åŠ¡ç»§ç»­
        prev_task_id = task_cfg.task.training.resume_task_id
        resume_path = task_cfg.task.training.resume_path

        print(f"\nğŸ“‚ Loading from Task {prev_task_id}: {resume_path}")
        policy = SmolVLAPolicyWrapper.from_pretrained(
            resume_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )
        print(
            f"âœ… Successfully loaded Task {prev_task_id} model for sequential training")

    else:
        # ä»å¤´è®­ç»ƒï¼ˆä¸æ¨èï¼‰
        print("\nâš ï¸  Training from scratch (not recommended for sequential training)")
        policy = SmolVLAPolicyWrapper(policy_cfg, dataset_stats)

    policy = policy.to(device)

    policy.train()

    # ==================== å‡†å¤‡æ•°æ® ====================
    # åŠ è½½replay bufferï¼ˆå¦‚æœéœ€è¦ï¼‰
    replay_manager = None
    if task_id > 1 and cfg.sequential.use_replay_buffer:
        cfg_root = Path(__file__).parent.parent / "configs/policy"
        replay_manager = ReplayDatasetManager(
            cfg, task_id, cfg_root, dataset_fps)
        replay_manager.load_replay_tasks()

    # åˆ›å»ºdataloaderï¼ˆä¼ é€’dataset_fpsï¼‰
    dataloader = create_mixed_dataloader(
        cfg, task_cfg, replay_manager, dataset_fps)

    # ==================== æ„å»ºä¼˜åŒ–å™¨ ====================
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())
    lr_scheduler = policy.config.get_scheduler_preset().build(
        optimizer,
        num_training_steps=task_cfg.task.training.max_epoch * len(dataloader)
    )

    print(f"\nğŸ¯ Training Configuration:")
    print(f"   Epochs: {task_cfg.task.training.max_epoch}")
    print(f"   Batch Size: {cfg.training.batch_size}")
    print(f"   Learning Rate: {policy_cfg.optimizer_lr}")
    print(f"   Steps per Epoch: {len(dataloader)}")
    print(
        f"   Total Steps: {task_cfg.task.training.max_epoch * len(dataloader)}")

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\nğŸš€ Starting Training...")
    print("="*70 + "\n")

    best_loss = float('inf')

    # æ€§èƒ½ç›‘æ§ï¼šbatchå¤„ç†æ—¶é—´ç»Ÿè®¡
    batch_process_times = []

    for epoch in range(task_cfg.task.training.max_epoch):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch + 1}/{task_cfg.task.training.max_epoch}")
        print(f"{'='*70}")

        # è®­ç»ƒ
        policy.train()
        total_loss = 0.0
        num_batches = 0

        # Epochçº§åˆ«çš„æ€§èƒ½ç»Ÿè®¡
        epoch_batch_process_time = 0.0

        epoch_bar = tqdm(
            dataloader,
            desc=f"Training Epoch {epoch+1}",
            dynamic_ncols=True,
            leave=False
        )

        for batch_idx, batch in enumerate(epoch_bar):
            # è®°å½•batchå¤„ç†æ—¶é—´
            batch_process_start = time.time()

            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in batch.items()}

            # Forward
            loss, _ = policy.forward(batch)

            # Backward
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                policy.parameters(),
                max_norm=policy_cfg.optimizer_grad_clip_norm
            )

            # Optimizer step
            optimizer.step()
            optimizer.zero_grad()
            lr_scheduler.step()

            batch_process_time = time.time() - batch_process_start
            epoch_batch_process_time += batch_process_time

            # Logging
            total_loss += loss.item()
            num_batches += 1

            epoch_bar.set_postfix(
                loss=f"{loss.item():.4f}",
                lr=f"{lr_scheduler.get_last_lr()[0]:.2e}",
                proc_ms=f"{batch_process_time*1000:.1f}"  # å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            )

        avg_loss = total_loss / num_batches
        avg_batch_process_time = epoch_batch_process_time / num_batches

        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f}")
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   - å¹³å‡batchå¤„ç†æ—¶é—´: {avg_batch_process_time*1000:.2f}ms/batch")
        print(
            f"   - ç†è®ºååé‡: {cfg.training.batch_size / avg_batch_process_time:.1f} samples/s")

        # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
        batch_process_times.append(avg_batch_process_time)

        # TensorBoard logging
        writer.add_scalar("train/loss", avg_loss, epoch)
        writer.add_scalar("train/lr", lr_scheduler.get_last_lr()[0], epoch)
        writer.add_scalar("performance/batch_process_time_ms",
                          avg_batch_process_time * 1000, epoch)
        writer.add_scalar("performance/throughput_samples_per_sec",
                          cfg.training.batch_size / avg_batch_process_time, epoch)

        # å¤šä»»åŠ¡éªŒè¯
        if (epoch + 1) % cfg.training.validation_freq_epoch == 0 and cfg.training.get('validate_all_previous_tasks', False):
            cfg_root = Path(__file__).parent.parent / "configs/policy"
            validation_results = validate_all_tasks(
                policy, cfg, task_id, device, cfg_root, dataset_fps)

            # Log validation results
            for val_task_id, val_loss in validation_results.items():
                writer.add_scalar(
                    f"validation/task{val_task_id}_loss", val_loss, epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            save_rng_state(best_path / "rng_state.pth")

            # ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºå®Œç¾æ¢å¤è®­ç»ƒï¼‰
            checkpoint = {
                "optimizer": optimizer.state_dict(),
                "lr_scheduler": lr_scheduler.state_dict(),
                "epoch": epoch + 1,
                "best_loss": best_loss
            }
            torch.save(checkpoint, best_path / "learning_state.pth")

            print(f"âœ… Best model saved: loss={best_loss:.4f}")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            save_rng_state(epoch_path / "rng_state.pth")

            # ä¿å­˜è®­ç»ƒçŠ¶æ€
            checkpoint = {
                "optimizer": optimizer.state_dict(),
                "lr_scheduler": lr_scheduler.state_dict(),
                "epoch": epoch + 1,
                "best_loss": best_loss
            }
            torch.save(checkpoint, epoch_path / "learning_state.pth")

            print(f"âœ… Checkpoint saved: epoch {epoch+1}")

    writer.close()

    # ==================== ä¿å­˜æœ€ç»ˆçŠ¶æ€ ====================
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºå®Œç¾æ¢å¤æˆ–ç»§ç»­è®­ç»ƒï¼‰
    print("\nğŸ’¾ Saving final model and training state...")
    policy.save_pretrained(output_directory)
    save_rng_state(output_directory / "rng_state.pth")

    # ä¿å­˜æœ€ç»ˆè®­ç»ƒçŠ¶æ€
    final_checkpoint = {
        "optimizer": optimizer.state_dict(),
        "lr_scheduler": lr_scheduler.state_dict(),
        "epoch": task_cfg.task.training.max_epoch,
        "best_loss": best_loss
    }
    torch.save(final_checkpoint, output_directory / "learning_state.pth")
    print("âœ… Final model and training state saved")

    # ==================== æœ€ç»ˆéªŒè¯ ====================
    print("\n" + "="*70)
    print("ğŸ¯ Final Multi-Task Validation")
    print("="*70)

    cfg_root = Path(__file__).parent.parent / "configs/policy"
    final_results = validate_all_tasks(
        policy, cfg, task_id, device, cfg_root, dataset_fps)

    # ä¿å­˜è®­ç»ƒç»“æœ
    results_file = output_directory / "training_results.json"

    # è®¡ç®—å¹³å‡æ€§èƒ½ç»Ÿè®¡
    avg_batch_process_time = sum(
        batch_process_times) / len(batch_process_times) if batch_process_times else 0

    with open(results_file, 'w') as f:
        json.dump({
            'task_id': task_id,
            'task_name': task_name,
            'description': task_cfg.task.description,
            'language_instruction': task_cfg.task.language_instruction,
            'best_loss': best_loss,
            'final_validation': {str(k): v for k, v in final_results.items()},
            'training_epochs': task_cfg.task.training.max_epoch,
            'learning_rate': policy_cfg.optimizer_lr,
            'performance': {
                'avg_batch_process_time_ms': avg_batch_process_time * 1000,
                'throughput_samples_per_sec': cfg.training.batch_size / avg_batch_process_time if avg_batch_process_time > 0 else 0,
            }
        }, f, indent=2)

    print(f"\n{'='*70}")
    print(f"âœ… Task {task_id} Training Completed!")
    print(f"{'='*70}")
    print(f"Best Loss: {best_loss:.4f}")
    print(f"Model saved to: {output_directory}")
    print(f"Results saved to: {results_file}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
