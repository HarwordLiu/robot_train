# -*- coding: utf-8 -*-
"""
ä»»åŠ¡ç‰¹å®šåˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyè®­ç»ƒè„šæœ¬

ä¸“é—¨ç”¨äºä»»åŠ¡ç‰¹å®šçš„åˆ†å±‚æ¶æ„è®­ç»ƒï¼Œæ”¯æŒï¼š
- ä»»åŠ¡æ¸è¿›å¼æ·»åŠ 
- é˜²é—å¿˜æœºåˆ¶
- ä»»åŠ¡æ¡ä»¶å±‚æƒé‡è°ƒæ•´
- æ™ºèƒ½è¯¾ç¨‹å­¦ä¹ ç­–ç•¥

ä½¿ç”¨æ–¹æ³•ï¼š
python kuavo_train/train_hierarchical_task_specific.py --config-name=humanoid_diffusion_config
"""

import lerobot_patches.custom_patches  # ç¡®ä¿è‡ªå®šä¹‰è¡¥ä¸ç”Ÿæ•ˆï¼Œä¸è¦åˆ é™¤è¿™è¡Œï¼
from lerobot.configs.policies import PolicyFeature
from typing import Any, Dict, List, Optional, Tuple
import os
import hydra
from omegaconf import DictConfig, OmegaConf, ListConfig
from pathlib import Path
from functools import partial

import torch
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, WeightedRandomSampler, ConcatDataset
from tqdm import tqdm
import shutil
from hydra.utils import instantiate
from diffusers.optimization import get_scheduler
import numpy as np
import logging

from lerobot.configs.types import FeatureType, NormalizationMode
from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.utils.random_utils import set_seed

# å¯¼å…¥åˆ†å±‚æ¶æ„æ¨¡å—
from kuavo_train.wrapper.policy.humanoid.HumanoidDiffusionPolicy import HumanoidDiffusionPolicy
from kuavo_train.wrapper.policy.humanoid.TaskSpecificTrainingManager import TaskSpecificTrainingManager
from kuavo_train.wrapper.dataset.LeRobotDatasetWrapper import CustomLeRobotDataset
from kuavo_train.utils.augmenter import crop_image, resize_image, DeterministicAugmenterColor
from kuavo_train.utils.utils import save_rng_state, load_rng_state
from utils.transforms import ImageTransforms, ImageTransformsConfig, ImageTransformConfig

from functools import partial
from contextlib import nullcontext


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('task_specific_training.log', encoding='utf-8')
        ]
    )
    return logging.getLogger("TaskSpecificTraining")


def build_augmenter(cfg):
    """æ„å»ºå›¾åƒå¢å¼ºå™¨"""
    img_tf_cfg = ImageTransformsConfig(
        enable=cfg.get("enable", False),
        max_num_transforms=cfg.get("max_num_transforms", 3),
        random_order=cfg.get("random_order", False),
        tfs={}
    )

    if "tfs" in cfg:
        for name, tf_dict in cfg["tfs"].items():
            img_tf_cfg.tfs[name] = ImageTransformConfig(
                weight=tf_dict.get("weight", 1.0),
                type=tf_dict.get("type", "Identity"),
                kwargs=tf_dict.get("kwargs", {}),
            )
    return ImageTransforms(img_tf_cfg)


def build_delta_timestamps(dataset_metadata, policy_cfg):
    """æ„å»ºdelta timestamps"""
    obs_indices = getattr(policy_cfg, "observation_delta_indices", None)
    act_indices = getattr(policy_cfg, "action_delta_indices", None)
    if obs_indices is None and act_indices is None:
        return None

    delta_timestamps = {}
    for key in dataset_metadata.info["features"]:
        if "observation" in key and obs_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in obs_indices]
        elif "action" in key and act_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in act_indices]

    return delta_timestamps if delta_timestamps else None


def build_optimizer_and_scheduler(policy, cfg, total_frames):
    """æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())

    if cfg.training.max_training_step is None:
        updates_per_epoch = (
            total_frames // (cfg.training.batch_size * cfg.training.accumulation_steps)) + 1
        num_training_steps = cfg.training.max_epoch * updates_per_epoch
    else:
        num_training_steps = cfg.training.max_training_step

    lr_scheduler = policy.config.get_scheduler_preset()
    if lr_scheduler is not None:
        lr_scheduler = lr_scheduler.build(optimizer, num_training_steps)
    else:
        lr_scheduler = get_scheduler(
            name=cfg.training.scheduler_name,
            optimizer=optimizer,
            num_warmup_steps=cfg.training.scheduler_warmup_steps,
            num_training_steps=num_training_steps,
        )

    return optimizer, lr_scheduler


def build_policy_config(cfg, input_features, output_features):
    """æ„å»ºpolicyé…ç½®"""
    def _normalize_feature_dict(d: Any) -> dict[str, PolicyFeature]:
        if isinstance(d, DictConfig):
            d = OmegaConf.to_container(d, resolve=True)
        if not isinstance(d, dict):
            raise TypeError(
                "Expected dict or DictConfig, got {}".format(type(d)))

        return {
            k: PolicyFeature(**v) if isinstance(v,
                                                dict) and not isinstance(v, PolicyFeature) else v
            for k, v in d.items()
        }

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=cfg.training.device,
    )

    policy_cfg.input_features = _normalize_feature_dict(
        policy_cfg.input_features)
    policy_cfg.output_features = _normalize_feature_dict(
        policy_cfg.output_features)
    return policy_cfg


def build_hierarchical_policy(policy_cfg, dataset_stats):
    """æ„å»ºåˆ†å±‚æ¶æ„çš„policy"""
    print("ğŸ¤– æ„å»ºä»»åŠ¡ç‰¹å®šçš„HumanoidDiffusionPolicy...")
    return HumanoidDiffusionPolicy(policy_cfg, dataset_stats)


def load_task_dataset(task_id: int, cfg: DictConfig, policy_cfg, image_transforms) -> Tuple[Optional[LeRobotDataset], Optional[LeRobotDatasetMetadata]]:
    """åŠ è½½ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†"""
    # ä¸´æ—¶ä¿®æ”¹ï¼šç›´æ¥ä»rooté…ç½®è¯»å–ä»»åŠ¡ä¸€æ•°æ®
    if task_id == 1:
        # ä½¿ç”¨rooté…ç½®çš„æ•°æ®è·¯å¾„
        task_data_path = cfg.get('root', '')
        if not task_data_path:
            print(f"âš ï¸  rooté…ç½®æœªè®¾ç½®")
            return None, None

        if not os.path.exists(task_data_path):
            print(f"âš ï¸  rootæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {task_data_path}")
            return None, None
    else:
        # å…¶ä»–ä»»åŠ¡ä»ä½¿ç”¨åŸæœ‰é€»è¾‘
        task_config = cfg.get('task_specific_training', {})
        data_config = task_config.get('data_config', {})

        # æ„å»ºä»»åŠ¡æ•°æ®è·¯å¾„
        base_path = data_config.get('base_path', '/robot/data')
        task_dir = data_config.get('task_directories', {}).get(task_id)

        if not task_dir:
            print(f"âš ï¸  ä»»åŠ¡{task_id}çš„æ•°æ®ç›®å½•æœªé…ç½®")
            return None, None

        task_data_path = os.path.join(base_path, task_dir)
        if not os.path.exists(task_data_path):
            print(f"âš ï¸  ä»»åŠ¡{task_id}æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {task_data_path}")
            return None, None

    try:
        # åŠ è½½ä»»åŠ¡æ•°æ®é›†å…ƒæ•°æ®
        task_repoid = f"lerobot/task_{task_id}"
        dataset_metadata = LeRobotDatasetMetadata(
            task_repoid, root=task_data_path)

        # æ„å»ºdelta timestamps
        delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)

        # åº”ç”¨episodeé™åˆ¶
        max_episodes = cfg.task_specific_training.memory_management.get(
            'max_episodes_per_task', 300)
        episodes_to_use = list(
            range(min(max_episodes, dataset_metadata.info["total_episodes"])))

        # åˆ›å»ºæ•°æ®é›†
        dataset = LeRobotDataset(
            task_repoid,
            delta_timestamps=delta_timestamps,
            root=task_data_path,
            episodes=episodes_to_use,
            image_transforms=image_transforms,
        )

        print(f"âœ… ä»»åŠ¡{task_id}æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(episodes_to_use)}ä¸ªepisodes")
        return dataset, dataset_metadata

    except Exception as e:
        print(f"âŒ ä»»åŠ¡{task_id}æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None, None


def create_task_specific_dataloader(datasets: Dict[int, LeRobotDataset], task_manager: TaskSpecificTrainingManager,
                                    cfg: DictConfig, device: torch.device) -> DataLoader:
    """åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„æ•°æ®åŠ è½½å™¨"""
    if len(datasets) == 1:
        # å•ä»»åŠ¡æƒ…å†µ
        task_id = list(datasets.keys())[0]
        dataset = datasets[task_id]
        return DataLoader(
            dataset,
            batch_size=cfg.training.batch_size,
            shuffle=True,
            num_workers=cfg.training.num_workers,
            pin_memory=(device.type != "cpu"),
            drop_last=cfg.training.drop_last,
            prefetch_factor=1,
        )

    # å¤šä»»åŠ¡æƒ…å†µ - ä½¿ç”¨åŠ æƒé‡‡æ ·
    sampling_strategy = task_manager.get_task_data_sampling_strategy()
    task_weights = sampling_strategy.get("task_weights", {})

    combined_datasets = []
    sample_weights = []

    for task_id, dataset in datasets.items():
        task_weight = task_weights.get(task_id, 1.0 / len(datasets))
        dataset_size = len(dataset)

        combined_datasets.append(dataset)
        # ä¸ºè¯¥ä»»åŠ¡çš„æ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
        sample_weights.extend([task_weight] * dataset_size)

    # åˆå¹¶æ•°æ®é›†
    combined_dataset = ConcatDataset(combined_datasets)

    # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(combined_dataset),
        replacement=True
    )

    return DataLoader(
        combined_dataset,
        batch_size=cfg.training.batch_size,
        sampler=sampler,
        num_workers=cfg.training.num_workers,
        pin_memory=(device.type != "cpu"),
        drop_last=cfg.training.drop_last,
        prefetch_factor=1,
    )


def run_task_specific_curriculum_stage(policy, stage_config: Dict[str, Any], dataloader: DataLoader,
                                       task_manager: TaskSpecificTrainingManager, cfg: DictConfig,
                                       device: torch.device, writer: SummaryWriter,
                                       optimizer, lr_scheduler, scaler, output_directory, amp_enabled: bool,
                                       current_step: int, start_epoch: int = 0) -> int:
    """è¿è¡Œä»»åŠ¡ç‰¹å®šçš„è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ"""
    stage_name = stage_config.get("name", "unknown")
    enabled_layers = stage_config.get("layers", [])
    stage_epochs = stage_config.get("epochs", 10)
    target_task = stage_config.get("target_task")

    print(f"ğŸ“ å¼€å§‹ä»»åŠ¡ç‰¹å®šè¯¾ç¨‹é˜¶æ®µ: {stage_name}")
    print(f"   æ¿€æ´»å±‚: {enabled_layers}")
    print(f"   ç›®æ ‡ä»»åŠ¡: {target_task}")
    print(f"   è®­ç»ƒè½®æ¬¡: {stage_epochs}")
    print(f"ğŸ”§ ä»»åŠ¡ç‰¹å®šè¯¾ç¨‹å­¦ä¹ é˜¶æ®µå‚æ•°:")
    print(f"   - output_directory: {output_directory}")
    print(f"   - optimizer: {optimizer is not None}")
    print(f"   - lr_scheduler: {lr_scheduler is not None}")
    print(f"   - scaler: {scaler is not None}")
    print(f"   - amp_enabled: {amp_enabled}")
    print(f"   - save_freq_epoch: {cfg.training.save_freq_epoch}")

    # é…ç½®policyçš„è¯¾ç¨‹å­¦ä¹ çŠ¶æ€
    if hasattr(policy, 'set_curriculum_stage'):
        policy.set_curriculum_stage(enabled_layers)

    # è·å–ä»»åŠ¡ç‰¹å®šçš„å±‚æƒé‡
    if target_task and target_task != "all":
        layer_weights = task_manager.get_task_specific_layer_weights(
            target_task)
        if hasattr(policy, 'set_task_layer_weights'):
            policy.set_task_layer_weights(layer_weights)

    has_torch_autocast = hasattr(torch, "autocast")

    def make_autocast(enabled: bool):
        if not enabled:
            return nullcontext()
        if device.type == "cuda":
            if has_torch_autocast:
                return torch.autocast(device_type="cuda")
            else:
                from torch.cuda.amp import autocast as cuda_autocast
                return cuda_autocast()
        return nullcontext()

    stage_steps = 0
    total_loss = 0.0
    best_stage_loss = float('inf')

    for epoch in range(start_epoch, stage_epochs):
        print(f"ğŸš€ å¼€å§‹ Epoch {epoch+1}/{stage_epochs}")
        epoch_bar = tqdm(
            dataloader, desc=f"è¯¾ç¨‹é˜¶æ®µ {stage_name} Epoch {epoch+1}/{stage_epochs}")
        epoch_loss = 0.0

        for batch in epoch_bar:
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            # æ·»åŠ ä»»åŠ¡ç‰¹å®šä¿¡æ¯åˆ°batch
            curriculum_info = {
                'stage': stage_name,
                'enabled_layers': enabled_layers,
                'target_task': target_task
            }

            with make_autocast(amp_enabled):
                loss, hierarchical_info = policy.forward(
                    batch, curriculum_info=curriculum_info)

            scaled_loss = loss / cfg.training.accumulation_steps

            if amp_enabled:
                scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            if stage_steps % cfg.training.accumulation_steps == 0:
                if amp_enabled:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                lr_scheduler.step()

            # è®°å½•æ—¥å¿—
            if stage_steps % cfg.training.log_freq == 0:
                writer.add_scalar(
                    f"curriculum/{stage_name}/loss", scaled_loss.item(), current_step + stage_steps)

                # è®°å½•ä»»åŠ¡ç‰¹å®šä¿¡æ¯
                if isinstance(hierarchical_info, dict):
                    for key, value in hierarchical_info.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(
                                f"curriculum/{stage_name}/{key}", value, current_step + stage_steps)

                epoch_bar.set_postfix(
                    loss=f"{scaled_loss.item():.3f}",
                    stage=stage_name,
                    target=target_task
                )

            stage_steps += 1
            total_loss += scaled_loss.item()
            epoch_loss += scaled_loss.item()

        print(f"ğŸ Epoch {epoch+1} è®­ç»ƒå®Œæˆï¼Œå¼€å§‹ä¿å­˜æ£€æŸ¥ç‚¹...")
        # è®¡ç®—å¹³å‡epochæŸå¤±
        avg_epoch_loss = epoch_loss / len(dataloader)
        print(
            f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}, å½“å‰æœ€ä½³æŸå¤±: {best_stage_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_stage_loss and output_directory is not None:
            print(
                f"ğŸ¯ å‘ç°æ›´å¥½çš„æ¨¡å‹! æŸå¤±ä» {best_stage_loss:.4f} æ”¹å–„åˆ° {avg_epoch_loss:.4f}")
            best_stage_loss = avg_epoch_loss
            best_save_path = output_directory / \
                "curriculum_{}_best".format(stage_name)
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_save_path}")
            try:
                policy.save_pretrained(best_save_path)
                save_rng_state(best_save_path / "rng_state.pth")
                print(f"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜æˆåŠŸ: {best_save_path}")
            except Exception as e:
                print(f"âŒ æœ€ä½³æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        elif avg_epoch_loss >= best_stage_loss:
            print(
                f"ğŸ“‰ æŸå¤±æœªæ”¹å–„ï¼Œè·³è¿‡æœ€ä½³æ¨¡å‹ä¿å­˜ (å½“å‰: {avg_epoch_loss:.4f}, æœ€ä½³: {best_stage_loss:.4f})")
        elif output_directory is None:
            print(f"âš ï¸  output_directory ä¸º Noneï¼Œè·³è¿‡æœ€ä½³æ¨¡å‹ä¿å­˜")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        print(
            f"ğŸ” æ£€æŸ¥å®šæœŸä¿å­˜æ¡ä»¶: output_directory={output_directory is not None}, epoch={epoch+1}, save_freq_epoch={cfg.training.save_freq_epoch}")
        if output_directory is not None and (epoch + 1) % cfg.training.save_freq_epoch == 0:
            checkpoint_save_path = output_directory / \
                "curriculum_{}_epoch{}".format(stage_name, epoch + 1)
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹åˆ°: {checkpoint_save_path}")
            try:
                policy.save_pretrained(checkpoint_save_path)
                save_rng_state(checkpoint_save_path / "rng_state.pth")
                print(f"âœ… å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {checkpoint_save_path}")
            except Exception as e:
                print(f"âŒ å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜è¯¾ç¨‹å­¦ä¹ é˜¶æ®µçš„è¯¦ç»†çŠ¶æ€
            if optimizer is not None and lr_scheduler is not None:
                print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä»»åŠ¡ç‰¹å®šè¯¦ç»†çŠ¶æ€...")
                try:
                    save_task_specific_checkpoint(
                        policy, optimizer, lr_scheduler, scaler,
                        current_step + stage_steps, epoch + 1, best_stage_loss,
                        output_directory, task_manager, amp_enabled
                    )
                    print(f"âœ… ä»»åŠ¡ç‰¹å®šçŠ¶æ€ä¿å­˜æˆåŠŸ")
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡ç‰¹å®šçŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
            else:
                print(f"âš ï¸  optimizer æˆ– lr_scheduler ä¸º Noneï¼Œè·³è¿‡è¯¦ç»†çŠ¶æ€ä¿å­˜")
        else:
            print(
                f"â­ï¸  è·³è¿‡å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜ (epoch {epoch+1} ä¸æ˜¯ {cfg.training.save_freq_epoch} çš„å€æ•°)")

        print(f"âœ… Epoch {epoch+1} æ£€æŸ¥ç‚¹ä¿å­˜å®Œæˆ")

    avg_stage_loss = total_loss / \
        stage_steps if stage_steps > 0 else float('inf')
    print(f"âœ… è¯¾ç¨‹é˜¶æ®µ {stage_name} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_stage_loss:.4f}")

    return current_step + stage_steps


def save_task_specific_checkpoint(policy, optimizer, lr_scheduler, scaler, steps, epoch, best_loss,
                                  output_directory: Path, task_manager: TaskSpecificTrainingManager,
                                  amp_enabled: bool):
    """ä¿å­˜ä»»åŠ¡ç‰¹å®šçš„æ£€æŸ¥ç‚¹"""
    print(f"ğŸ”§ å¼€å§‹ä¿å­˜ä»»åŠ¡ç‰¹å®šæ£€æŸ¥ç‚¹...")
    print(f"   - output_directory: {output_directory}")
    print(f"   - steps: {steps}, epoch: {epoch}, best_loss: {best_loss:.4f}")
    print(f"   - amp_enabled: {amp_enabled}")

    # ä¿å­˜policy
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ policy åˆ°: {output_directory}")
    try:
        policy.save_pretrained(output_directory)
        save_rng_state(output_directory / "rng_state.pth")
        print(f"âœ… Policy ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Policy ä¿å­˜å¤±è´¥: {e}")
        return

    # ä¿å­˜è®­ç»ƒçŠ¶æ€
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜è®­ç»ƒçŠ¶æ€...")
    try:
        checkpoint = {
            "optimizer": optimizer.state_dict(),
            "lr_scheduler": lr_scheduler.state_dict(),
            "scaler": scaler.state_dict() if amp_enabled else None,
            "steps": steps,
            "epoch": epoch,
            "best_loss": best_loss,
            # ä¿å­˜åˆ†å±‚æ¶æ„ç‰¹æœ‰çš„çŠ¶æ€
            "hierarchical_stats": policy.get_performance_stats() if hasattr(policy, 'get_performance_stats') else {},
            "layer_states": policy.get_layer_states() if hasattr(policy, 'get_layer_states') else {},
            # ä¿å­˜ä»»åŠ¡ç‰¹å®šçŠ¶æ€
            "available_tasks": task_manager.available_tasks,
            "current_phase": task_manager.current_training_phase
        }

        state_file = output_directory / "task_specific_learning_state.pth"
        torch.save(checkpoint, state_file)
        print(f"âœ… è®­ç»ƒçŠ¶æ€ä¿å­˜æˆåŠŸ: {state_file}")

        # ä¿å­˜éšæœºæ•°çŠ¶æ€
        rng_file = output_directory / "rng_state.pth"
        save_rng_state(rng_file)
        print(f"âœ… éšæœºæ•°çŠ¶æ€ä¿å­˜æˆåŠŸ: {rng_file}")

        # ä¿å­˜ä»»åŠ¡ç®¡ç†å™¨çŠ¶æ€
        task_manager.save_training_state(output_directory, epoch,
                                         policy.get_performance_stats() if hasattr(policy, 'get_performance_stats') else {})
        print(f"âœ… ä»»åŠ¡ç®¡ç†å™¨çŠ¶æ€ä¿å­˜æˆåŠŸ")

    except Exception as e:
        print(f"âŒ è®­ç»ƒçŠ¶æ€ä¿å­˜å¤±è´¥: {e}")


@hydra.main(config_path="../configs/policy/", config_name="humanoid_diffusion_config", version_base=None)
def main(cfg: DictConfig):
    """ä»»åŠ¡ç‰¹å®šè®­ç»ƒä¸»å‡½æ•°"""
    logger = setup_logging()
    set_seed(cfg.training.seed)

    print("ğŸ¯ ä»»åŠ¡ç‰¹å®šåˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyè®­ç»ƒ")
    print("=" * 70)
    print(f"ä»»åŠ¡: {cfg.task}")
    print(f"æ–¹æ³•: {cfg.method}")
    print(f"ä½¿ç”¨åˆ†å±‚æ¶æ„: {cfg.policy.get('use_hierarchical', False)}")
    print(
        f"ä»»åŠ¡ç‰¹å®šè®­ç»ƒ: {cfg.get('task_specific_training', {}).get('enable', False)}")

    # éªŒè¯é…ç½®
    if not cfg.policy.get('use_hierarchical', False):
        logger.warning("âš ï¸  use_hierarchicalä¸ºFalseã€‚è¯·åœ¨é…ç½®ä¸­è®¾ä¸ºTrueä»¥ä½¿ç”¨åˆ†å±‚æ¶æ„ã€‚")

    if not cfg.get('task_specific_training', {}).get('enable', False):
        logger.warning("âš ï¸  task_specific_trainingæœªå¯ç”¨ã€‚")

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_directory = Path(cfg.training.output_directory) / \
        f"task_specific_run_{cfg.timestamp}"
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    device = torch.device(cfg.training.device)

    # åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨
    task_manager = TaskSpecificTrainingManager(cfg)

    # æ£€æµ‹å¯ç”¨ä»»åŠ¡æ•°æ®
    task_config = cfg.get('task_specific_training', {})
    available_tasks = task_config.get(
        'data_config', {}).get('task_directories', {})

    print(f"ğŸ” æ£€æµ‹å¯ç”¨ä»»åŠ¡æ•°æ®...")
    datasets = {}
    dataset_metadatas = {}

    # æ„å»ºå›¾åƒå¢å¼ºå™¨
    image_transforms = build_augmenter(cfg.training.RGB_Augmenter)

    # å…ˆåŠ è½½ç¬¬ä¸€ä¸ªä»»åŠ¡ä»¥è·å–åŸºç¡€ç‰¹å¾ä¿¡æ¯
    first_task_loaded = False
    input_features = None
    output_features = None

    # ä¸´æ—¶ä¿®æ”¹ï¼šåªå¤„ç†ä»»åŠ¡ä¸€ï¼Œç›´æ¥ä»rooté…ç½®è¯»å–
    task_id = 1
    temp_dataset_metadata = None
    try:
        # ä½¿ç”¨rooté…ç½®çš„æ•°æ®è·¯å¾„
        task_data_path = cfg.get('root', '')

        if task_data_path and os.path.exists(task_data_path):
            task_repoid = f"lerobot/task_{task_id}"
            temp_dataset_metadata = LeRobotDatasetMetadata(
                task_repoid, root=task_data_path)

            if not first_task_loaded:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„ç‰¹å¾ä¿¡æ¯
                features = dataset_to_policy_features(
                    temp_dataset_metadata.features)
                input_features = {k: ft for k, ft in features.items(
                ) if ft.type is not FeatureType.ACTION}
                output_features = {
                    k: ft for k, ft in features.items() if ft.type is FeatureType.ACTION}
                first_task_loaded = True

            task_manager.register_available_task(
                task_id, temp_dataset_metadata.info["total_episodes"], task_data_path)
            print(
                f"âœ… æ£€æµ‹åˆ°ä»»åŠ¡{task_id}æ•°æ®: {temp_dataset_metadata.info['total_episodes']}ä¸ªepisodes")
        else:
            print(f"âš ï¸  rootæ•°æ®è·¯å¾„ä¸å­˜åœ¨æˆ–æœªé…ç½®: {task_data_path}")

    except Exception as e:
        print(f"âš ï¸  ä»»åŠ¡{task_id}æ•°æ®æ£€æµ‹å¤±è´¥: {e}")

    if not task_manager.available_tasks:
        logger.error("âŒ æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨çš„ä»»åŠ¡æ•°æ®")
        return

    # éªŒè¯è®­ç»ƒå‡†å¤‡çŠ¶æ€
    ready, issues = task_manager.validate_training_readiness()
    if not ready:
        logger.error("âŒ è®­ç»ƒå‡†å¤‡æœªå®Œæˆ:")
        for issue in issues:
            logger.error(f"   - {issue}")
        return

    # æ‰“å°è®­ç»ƒè®¡åˆ’
    task_manager.print_training_plan()

    # æ„å»ºpolicyé…ç½®ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„ç‰¹å¾ï¼‰
    policy_cfg = build_policy_config(cfg, input_features, output_features)
    print(f"Policyé…ç½®: {policy_cfg}")

    # æ„å»ºåˆ†å±‚æ¶æ„çš„policyï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ä»»åŠ¡çš„ç»Ÿè®¡ä¿¡æ¯ï¼‰
    first_task_id = task_manager.available_tasks[0]
    first_task_dataset, first_task_metadata = load_task_dataset(
        first_task_id, cfg, policy_cfg, image_transforms)

    if first_task_dataset is None:
        logger.error(f"âŒ æ— æ³•åŠ è½½ä»»åŠ¡{first_task_id}æ•°æ®")
        return

    datasets[first_task_id] = first_task_dataset
    dataset_metadatas[first_task_id] = first_task_metadata

    policy = build_hierarchical_policy(policy_cfg, first_task_metadata.stats)

    # è®¡ç®—æ€»å¸§æ•°ï¼ˆç”¨äºä¼˜åŒ–å™¨é…ç½®ï¼‰
    total_frames = sum(metadata.info["total_frames"]
                       for metadata in dataset_metadatas.values())
    optimizer, lr_scheduler = build_optimizer_and_scheduler(
        policy, cfg, total_frames)

    # AMPæ”¯æŒ
    amp_requested = bool(getattr(cfg.policy, "use_amp", False))
    amp_enabled = amp_requested and device.type == "cuda"

    has_torch_autocast = hasattr(torch, "autocast")

    def make_autocast(enabled: bool):
        if not enabled:
            return nullcontext()
        if device.type == "cuda":
            if has_torch_autocast:
                return torch.autocast(device_type="cuda")
            else:
                from torch.cuda.amp import autocast as cuda_autocast
                return cuda_autocast()
        return nullcontext()

    scaler = torch.amp.GradScaler(device=device.type, enabled=amp_enabled) if hasattr(
        torch, "amp") else torch.cuda.amp.GradScaler(enabled=amp_enabled)

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    steps = 0
    best_loss = float('inf')

    # æ¢å¤è®­ç»ƒé€»è¾‘
    if cfg.training.resume and cfg.training.resume_timestamp:
        resume_path = Path(cfg.training.output_directory) / \
            cfg.training.resume_timestamp
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡ç‰¹å®šè®­ç»ƒ: {resume_path}")
        try:
            load_rng_state(resume_path / "rng_state.pth")
            policy = policy.from_pretrained(resume_path, strict=True)

            optimizer, lr_scheduler = build_optimizer_and_scheduler(
                policy, cfg, total_frames)

            checkpoint_file = resume_path / "task_specific_learning_state.pth"
            if not checkpoint_file.exists():
                checkpoint_file = resume_path / "hierarchical_learning_state.pth"
            if not checkpoint_file.exists():
                checkpoint_file = resume_path / "learning_state.pth"

            checkpoint = torch.load(checkpoint_file, map_location=device)
            optimizer.load_state_dict(checkpoint["optimizer"])

            if "lr_scheduler" in checkpoint:
                lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
            if "scaler" in checkpoint and amp_enabled:
                scaler.load_state_dict(checkpoint["scaler"])
            if "steps" in checkpoint:
                steps = checkpoint["steps"]
            if "epoch" in checkpoint:
                start_epoch = checkpoint["epoch"]
            if "best_loss" in checkpoint:
                best_loss = checkpoint["best_loss"]

            # æ¢å¤ä»»åŠ¡ç‰¹å®šçŠ¶æ€
            if "available_tasks" in checkpoint:
                for task_id in checkpoint["available_tasks"]:
                    if task_id not in task_manager.available_tasks:
                        task_manager.available_tasks.append(task_id)

            if "current_phase" in checkpoint:
                task_manager.current_training_phase = checkpoint["current_phase"]

            # æ¢å¤ä»»åŠ¡ç®¡ç†å™¨çŠ¶æ€
            state_file = resume_path / "task_training_state.json"
            if state_file.exists():
                task_manager.load_training_state(state_file)

            for file in resume_path.glob("events.*"):
                shutil.copy(file, output_directory)

            print(f"å·²æ¢å¤ä»»åŠ¡ç‰¹å®šè®­ç»ƒä»epoch {start_epoch}, step {steps}")
        except Exception as e:
            print(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return
    else:
        print("ä»å¤´å¼€å§‹ä»»åŠ¡ç‰¹å®šè®­ç»ƒ!")

    policy.train().to(device)
    print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in policy.parameters()):,}")
    print(f"ä½¿ç”¨AMP: {amp_enabled}")

    # æ‰“å°åˆ†å±‚æ¶æ„ä¿¡æ¯
    if hasattr(policy, 'print_architecture_summary'):
        policy.print_architecture_summary()

    # è·å–å½“å‰çš„è¯¾ç¨‹å­¦ä¹ é…ç½®
    curriculum_stages = task_manager.get_current_curriculum_stages()
    use_curriculum = bool(curriculum_stages)

    if use_curriculum:
        print(f"ğŸ“ å¯åŠ¨ä»»åŠ¡ç‰¹å®šè¯¾ç¨‹å­¦ä¹ ï¼Œå…±{len(curriculum_stages)}ä¸ªé˜¶æ®µ")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = create_task_specific_dataloader(
            datasets, task_manager, cfg, device)

        # è¿è¡Œè¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
        current_step = steps
        for stage_name, stage_config in curriculum_stages.items():
            current_step = run_task_specific_curriculum_stage(
                policy, stage_config, dataloader, task_manager, cfg, device, writer,
                optimizer, lr_scheduler, scaler, output_directory, amp_enabled, current_step, start_epoch
            )

        print("âœ… ä»»åŠ¡ç‰¹å®šè¯¾ç¨‹å­¦ä¹ å®Œæˆã€‚å¼€å§‹å®Œæ•´è®­ç»ƒ...")
        steps = current_step

    # ä¸»è¦è®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹ä¸»è¦è®­ç»ƒå¾ªç¯...")
    for epoch in range(start_epoch, cfg.training.max_epoch):
        dataloader = create_task_specific_dataloader(
            datasets, task_manager, cfg, device)

        epoch_bar = tqdm(
            dataloader, desc=f"Epoch {epoch+1}/{cfg.training.max_epoch}")

        total_loss = 0.0
        for batch in epoch_bar:
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            # è·å–ä»»åŠ¡ç‰¹å®šçš„æŸå¤±æƒé‡
            task_loss_weights = task_manager.get_task_specific_loss_weights(
                batch)

            with make_autocast(amp_enabled):
                loss, hierarchical_info = policy.forward(
                    batch, task_weights=task_loss_weights)

            scaled_loss = loss / cfg.training.accumulation_steps

            if amp_enabled:
                scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            if steps % cfg.training.accumulation_steps == 0:
                if amp_enabled:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                lr_scheduler.step()

            # è®°å½•è®­ç»ƒæ—¥å¿—
            if steps % cfg.training.log_freq == 0:
                writer.add_scalar("train/loss", scaled_loss.item(), steps)
                writer.add_scalar(
                    "train/lr", lr_scheduler.get_last_lr()[0], steps)

                # è®°å½•ä»»åŠ¡ç‰¹å®šç»Ÿè®¡ä¿¡æ¯
                if isinstance(hierarchical_info, dict):
                    for key, value in hierarchical_info.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(
                                f"task_specific/{key}", value, steps)

                # è®°å½•å±‚æ€§èƒ½ç»Ÿè®¡
                if hasattr(policy, 'get_performance_stats'):
                    perf_stats = policy.get_performance_stats()
                    for layer_name, stats in perf_stats.items():
                        if isinstance(stats, dict):
                            for stat_name, stat_value in stats.items():
                                if isinstance(stat_value, (int, float)):
                                    writer.add_scalar(
                                        f"performance/{layer_name}/{stat_name}", stat_value, steps)

                epoch_bar.set_postfix(
                    loss=f"{scaled_loss.item():.3f}",
                    step=steps,
                    phase=task_manager.current_training_phase,
                    lr=lr_scheduler.get_last_lr()[0]
                )

            steps += 1
            total_loss += scaled_loss.item()

        # æ›´æ–°æœ€ä½³æŸå¤±
        if total_loss < best_loss:
            best_loss = total_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            save_rng_state(best_path / "rng_state.pth")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            save_rng_state(epoch_path / "rng_state.pth")

        # ä¿å­˜æœ€æ–°çš„ä»»åŠ¡ç‰¹å®šæ£€æŸ¥ç‚¹
        save_task_specific_checkpoint(
            policy, optimizer, lr_scheduler, scaler, steps, epoch + 1, best_loss,
            output_directory, task_manager, amp_enabled
        )

    writer.close()

    # è®­ç»ƒå®Œæˆåæ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ‰ ä»»åŠ¡ç‰¹å®šè®­ç»ƒå®Œæˆ!")
    print("=" * 70)

    if hasattr(policy, 'get_performance_stats'):
        final_stats = policy.get_performance_stats()
        print("æœ€ç»ˆå±‚æ€§èƒ½ç»Ÿè®¡:")
        for layer_name, stats in final_stats.items():
            print(f"  {layer_name}: {stats}")

    # æ‰“å°ä»»åŠ¡è®­ç»ƒæ€»ç»“
    training_summary = task_manager.get_training_summary()
    print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   è®­ç»ƒé˜¶æ®µ: {training_summary['current_phase']}")
    print(f"   å¤„ç†ä»»åŠ¡: {len(training_summary['available_tasks'])}")
    print(f"   æ€»episodes: {training_summary['total_episodes']}")
    print(f"   æœ€ç»ˆæŸå¤±: {best_loss:.4f}")


if __name__ == "__main__":
    main()
