"""
ManipulationLayer: æ“ä½œæ§åˆ¶å±‚ - Transformerä¸»å¯¼
"""
import torch
import torch.nn as nn
from typing import Dict, Any, Optional, List

from .BaseLayer import BaseLayer


class ManipulationLayer(BaseLayer):
    """
    æ“ä½œæ§åˆ¶å±‚ - ä¼˜å…ˆçº§3

    ç‰¹ç‚¹ï¼š
    - Transformerä¸»å¯¼æ¶æ„
    - å¤„ç†æŠ“å–ã€æ‘†æ”¾ç­‰ç²¾ç»†æ“ä½œ
    - çº¦æŸæ»¡è¶³å’ŒåŒè‡‚åè°ƒ
    - çº¦100mså“åº”æ—¶é—´
    """

    def __init__(self, config: Dict[str, Any], base_config: Any):
        super().__init__(config, "manipulation", priority=3)

        self.base_config = base_config

        # é…ç½®å‚æ•°
        self.hidden_size = config.get('hidden_size', 512)
        self.num_layers = config.get('layers', 3)
        self.num_heads = config.get('heads', 8)
        self.dim_feedforward = config.get('dim_feedforward', 2048)

        # ç‰¹å¾ç»´åº¦è®¡ç®—ï¼ˆè§†è§‰+çŠ¶æ€ï¼‰- é€‚é…å®é™…æœºå™¨äººé…ç½®
        self.visual_dim = 1280  # EfficientNet-B0è¾“å‡º
        state_shape = getattr(base_config, 'robot_state_feature', None)
        if state_shape and hasattr(state_shape, 'shape'):
            self.state_dim = state_shape.shape[0]
        else:
            # é»˜è®¤é…ç½®ï¼šonly_arm=trueæ—¶çš„åŒè‡‚+æ‰‹çˆªé…ç½®
            self.state_dim = 16

        self.input_projection = nn.Linear(self.visual_dim + self.state_dim, self.hidden_size)

        # ä¸»è¦çš„Transformerç½‘ç»œ
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_size,
            nhead=self.num_heads,
            dim_feedforward=self.dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.manipulation_transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)

        # çº¦æŸæ»¡è¶³æ¨¡å—
        self.constraint_solver = ConstraintSatisfactionModule(self.hidden_size)

        # åŒè‡‚åè°ƒæ¨¡å—
        self.bimanual_coordinator = BimanualCoordinationModule(self.hidden_size)

        # è¾“å‡ºæŠ•å½±
        self.action_head = nn.Linear(self.hidden_size, 32)  # åŠ¨ä½œè¾“å‡º

    def should_activate(self, inputs: Dict[str, torch.Tensor], context: Optional[Dict[str, Any]] = None) -> bool:
        """å½“éœ€è¦ç²¾ç»†æ“ä½œæ—¶æ¿€æ´»"""
        if context is None:
            return True
        return context.get('requires_manipulation', True)

    def get_required_input_keys(self) -> List[str]:
        return ['observation.state']  # è‡³å°‘éœ€è¦çŠ¶æ€ï¼Œè§†è§‰ç‰¹å¾å¯é€‰

    def forward(self, inputs: Dict[str, torch.Tensor], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ“ä½œæ§åˆ¶å‰å‘ä¼ æ’­"""
        # æå–ç‰¹å¾
        features = self._extract_features(inputs)
        if features is None:
            return self._generate_default_output(inputs)

        batch_size, seq_len, _ = features.shape

        # Transformerå¤„ç†
        manipulation_features = self.manipulation_transformer(features)

        # çº¦æŸæ»¡è¶³
        constraint_solution = self.constraint_solver(manipulation_features, context)

        # åŒè‡‚åè°ƒ
        coordinated_actions = self.bimanual_coordinator(manipulation_features, context)

        # æœ€ç»ˆåŠ¨ä½œ
        final_action = self.action_head(manipulation_features[:, -1, :])

        return {
            'manipulation_features': manipulation_features,
            'constraint_solution': constraint_solution,
            'coordinated_actions': coordinated_actions,
            'action': final_action,
            'layer': 'manipulation'
        }

    def _extract_features(self, inputs: Dict[str, torch.Tensor]) -> Optional[torch.Tensor]:
        """æå–å¹¶èåˆå¤šæ¨¡æ€ç‰¹å¾"""
        features_list = []

        print(f"ğŸ” ManipulationLayer: Available input keys: {list(inputs.keys())}")

        # çŠ¶æ€ç‰¹å¾
        if 'observation.state' in inputs:
            state_features = inputs['observation.state']
            print(f"ğŸ” ManipulationLayer: state_features.shape = {state_features.shape}")
            # å¤„ç†ç»´åº¦ï¼šç¡®ä¿æ˜¯3D tensor [batch_size, seq_len, state_dim]
            if len(state_features.shape) == 1:
                state_features = state_features.unsqueeze(0).unsqueeze(0)  # [1, 1, state_dim]
            elif len(state_features.shape) == 2:
                state_features = state_features.unsqueeze(1)  # [batch_size, 1, state_dim]
            features_list.append(state_features)
            print(f"ğŸ” ManipulationLayer: Processed state_features.shape = {state_features.shape}")

        # è§†è§‰ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'observation.images' in inputs:
            # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å‡å€¼æ± åŒ–
            visual_features = inputs['observation.images']
            if len(visual_features.shape) > 3:
                visual_features = visual_features.mean(dim=(-2, -1))  # å…¨å±€å¹³å‡æ± åŒ–
            # ç¡®ä¿æ˜¯3D tensor
            if len(visual_features.shape) == 2:
                visual_features = visual_features.unsqueeze(1)
            features_list.append(visual_features)
        else:
            # å¦‚æœæ²¡æœ‰è§†è§‰ç‰¹å¾ï¼Œéœ€è¦ç”¨é›¶å¡«å……ä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹çš„è¾“å…¥ç»´åº¦
            if features_list:
                batch_size, seq_len = features_list[0].shape[:2]
                device = features_list[0].device
                # åˆ›å»º1280ç»´çš„é›¶è§†è§‰ç‰¹å¾
                zero_visual = torch.zeros(batch_size, seq_len, self.visual_dim, device=device)
                features_list.append(zero_visual)

        if not features_list:
            return None

        # ç‰¹å¾æ‹¼æ¥å’ŒæŠ•å½±
        combined_features = torch.cat(features_list, dim=-1)
        print(f"ğŸ” ManipulationLayer: combined_features.shape = {combined_features.shape}")
        print(f"ğŸ” ManipulationLayer: input_projection expects: {self.input_projection.in_features}")

        projected_features = self.input_projection(combined_features)
        print(f"ğŸ” ManipulationLayer: projected_features.shape = {projected_features.shape}")

        return projected_features

    def _generate_default_output(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """ç”Ÿæˆé»˜è®¤è¾“å‡º"""
        batch_size = list(inputs.values())[0].size(0)
        device = list(inputs.values())[0].device

        zero_features = torch.zeros(batch_size, 10, self.hidden_size, device=device)
        zero_action = torch.zeros(batch_size, 32, device=device)

        return {
            'manipulation_features': zero_features,
            'constraint_solution': {},
            'coordinated_actions': zero_action,
            'action': zero_action,
            'layer': 'manipulation'
        }


class ConstraintSatisfactionModule(nn.Module):
    """çº¦æŸæ»¡è¶³æ¨¡å—"""

    def __init__(self, feature_dim: int):
        super().__init__()
        self.feature_dim = feature_dim
        self.constraint_net = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(),
            nn.Linear(feature_dim // 2, feature_dim // 4),
            nn.ReLU(),
            nn.Linear(feature_dim // 4, 1),
            nn.Sigmoid()
        )

    def forward(self, features: torch.Tensor, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """çº¦æŸæ»¡è¶³å¤„ç†"""
        constraint_satisfaction = self.constraint_net(features)
        return {
            'constraint_satisfaction_score': constraint_satisfaction,
            'constraints_met': constraint_satisfaction > 0.5
        }


class BimanualCoordinationModule(nn.Module):
    """åŒè‡‚åè°ƒæ¨¡å—"""

    def __init__(self, feature_dim: int):
        super().__init__()
        self.feature_dim = feature_dim
        self.coordination_net = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, 32)  # è¾“å‡ºåè°ƒåŠ¨ä½œ
        )

    def forward(self, features: torch.Tensor, context: Optional[Dict[str, Any]] = None) -> torch.Tensor:
        """åŒè‡‚åè°ƒå¤„ç†"""
        coordinated_action = self.coordination_net(features[:, -1, :])
        return coordinated_action