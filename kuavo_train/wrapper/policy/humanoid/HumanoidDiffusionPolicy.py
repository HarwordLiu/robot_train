"""
HumanoidDiffusionPolicy: åˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyä¸»å…¥å£
"""
import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple, List
from pathlib import Path

# å¯¼å…¥åŸæœ‰çš„ç»„ä»¶
from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy
from kuavo_train.wrapper.policy.diffusion.DiffusionPolicyWrapper import CustomDiffusionPolicyWrapper
from kuavo_train.wrapper.policy.diffusion.DiffusionConfigWrapper import CustomDiffusionConfigWrapper

# å¯¼å…¥åˆ†å±‚æ¶æ„ç»„ä»¶
from .HierarchicalScheduler import HierarchicalScheduler
from .HierarchicalDiffusionModel import HierarchicalDiffusionModel


class HumanoidDiffusionPolicyWrapper(CustomDiffusionPolicyWrapper):
    """
    åˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policy

    ç»§æ‰¿è‡ªCustomDiffusionPolicyWrapperä»¥ä¿æŒå‘åå…¼å®¹æ€§
    æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ä½¿ç”¨åˆ†å±‚æ¶æ„æˆ–ä¼ ç»Ÿæ¶æ„
    """

    def __init__(self,
                 config: CustomDiffusionConfigWrapper,
                 dataset_stats: Optional[Dict[str,
                                              Dict[str, torch.Tensor]]] = None,
                 use_hierarchical: Optional[bool] = None,
                 hierarchical: Optional[Dict[str, Any]] = None,
                 **kwargs):
        """
        åˆå§‹åŒ–åˆ†å±‚Diffusion Policy

        Args:
            config: é…ç½®å¯¹è±¡
            dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            use_hierarchical: æ˜¯å¦å¯ç”¨åˆ†å±‚æ¶æ„ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨configä¸­çš„è®¾ç½®ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆç”¨äºHydraå…¼å®¹æ€§ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†å±‚æ¶æ„ï¼ˆä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨configï¼‰
        if use_hierarchical is not None:
            self.use_hierarchical = use_hierarchical
        else:
            self.use_hierarchical = getattr(config, 'use_hierarchical', False)

        if self.use_hierarchical:
            # ä½¿ç”¨åˆ†å±‚æ¶æ„
            super().__init__(config, dataset_stats)

            # å¦‚æœæä¾›äº†å¤–éƒ¨hierarchicalé…ç½®ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä»configä¸­è·å–
            hierarchical_config = hierarchical if hierarchical is not None else getattr(
                config, 'hierarchical', {})
            self._init_hierarchical_components(config, hierarchical_config)

            # åˆå§‹åŒ–ä»»åŠ¡æ¡ä»¶æƒé‡ç³»ç»Ÿ
            self._init_task_conditional_weights(config, hierarchical_config)
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰æ¶æ„
            super().__init__(config, dataset_stats)
            self.scheduler = None
            self.task_layer_weights = None
            self.current_curriculum_stage = None

        # ç”¨äºå­˜å‚¨æœ€åä¸€æ¬¡æ¨ç†çš„å±‚è¾“å‡ºï¼ˆä¾›æ—¥å¿—è®°å½•ä½¿ç”¨ï¼‰
        self._last_layer_outputs = None

    def _init_hierarchical_components(self, config, hierarchical_config):
        """åˆå§‹åŒ–åˆ†å±‚æ¶æ„ç»„ä»¶"""
        try:
            # æ›¿æ¢åŸæœ‰çš„diffusionæ¨¡å‹ä¸ºåˆ†å±‚ç‰ˆæœ¬
            self.diffusion = HierarchicalDiffusionModel(config)

            # åˆ›å»ºåˆ†å±‚è°ƒåº¦å™¨
            self.scheduler = HierarchicalScheduler(hierarchical_config, config)

            print(
                f"âœ… Hierarchical architecture initialized with {len(self.scheduler.layers)} layers")

        except Exception as e:
            print(f"âŒ Failed to initialize hierarchical components: {e}")
            print("ğŸ”„ Falling back to traditional architecture")
            self.use_hierarchical = False
            self.scheduler = None

    def _init_task_conditional_weights(self, config, hierarchical_config):
        """åˆå§‹åŒ–ä»»åŠ¡æ¡ä»¶æƒé‡ç³»ç»Ÿ"""
        try:
            # é»˜è®¤å±‚æƒé‡
            self.default_layer_weights = hierarchical_config.get('layer_weights', {
                'safety': 2.0,
                'gait': 1.5,
                'manipulation': 1.0,
                'planning': 0.8
            })

            # å½“å‰æ¿€æ´»çš„ä»»åŠ¡ç‰¹å®šæƒé‡
            self.task_layer_weights = self.default_layer_weights.copy()

            # è¯¾ç¨‹å­¦ä¹ çŠ¶æ€
            self.current_curriculum_stage = None
            self.enabled_layers = list(self.default_layer_weights.keys())

            print("âœ… ä»»åŠ¡æ¡ä»¶æƒé‡ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            print(f"âš ï¸  ä»»åŠ¡æ¡ä»¶æƒé‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.task_layer_weights = self.default_layer_weights
            self.current_curriculum_stage = None

    def forward(self, batch: Dict[str, torch.Tensor],
                curriculum_info: Optional[Dict[str, Any]] = None,
                task_weights: Optional[Dict[str, float]] = None) -> Tuple[torch.Tensor, Optional[Dict[str, Any]]]:
        """
        å‰å‘ä¼ æ’­ï¼Œæ ¹æ®æ¶æ„ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡æ•°æ®
            curriculum_info: è¯¾ç¨‹å­¦ä¹ ä¿¡æ¯
            task_weights: ä»»åŠ¡ç‰¹å®šæƒé‡

        Returns:
            Tuple[loss, outputs]: æŸå¤±å’Œè¾“å‡ºç»“æœ
        """
        if self.use_hierarchical and self.scheduler is not None:
            return self._hierarchical_forward(batch, curriculum_info, task_weights)
        else:
            return super().forward(batch)

    def _hierarchical_forward(self, batch: Dict[str, torch.Tensor],
                              curriculum_info: Optional[Dict[str, Any]] = None,
                              task_weights: Optional[Dict[str, float]] = None) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """åˆ†å±‚æ¶æ„çš„å‰å‘ä¼ æ’­"""
        # æ›´æ–°ä»»åŠ¡æ¡ä»¶æƒé‡
        if task_weights is not None:
            self._update_task_weights(task_weights)

        # æ›´æ–°è¯¾ç¨‹å­¦ä¹ çŠ¶æ€
        if curriculum_info is not None:
            self._update_curriculum_state(curriculum_info)

        # å›¾åƒé¢„å¤„ç†ï¼ˆä¿æŒä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰
        batch = self._preprocess_batch(batch)

        # å½’ä¸€åŒ–è¾“å…¥å’Œç›®æ ‡
        batch = self.normalize_inputs(batch)
        batch = self.normalize_targets(batch)

        # ä»»åŠ¡è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼Œè€ƒè™‘è¯¾ç¨‹å­¦ä¹ ä¿¡æ¯ï¼‰
        task_info = self._identify_task(batch, curriculum_info)

        # åˆ†å±‚å¤„ç†
        layer_outputs = self.scheduler(batch, task_info)

        # DiffusionæŸå¤±è®¡ç®—
        diffusion_loss = self.diffusion.compute_loss(batch, layer_outputs)

        # åˆ†å±‚æŸå¤±èšåˆï¼ˆä½¿ç”¨ä»»åŠ¡ç‰¹å®šæƒé‡ï¼‰
        total_loss = self._aggregate_hierarchical_loss(
            diffusion_loss, layer_outputs, use_task_weights=True)

        # æ·»åŠ è¯¾ç¨‹å­¦ä¹ å’Œä»»åŠ¡ç‰¹å®šä¿¡æ¯åˆ°è¾“å‡º
        hierarchical_info = {
            'curriculum_stage': self.current_curriculum_stage,
            'enabled_layers': self.enabled_layers.copy(),
            'task_weights': self.task_layer_weights.copy(),
            'layer_performance': self._get_layer_performance_metrics(layer_outputs)
        }

        return total_loss, hierarchical_info

    def _preprocess_batch(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """é¢„å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼ˆå›¾åƒè£å‰ªã€ç¼©æ”¾ç­‰ï¼‰"""
        # å¤ç”¨åŸæœ‰çš„å›¾åƒé¢„å¤„ç†é€»è¾‘
        random_crop = self.config.crop_is_random and self.training
        crop_position = None

        # RGBå›¾åƒé¢„å¤„ç†
        if self.config.image_features:
            batch = dict(batch)  # shallow copy
            for key in self.config.image_features:
                from kuavo_train.utils.augmenter import crop_image, resize_image
                batch[key], crop_position = crop_image(
                    batch[key],
                    target_range=self.config.crop_shape,
                    random_crop=random_crop
                )
                batch[key] = resize_image(
                    batch[key],
                    target_size=self.config.resize_shape,
                    image_type="rgb"
                )

            # å †å RGBç‰¹å¾
            from lerobot.constants import OBS_IMAGES
            batch[OBS_IMAGES] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4)

        # æ·±åº¦å›¾åƒé¢„å¤„ç†
        if self.config.use_depth and self.config.depth_features:
            batch = dict(batch)
            for key in self.config.depth_features:
                import torchvision.transforms.functional as TF
                if len(crop_position) == 4:
                    batch[key] = TF.crop(batch[key], *crop_position)
                else:
                    batch[key] = TF.center_crop(batch[key], crop_position)

                from kuavo_train.utils.augmenter import resize_image
                batch[key] = resize_image(
                    batch[key],
                    target_size=self.config.resize_shape,
                    image_type="depth"
                )

            # å †å æ·±åº¦ç‰¹å¾
            OBS_DEPTH = "observation.depth"
            batch[OBS_DEPTH] = torch.stack(
                [batch[key] for key in self.config.depth_features], dim=-4)

        return batch

    def _update_task_weights(self, task_weights: Dict[str, float]):
        """æ›´æ–°ä»»åŠ¡ç‰¹å®šçš„å±‚æƒé‡"""
        if task_weights:
            self.task_layer_weights.update(task_weights)

    def _update_curriculum_state(self, curriculum_info: Dict[str, Any]):
        """æ›´æ–°è¯¾ç¨‹å­¦ä¹ çŠ¶æ€"""
        stage_changed = False
        layers_changed = False

        if 'stage' in curriculum_info:
            new_stage = curriculum_info['stage']
            if new_stage != self.current_curriculum_stage:
                self.current_curriculum_stage = new_stage
                stage_changed = True

        if 'enabled_layers' in curriculum_info:
            new_layers = curriculum_info['enabled_layers'].copy()
            if new_layers != self.enabled_layers:
                self.enabled_layers = new_layers
                layers_changed = True

        # åªåœ¨çŠ¶æ€çœŸæ­£æ”¹å˜æ—¶è¾“å‡ºæ—¥å¿—ï¼Œé¿å…è¿›åº¦æ¡é‡å¤æ¸²æŸ“
        if stage_changed or layers_changed:
            print(
                f"ğŸ“ è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: {self.current_curriculum_stage}, æ¿€æ´»å±‚: {self.enabled_layers}")

    def _identify_task(self, batch: Dict[str, torch.Tensor],
                       curriculum_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ä»»åŠ¡è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼Œè€ƒè™‘è¯¾ç¨‹å­¦ä¹ ä¿¡æ¯ï¼‰"""
        # åŸºç¡€ä»»åŠ¡ä¿¡æ¯
        task_info = {
            'task_type': 'general',
            'task_complexity': 'medium',
            'requires_locomotion': True,
            'requires_manipulation': True,
            'requires_planning': False
        }

        # æ ¹æ®è¯¾ç¨‹å­¦ä¹ é˜¶æ®µè°ƒæ•´ä»»åŠ¡ä¿¡æ¯
        if curriculum_info:
            target_task = curriculum_info.get('target_task')
            if target_task == 1:  # åŠ¨æ€æŠ“å–
                task_info.update({
                    'task_type': 'dynamic_grasping',
                    'task_complexity': 'medium',
                    'requires_locomotion': False,  # ä¸»è¦æ˜¯æ“ä½œä»»åŠ¡
                    'requires_manipulation': True,
                    'requires_planning': False
                })
            elif target_task == 2:  # ç§°é‡
                task_info.update({
                    'task_type': 'package_weighing',
                    'task_complexity': 'high',
                    'requires_locomotion': True,  # éœ€è¦å¹³è¡¡æ§åˆ¶
                    'requires_manipulation': True,
                    'requires_planning': True
                })
            elif target_task == 3:  # æ‘†æ”¾
                task_info.update({
                    'task_type': 'precise_placement',
                    'task_complexity': 'high',
                    'requires_locomotion': False,
                    'requires_manipulation': True,
                    'requires_planning': True  # éœ€è¦ç©ºé—´è§„åˆ’
                })
            elif target_task == 4:  # åˆ†æ‹£
                task_info.update({
                    'task_type': 'full_process_sorting',
                    'task_complexity': 'very_high',
                    'requires_locomotion': True,
                    'requires_manipulation': True,
                    'requires_planning': True
                })

        return task_info

    def _get_layer_performance_metrics(self, layer_outputs: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–å±‚æ€§èƒ½æŒ‡æ ‡"""
        performance = {}

        for layer_name, layer_output in layer_outputs.items():
            if isinstance(layer_output, dict):
                metrics = {}

                # æ‰§è¡Œæ—¶é—´
                if 'execution_time' in layer_output:
                    metrics['execution_time'] = layer_output['execution_time']

                # æŸå¤±å€¼
                if 'loss' in layer_output:
                    metrics['loss'] = layer_output['loss'].item() if torch.is_tensor(
                        layer_output['loss']) else layer_output['loss']

                # æ¿€æ´»çŠ¶æ€
                metrics['active'] = layer_name in self.enabled_layers

                # æƒé‡
                metrics['weight'] = self.task_layer_weights.get(
                    layer_name, 1.0)

                performance[layer_name] = metrics

        return performance

    def _aggregate_hierarchical_loss(self,
                                     diffusion_loss: torch.Tensor,
                                     layer_outputs: Dict[str, Any],
                                     use_task_weights: bool = False) -> torch.Tensor:
        """èšåˆåˆ†å±‚æŸå¤±"""
        total_loss = diffusion_loss

        # é€‰æ‹©æƒé‡æ¥æº
        if use_task_weights and hasattr(self, 'task_layer_weights'):
            layer_weights = self.task_layer_weights
        else:
            layer_weights = getattr(
                self.scheduler.config, 'layer_weights', {}) if self.scheduler else {}

        # èšåˆå„å±‚çš„æŸå¤±ï¼ˆåªè®¡ç®—æ¿€æ´»å±‚çš„æŸå¤±ï¼‰
        for layer_name, layer_output in layer_outputs.items():
            if isinstance(layer_output, dict) and 'loss' in layer_output:
                # æ£€æŸ¥å±‚æ˜¯å¦åœ¨å½“å‰æ¿€æ´»åˆ—è¡¨ä¸­
                if layer_name in self.enabled_layers:
                    layer_weight = layer_weights.get(layer_name, 1.0)
                    layer_loss = layer_output['loss']
                    total_loss = total_loss + layer_weight * layer_loss

        return total_loss

    def select_action(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰

        Args:
            batch: è§‚æµ‹æ•°æ®

        Returns:
            torch.Tensor: é€‰æ‹©çš„åŠ¨ä½œ
        """
        if self.use_hierarchical and self.scheduler is not None:
            return self._hierarchical_select_action(batch)
        else:
            return super().select_action(batch)

    def _hierarchical_select_action(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """åˆ†å±‚æ¶æ„çš„åŠ¨ä½œé€‰æ‹©"""
        # é¢„å¤„ç†
        batch = self._preprocess_batch(batch)
        batch = self.normalize_inputs(batch)

        # ä»»åŠ¡è¯†åˆ«
        task_info = self._identify_task(batch)

        # åˆ†å±‚æ¨ç†ï¼ˆç¦»çº¿è¯„ä¼°æ—¶ä½¿ç”¨æ ‡å‡†æ¨¡å¼ï¼‰
        with torch.no_grad():
            layer_outputs = self.scheduler(batch, task_info)

        # ä¿å­˜layer_outputsä¾›æ—¥å¿—è®°å½•ä½¿ç”¨
        self._last_layer_outputs = layer_outputs

        # ä»åˆ†å±‚è¾“å‡ºä¸­æå–æœ€ç»ˆåŠ¨ä½œ
        return self._extract_action_from_layers(layer_outputs, batch)

    def _extract_action_from_layers(self,
                                    layer_outputs: Dict[str, Any],
                                    batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ä»åˆ†å±‚è¾“å‡ºä¸­æå–æœ€ç»ˆåŠ¨ä½œ"""
        # ä¼˜å…ˆçº§å¤„ç†ï¼šå®‰å…¨å±‚å¯ä»¥è¦†ç›–å…¶ä»–å±‚çš„è¾“å‡º
        if 'safety' in layer_outputs and layer_outputs['safety'].get('emergency', False):
            return layer_outputs['safety'].get('emergency_action', torch.zeros_like(batch.get('action', torch.zeros(1, 32))))

        # æ­£å¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨æœ€é«˜çº§åˆ«å¯ç”¨å±‚çš„è¾“å‡º
        for layer_name in ['planning', 'manipulation', 'gait', 'safety']:
            if layer_name in layer_outputs and 'action' in layer_outputs[layer_name]:
                return layer_outputs[layer_name]['action']

        # å›é€€ï¼šä½¿ç”¨ä¼ ç»Ÿdiffusionè¾“å‡º
        return super().select_action(batch)

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if self.use_hierarchical and self.scheduler is not None:
            return self.scheduler.get_performance_stats()
        else:
            return {'architecture': 'traditional', 'hierarchical_enabled': False}

    def set_layer_enabled(self, layer_name: str, enabled: bool):
        """åŠ¨æ€å¯ç”¨/ç¦ç”¨ç‰¹å®šå±‚"""
        if self.use_hierarchical and self.scheduler is not None:
            self.scheduler.set_layer_enabled(layer_name, enabled)

    def get_active_layers(self) -> List[str]:
        """è·å–å½“å‰æ¿€æ´»çš„å±‚åˆ—è¡¨"""
        if self.use_hierarchical and self.scheduler is not None:
            return self.scheduler.get_active_layers()
        else:
            return ['traditional']

    def set_task_layer_weights(self, task_weights: Dict[str, float]):
        """è®¾ç½®ä»»åŠ¡ç‰¹å®šçš„å±‚æƒé‡"""
        if self.use_hierarchical:
            self._update_task_weights(task_weights)

    def set_curriculum_stage(self, enabled_layers: List[str]):
        """è®¾ç½®è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ"""
        if self.use_hierarchical:
            self.enabled_layers = enabled_layers.copy()
            self.current_curriculum_stage = f"layers_{'_'.join(enabled_layers)}"
            print(f"ğŸ“ è®¾ç½®è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: æ¿€æ´»å±‚ {enabled_layers}")

    def get_layer_states(self) -> Dict[str, Any]:
        """è·å–å±‚çŠ¶æ€ä¿¡æ¯"""
        if not self.use_hierarchical:
            return {}

        return {
            'current_curriculum_stage': self.current_curriculum_stage,
            'enabled_layers': self.enabled_layers.copy(),
            'task_layer_weights': self.task_layer_weights.copy(),
            'default_layer_weights': self.default_layer_weights.copy()
        }

    def load_layer_states(self, layer_states: Dict[str, Any]):
        """åŠ è½½å±‚çŠ¶æ€ä¿¡æ¯"""
        if not self.use_hierarchical:
            return

        if 'current_curriculum_stage' in layer_states:
            self.current_curriculum_stage = layer_states['current_curriculum_stage']

        if 'enabled_layers' in layer_states:
            self.enabled_layers = layer_states['enabled_layers'].copy()

        if 'task_layer_weights' in layer_states:
            self.task_layer_weights = layer_states['task_layer_weights'].copy()

        print(
            f"âœ… å·²æ¢å¤å±‚çŠ¶æ€: é˜¶æ®µ={self.current_curriculum_stage}, æ¿€æ´»å±‚={self.enabled_layers}")

    def get_last_layer_outputs(self) -> Optional[Dict[str, Any]]:
        """
        è·å–æœ€åä¸€æ¬¡æ¨ç†çš„å±‚è¾“å‡ºä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰

        Returns:
            Dict: å±‚è¾“å‡ºä¿¡æ¯ï¼Œå¦‚æœä¸æ˜¯åˆ†å±‚æ¶æ„æˆ–æœªæ‰§è¡Œæ¨ç†åˆ™è¿”å›None
        """
        if not self.use_hierarchical:
            return None
        return self._last_layer_outputs

    def print_architecture_summary(self):
        """æ‰“å°æ¶æ„æ‘˜è¦"""
        if not self.use_hierarchical:
            print("ğŸ“Š ä½¿ç”¨ä¼ ç»ŸDiffusion Policyæ¶æ„")
            return

        print("ğŸ—ï¸  åˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyæ¶æ„æ‘˜è¦")
        print("=" * 50)
        print(f"å½“å‰è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: {self.current_curriculum_stage}")
        print(f"æ¿€æ´»å±‚: {self.enabled_layers}")
        print(f"å±‚æƒé‡é…ç½®:")
        for layer_name, weight in self.task_layer_weights.items():
            status = "âœ…" if layer_name in self.enabled_layers else "â¸ï¸ "
            print(f"  {status} {layer_name}: {weight}")

        if self.scheduler:
            print(f"åˆ†å±‚è°ƒåº¦å™¨: {type(self.scheduler).__name__}")
            print(
                f"æ€»å±‚æ•°: {len(self.scheduler.layers) if hasattr(self.scheduler, 'layers') else 'unknown'}")

    def _save_pretrained(self, save_directory: Path) -> None:
        """ä¿å­˜åˆ†å±‚æ¶æ„æ¨¡å‹ï¼Œå¤„ç†å…±äº«å¼ é‡é—®é¢˜"""
        print(f"ğŸ”§ å¼€å§‹ä¿å­˜åˆ†å±‚æ¶æ„æ¨¡å‹åˆ°: {save_directory}")

        # åˆ›å»ºä¿å­˜ç›®å½•
        save_directory.mkdir(parents=True, exist_ok=True)

        try:
            # ä¿å­˜é…ç½®
            self.config._save_pretrained(save_directory)
            print(f"âœ… é…ç½®ä¿å­˜æˆåŠŸ")

            # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
            state_dict = self.state_dict()
            print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€å­—å…¸åŒ…å« {len(state_dict)} ä¸ªå‚æ•°")

            # å¤„ç†å…±äº«å¼ é‡é—®é¢˜
            # åˆ›å»ºæ–°çš„çŠ¶æ€å­—å…¸ï¼Œé¿å…å…±äº«å¼ é‡
            clean_state_dict = {}
            for name, param in state_dict.items():
                # å…‹éš†å‚æ•°ä»¥é¿å…å…±äº«å¼ é‡
                clean_state_dict[name] = param.clone()

            # ä¿å­˜ä¸º safetensors æ ¼å¼
            from safetensors.torch import save_file
            model_file = save_directory / "model.safetensors"
            save_file(clean_state_dict, model_file)
            print(f"âœ… æ¨¡å‹æƒé‡ä¿å­˜æˆåŠŸ: {model_file}")

        except Exception as e:
            print(f"âŒ åˆ†å±‚æ¶æ„æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            # å›é€€åˆ°çˆ¶ç±»æ–¹æ³•
            try:
                print(f"ğŸ”„ å°è¯•ä½¿ç”¨çˆ¶ç±»ä¿å­˜æ–¹æ³•...")
                super()._save_pretrained(save_directory)
                print(f"âœ… çˆ¶ç±»ä¿å­˜æ–¹æ³•æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ çˆ¶ç±»ä¿å­˜æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                raise e2

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼ˆåˆ†å±‚æ¶æ„ä¸“ç”¨ï¼‰"""

        # æå–åˆ†å±‚æ¶æ„ç›¸å…³å‚æ•°
        use_hierarchical = kwargs.pop('use_hierarchical', None)
        hierarchical_config = kwargs.pop('hierarchical', None)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œå°è¯•ä»é…ç½®ä¸­æ¨æ–­
        if use_hierarchical is None:
            if len(args) > 0:
                pretrained_path = args[0]
            else:
                pretrained_path = kwargs.get('pretrained_name_or_path')

            if pretrained_path and 'hierarchical' in str(pretrained_path):
                use_hierarchical = True
                print(f"ğŸ” æ£€æµ‹åˆ°åˆ†å±‚æ¶æ„æ¨¡å‹è·¯å¾„ï¼Œå¯ç”¨åˆ†å±‚æ¶æ„")

        # å¦‚æœéœ€è¦å¯ç”¨åˆ†å±‚æ¶æ„ï¼Œå°†å‚æ•°ä¼ é€’ç»™æ„é€ å‡½æ•°
        if use_hierarchical:
            kwargs['use_hierarchical'] = True
            if hierarchical_config:
                kwargs['hierarchical'] = hierarchical_config

        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è¿›è¡ŒåŸºç¡€åŠ è½½
        instance = super().from_pretrained(*args, **kwargs)

        # éªŒè¯åˆ†å±‚æ¶æ„æ˜¯å¦æ­£ç¡®åŠ è½½
        if use_hierarchical:
            if not hasattr(instance, 'scheduler') or instance.scheduler is None:
                print(f"âš ï¸  åˆ†å±‚æ¶æ„ç»„ä»¶æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œå¯èƒ½ç¼ºå°‘hierarchicalé…ç½®")
                print(f"ğŸ’¡ è¯·ç¡®ä¿è¯„ä¼°é…ç½®åŒ…å«å®Œæ•´çš„hierarchicalé…ç½®æ®µ")
                instance.use_hierarchical = False  # å›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼
            else:
                print(f"âœ… åˆ†å±‚æ¶æ„æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(instance.scheduler.layers)} ä¸ªå±‚")

        return instance


# ä¸ºäº†å‘åå…¼å®¹æ€§ï¼Œåˆ›å»ºåˆ«å
HumanoidDiffusionPolicy = HumanoidDiffusionPolicyWrapper
