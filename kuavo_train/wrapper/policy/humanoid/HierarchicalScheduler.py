"""
HierarchicalScheduler: åˆ†å±‚æ¶æ„çš„æ ¸å¿ƒè°ƒåº¦å™¨
"""
import torch
import torch.nn as nn
import time
from typing import Dict, Any, Optional, List
from collections import OrderedDict

from .layers import BaseLayer, SafetyReflexLayer, GaitControlLayer, ManipulationLayer, GlobalPlanningLayer


class HierarchicalScheduler(nn.Module):
    """
    åˆ†å±‚æ¶æ„çš„æ ¸å¿ƒè°ƒåº¦å™¨

    è´Ÿè´£ç®¡ç†å››ä¸ªå±‚æ¬¡çš„æ¿€æ´»ã€è°ƒåº¦å’Œè¾“å‡ºèšåˆ
    """

    def __init__(self, hierarchical_config: Dict[str, Any], base_config: Any):
        super().__init__()
        self.config = hierarchical_config
        self.base_config = base_config

        # æ„å»ºå››ä¸ªå±‚æ¬¡
        self.layers = self._build_layers()

        # è°ƒåº¦é…ç½®
        self.layer_priorities = {name: layer.get_priority() for name, layer in self.layers.items()}
        self.layer_weights = hierarchical_config.get('layer_weights', {})

        # æ€§èƒ½ç›‘æ§
        self.total_forward_calls = 0
        self.layer_activation_stats = {name: 0 for name in self.layers.keys()}

        print(f"ğŸ—ï¸ HierarchicalScheduler initialized with layers: {list(self.layers.keys())}")

    def _build_layers(self) -> nn.ModuleDict:
        """æ„å»ºå››ä¸ªå±‚æ¬¡çš„ç½‘ç»œ"""
        layers = nn.ModuleDict()

        layer_configs = self.config.get('layers', {})
        print(f"ğŸ” Available layer configs: {list(layer_configs.keys())}")

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ„å»ºå±‚
        layer_builders = {
            'safety': SafetyReflexLayer,
            'gait': GaitControlLayer,
            'manipulation': ManipulationLayer,
            'planning': GlobalPlanningLayer
        }

        for layer_name, layer_class in layer_builders.items():
            if layer_name in layer_configs:
                layer_config = layer_configs[layer_name]
                try:
                    layer = layer_class(layer_config, self.base_config)
                    layers[layer_name] = layer
                    print(f"âœ… {layer_name} layer created successfully")
                except Exception as e:
                    print(f"âŒ Failed to create {layer_name} layer: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print(f"âš ï¸ No config found for {layer_name} layer")

        print(f"ğŸ—ï¸ Total layers built: {len(layers)} - {list(layers.keys())}")
        return layers

    def forward(self, batch: Dict[str, torch.Tensor], task_info: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åˆ†å±‚å¤„ç†å‰å‘ä¼ æ’­

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡æ•°æ®
            task_info: ä»»åŠ¡ä¿¡æ¯

        Returns:
            Dict: å„å±‚çš„è¾“å‡ºç»“æœ
        """
        self.total_forward_calls += 1
        outputs = {}
        context = self._build_context(batch, task_info)

        print(f"ğŸ”„ Starting hierarchical forward pass with {len(self.layers)} layers available")
        processing_order = self._get_processing_order()
        print(f"ğŸ”„ Processing order: {processing_order}")

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†å„å±‚
        for layer_name in processing_order:
            if layer_name not in self.layers:
                print(f"âš ï¸ Layer {layer_name} not found in available layers")
                continue
            layer = self.layers[layer_name]

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ¿€æ´»è¯¥å±‚
            should_activate = layer.should_activate(batch, context)
            print(f"ğŸ” Layer {layer_name} should_activate: {should_activate}")
            if not should_activate:
                continue

            # æ‰§è¡Œå±‚çš„å‰å‘ä¼ æ’­ï¼ˆå¸¦æ—¶é—´ç›‘æ§ï¼‰
            try:
                print(f"ğŸš€ Executing layer {layer_name}")
                layer_output = layer.forward_with_timing(batch, context)
                outputs[layer_name] = layer_output
                self.layer_activation_stats[layer_name] += 1
                print(f"âœ… Layer {layer_name} executed successfully, output keys: {list(layer_output.keys())}")

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context.update(layer_output)

                # å®‰å…¨å±‚å¯ä»¥ç«‹å³è¿”å›ï¼ˆç´§æ€¥æƒ…å†µï¼‰
                if layer_name == 'safety' and layer_output.get('emergency', False):
                    print(f"ğŸš¨ Emergency stop triggered by safety layer")
                    return {layer_name: layer_output}

            except Exception as e:
                print(f"âŒ Error in {layer_name} layer: {e}")
                import traceback
                traceback.print_exc()
                outputs[layer_name] = {
                    'layer': layer_name,
                    'error': str(e),
                    'execution_time_ms': 0
                }

        print(f"ğŸ¯ Forward pass completed with {len(outputs)} active layers: {list(outputs.keys())}")
        return outputs

    def inference_mode(self,
                      batch: Dict[str, torch.Tensor],
                      task_info: Optional[Dict[str, Any]] = None,
                      latency_budget_ms: float = 50.0) -> Dict[str, Any]:
        """
        æ¨ç†æ¨¡å¼ï¼šæ ¹æ®å»¶è¿Ÿé¢„ç®—è‡ªé€‚åº”æ¿€æ´»å±‚

        Args:
            batch: è¾“å…¥æ•°æ®
            task_info: ä»»åŠ¡ä¿¡æ¯
            latency_budget_ms: å»¶è¿Ÿé¢„ç®—ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            Dict: åœ¨é¢„ç®—å†…å®Œæˆçš„å±‚è¾“å‡º
        """
        start_time = time.time()
        outputs = {}
        context = self._build_context(batch, task_info)
        remaining_budget = latency_budget_ms

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†ï¼Œåœ¨é¢„ç®—å†…å°½å¯èƒ½å¤šåœ°æ¿€æ´»å±‚
        for layer_name in self._get_processing_order():
            if layer_name not in self.layers:
                continue
            layer = self.layers[layer_name]

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ—¶é—´é¢„ç®—
            layer_budget = layer.get_latency_budget()
            if remaining_budget < layer_budget:
                print(f"â° Skipping {layer_name} due to time budget ({remaining_budget:.1f}ms < {layer_budget}ms)")
                continue

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ¿€æ´»
            if not layer.should_activate(batch, context):
                continue

            # æ‰§è¡Œå±‚æ¨ç†
            layer_start = time.time()
            try:
                layer_output = layer.forward_with_timing(batch, context)
                outputs[layer_name] = layer_output
                context.update(layer_output)

                # æ›´æ–°å‰©ä½™é¢„ç®—
                layer_time = (time.time() - layer_start) * 1000
                remaining_budget -= layer_time

                # å®‰å…¨å±‚ç´§æ€¥æƒ…å†µ
                if layer_name == 'safety' and layer_output.get('emergency', False):
                    break

            except Exception as e:
                print(f"âŒ Inference error in {layer_name}: {e}")
                remaining_budget -= 1  # å°æƒ©ç½š

        total_time = (time.time() - start_time) * 1000
        outputs['_inference_stats'] = {
            'total_time_ms': total_time,
            'budget_ms': latency_budget_ms,
            'within_budget': total_time <= latency_budget_ms
        }

        return outputs

    def _build_context(self, batch: Dict[str, torch.Tensor], task_info: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = {
            'batch_size': batch.get('observation.state', list(batch.values())[0]).size(0),
            'device': list(batch.values())[0].device,
            'training': self.training
        }

        if task_info:
            context.update(task_info)

        return context

    def _get_processing_order(self) -> List[str]:
        """è·å–å±‚çš„å¤„ç†é¡ºåºï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰"""
        return sorted(self.layers.keys(), key=lambda x: self.layer_priorities.get(x, 999))

    def set_layer_enabled(self, layer_name: str, enabled: bool):
        """è®¾ç½®ç‰¹å®šå±‚çš„å¯ç”¨çŠ¶æ€"""
        if layer_name in self.layers:
            self.layers[layer_name].set_enabled(enabled)
            print(f"ğŸ“ Layer {layer_name} {'enabled' if enabled else 'disabled'}")

    def get_active_layers(self) -> List[str]:
        """è·å–å½“å‰å¯ç”¨çš„å±‚åˆ—è¡¨"""
        return [name for name, layer in self.layers.items() if layer.is_enabled()]

    def get_layer_performance(self, layer_name: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šå±‚çš„æ€§èƒ½ç»Ÿè®¡"""
        if layer_name in self.layers:
            return self.layers[layer_name].get_performance_stats()
        return {}

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ•´ä½“æ€§èƒ½ç»Ÿè®¡"""
        stats = {
            'total_forward_calls': self.total_forward_calls,
            'layer_activation_stats': self.layer_activation_stats.copy(),
            'layer_performance': {}
        }

        for layer_name, layer in self.layers.items():
            stats['layer_performance'][layer_name] = layer.get_performance_stats()

        return stats

    def reset_performance_stats(self):
        """é‡ç½®æ‰€æœ‰æ€§èƒ½ç»Ÿè®¡"""
        self.total_forward_calls = 0
        self.layer_activation_stats = {name: 0 for name in self.layers.keys()}

        for layer in self.layers.values():
            layer.reset_performance_stats()

    def check_layer_health(self) -> Dict[str, bool]:
        """æ£€æŸ¥å„å±‚çš„å¥åº·çŠ¶æ€ï¼ˆå»¶è¿Ÿé¢„ç®—ç­‰ï¼‰"""
        health_status = {}

        for layer_name, layer in self.layers.items():
            health_status[layer_name] = {
                'enabled': layer.is_enabled(),
                'within_budget': layer.check_latency_budget(),
                'activation_count': layer.activation_count
            }

        return health_status

    def auto_tune_layers(self, target_latency_ms: float = 50.0):
        """æ ¹æ®ç›®æ ‡å»¶è¿Ÿè‡ªåŠ¨è°ƒæ•´å±‚çš„å¯ç”¨çŠ¶æ€"""
        print(f"ğŸ”§ Auto-tuning layers for target latency: {target_latency_ms}ms")

        # è·å–å„å±‚çš„å¹³å‡æ‰§è¡Œæ—¶é—´
        layer_times = {}
        for layer_name, layer in self.layers.items():
            stats = layer.get_performance_stats()
            layer_times[layer_name] = stats.get('avg_time_ms', 0)

        # æŒ‰ä¼˜å…ˆçº§å’Œæ‰§è¡Œæ—¶é—´è°ƒæ•´
        cumulative_time = 0
        for layer_name in self._get_processing_order():
            layer_time = layer_times.get(layer_name, 0)

            if cumulative_time + layer_time <= target_latency_ms:
                self.set_layer_enabled(layer_name, True)
                cumulative_time += layer_time
            else:
                self.set_layer_enabled(layer_name, False)
                print(f"âš ï¸ Disabled {layer_name} to meet latency target")

        print(f"âœ… Auto-tuning complete. Estimated latency: {cumulative_time:.1f}ms")

    def __repr__(self) -> str:
        layer_info = ", ".join([f"{name}({'âœ“' if layer.is_enabled() else 'âœ—'})"
                               for name, layer in self.layers.items()])
        return f"HierarchicalScheduler({layer_info})"