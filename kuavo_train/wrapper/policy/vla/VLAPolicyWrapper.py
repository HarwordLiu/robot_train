"""
VLAPolicyWrapper: VLA Transformerç­–ç•¥ä¸»ç±»
"""
import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
from collections import deque

from kuavo_train.wrapper.policy.diffusion.DiffusionPolicyWrapper import CustomDiffusionPolicyWrapper
from .VLAConfigWrapper import VLAConfigWrapper
from .tokenizers.VisionTokenizer import VisionTokenizer
from .tokenizers.StateTokenizer import StateTokenizer
from .decoders.DiffusionDecoder import DiffusionDecoder

# å¯¼å…¥å½’ä¸€åŒ–å·¥å…·
from lerobot.policies.normalize import Normalize, Unnormalize


class VLAPolicyWrapper(CustomDiffusionPolicyWrapper):
    """
    VLA Transformerç­–ç•¥

    æ¶æ„æµç¨‹ï¼š
    è¾“å…¥ â†’ TokenåŒ– â†’ Transformer Encoder â†’ Tokenç©ºé—´Diffusion â†’ åŠ¨ä½œè¾“å‡º

    æ ¸å¿ƒç‰¹ç‚¹ï¼š
    - æ‰€æœ‰è¾“å…¥ç»´åº¦é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰
    - å®Œæ•´tokenåŒ–æ¶æ„
    - åœ¨tokenç©ºé—´åšdiffusion
    - å¯¹æ ‡OpenVLA/RT-2ç­‰SOTAæ–¹æ³•
    """

    def __init__(
        self,
        config: VLAConfigWrapper,
        dataset_stats: Optional[Dict[str, Dict[str, torch.Tensor]]] = None
    ):
        """
        åˆå§‹åŒ–VLAç­–ç•¥

        Args:
            config: VLAé…ç½®å¯¹è±¡
            dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        """
        # ä¸è°ƒç”¨CustomDiffusionPolicyWrapperçš„__init__ï¼Œå› ä¸ºæ¶æ„å®Œå…¨ä¸åŒ
        # ç›´æ¥åˆå§‹åŒ–nn.Module
        nn.Module.__init__(self)

        self.config = config

        # æ„å»ºå½’ä¸€åŒ–å™¨ï¼ˆä½¿ç”¨lerobotçš„Normalizeç±»ï¼‰
        self.normalize_inputs = Normalize(
            config.input_features,
            config.normalization_mapping,
            dataset_stats
        )
        self.normalize_targets = Normalize(
            config.output_features,
            config.normalization_mapping,
            dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_features,
            config.normalization_mapping,
            dataset_stats
        )

        # ä»é…ç½®è·å–å…³èŠ‚é…ç½®
        self.joint_configs = config.state_config.get('joints', [])
        if not self.joint_configs:
            raise ValueError("state_config must contain 'joints' list")

        # è·å–åŠ¨ä½œç»´åº¦
        if hasattr(config, 'output_features') and 'action' in config.output_features:
            self.action_dim = config.output_features["action"].shape[0]
        else:
            # ä»jointé…ç½®æ¨æ–­
            self.action_dim = len(self.joint_configs)

        print(f"ğŸ¤– Initializing VLA Transformer Policy")
        print(f"   Action dim: {self.action_dim}")
        print(f"   Horizon: {config.horizon}")
        print(f"   Token dim: {config.token_embed_dim}")

        # ==================== Tokenizers ====================
        self.vision_tokenizer = VisionTokenizer(
            patch_size=config.patch_size,
            embed_dim=config.token_embed_dim,
            image_size=config.image_size
        )

        self.state_tokenizer = StateTokenizer(
            embed_dim=config.token_embed_dim,
            max_joints=50
        )

        # ==================== Transformer Encoder ====================
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.token_embed_dim,
            nhead=config.transformer_heads,
            dim_feedforward=config.transformer_dim_feedforward,
            dropout=config.transformer_dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-LNæ¶æ„
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=config.transformer_depth
        )

        # ==================== Diffusion Decoder ====================
        self.diffusion_decoder = DiffusionDecoder(
            action_dim=self.action_dim,
            horizon=config.horizon,
            context_dim=config.token_embed_dim,
            num_train_timesteps=config.num_train_timesteps,
            num_denoiser_layers=config.num_denoiser_layers,
            num_heads=config.denoiser_heads,
            dim_feedforward=config.denoiser_dim_feedforward,
            noise_scheduler_type=config.noise_scheduler_type,
            beta_schedule=config.beta_schedule,
            beta_start=config.beta_start,
            beta_end=config.beta_end,
            prediction_type=config.prediction_type,
            clip_sample=config.clip_sample,
            clip_sample_range=config.clip_sample_range
        )

        # è§‚æµ‹å’ŒåŠ¨ä½œé˜Ÿåˆ—ï¼ˆç”¨äºæ¨ç†ï¼‰
        self._queues = None
        self.reset()

        print(f"âœ… VLA Transformer Policy initialized successfully")

    def reset(self):
        """é‡ç½®è§‚æµ‹å’ŒåŠ¨ä½œé˜Ÿåˆ—"""
        self._queues = {
            "observation.state": deque(maxlen=self.config.n_obs_steps),
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if self.config.image_features:
            self._queues["observation.images"] = deque(
                maxlen=self.config.n_obs_steps)
        if self.config.use_depth and self.config.depth_features:
            self._queues["observation.depth"] = deque(
                maxlen=self.config.n_obs_steps)

    def forward(self, batch: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Optional[Dict[str, Any]]]:
        """
        è®­ç»ƒforward

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡ï¼ŒåŒ…å«:
                - observation.images: [B, T, C, H, W] RGBå›¾åƒ
                - observation.depth: [B, T, C, H, W] æ·±åº¦å›¾ï¼ˆå¯é€‰ï¼‰
                - observation.state: [B, T, state_dim] çŠ¶æ€
                - action: [B, horizon, action_dim] ç›®æ ‡åŠ¨ä½œ

        Returns:
            loss: æ ‡é‡tensor
            info: é¢å¤–ä¿¡æ¯å­—å…¸ï¼ˆå¯ä¸ºNoneï¼‰
        """
        # 1. å½’ä¸€åŒ–è¾“å…¥
        batch = self.normalize_inputs(batch)
        batch = self.normalize_targets(batch)

        # 2. TokenåŒ–æ‰€æœ‰è¾“å…¥
        # Vision tokens
        rgb_images = batch.get('observation.images')
        depth_images = batch.get(
            'observation.depth') if self.config.use_depth else None

        if rgb_images is not None:
            # å¤„ç†æ—¶é—´ç»´åº¦ï¼šå–æœ€åä¸€å¸§æˆ–å¹³å‡
            if len(rgb_images.shape) == 5:  # [B, T, C, H, W]
                rgb_images = rgb_images[:, -1]  # å–æœ€åä¸€å¸§ [B, C, H, W]
            if depth_images is not None and len(depth_images.shape) == 5:
                depth_images = depth_images[:, -1]

            vision_tokens = self.vision_tokenizer(rgb_images, depth_images)
        else:
            # å¦‚æœæ²¡æœ‰è§†è§‰è¾“å…¥ï¼Œåˆ›å»ºé›¶tokens
            batch_size = batch['observation.state'].shape[0]
            device = batch['observation.state'].device
            num_patches = self.vision_tokenizer.num_patches
            vision_tokens = torch.zeros(
                batch_size, num_patches, self.config.token_embed_dim,
                device=device
            )

        # State tokens
        state = batch['observation.state']
        if len(state.shape) == 3:  # [B, T, state_dim]
            state = state[:, -1]  # å–æœ€åä¸€å¸§ [B, state_dim]

        state_tokens = self.state_tokenizer(state, self.joint_configs)

        # 3. æ‹¼æ¥æ‰€æœ‰tokens
        all_tokens = torch.cat([vision_tokens, state_tokens], dim=1)
        # [B, num_vision_tokens + num_state_tokens, token_embed_dim]

        # 4. Transformerç¼–ç 
        context_tokens = self.transformer_encoder(all_tokens)

        # 5. Diffusion loss
        target_actions = batch['action']
        loss = self.diffusion_decoder.compute_loss(
            target_actions, context_tokens)

        # è¿”å›losså’Œç©ºçš„infoå­—å…¸
        return loss, None

    def select_action(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        æ¨ç†forwardï¼šç”ŸæˆåŠ¨ä½œ

        Args:
            batch: è§‚æµ‹æ•°æ®

        Returns:
            action: [B, action_dim] ä¸‹ä¸€æ­¥åŠ¨ä½œ
        """
        # 1. å½’ä¸€åŒ–è¾“å…¥
        batch = self.normalize_inputs(batch)

        # 2. TokenåŒ–è¾“å…¥
        rgb_images = batch.get('observation.images')
        depth_images = batch.get(
            'observation.depth') if self.config.use_depth else None

        if rgb_images is not None:
            if len(rgb_images.shape) == 5:
                rgb_images = rgb_images[:, -1]
            if depth_images is not None and len(depth_images.shape) == 5:
                depth_images = depth_images[:, -1]

            vision_tokens = self.vision_tokenizer(rgb_images, depth_images)
        else:
            batch_size = batch['observation.state'].shape[0]
            device = batch['observation.state'].device
            num_patches = self.vision_tokenizer.num_patches
            vision_tokens = torch.zeros(
                batch_size, num_patches, self.config.token_embed_dim,
                device=device
            )

        state = batch['observation.state']
        if len(state.shape) == 3:
            state = state[:, -1]

        state_tokens = self.state_tokenizer(state, self.joint_configs)

        # 3. æ‹¼æ¥å¹¶ç¼–ç 
        all_tokens = torch.cat([vision_tokens, state_tokens], dim=1)
        context_tokens = self.transformer_encoder(all_tokens)

        # 4. Diffusioné‡‡æ ·
        num_inference_steps = self.config.num_inference_steps or 50
        actions = self.diffusion_decoder.sample(
            context_tokens,
            num_inference_steps=num_inference_steps
        )

        # è¿”å›ç¬¬ä¸€æ­¥åŠ¨ä½œ
        return actions[:, 0]

    def _save_pretrained(self, save_directory: Path) -> None:
        """ä¿å­˜æ¨¡å‹"""
        print(f"ğŸ’¾ Saving VLA model to {save_directory}")

        save_directory.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜é…ç½®
        self.config._save_pretrained(save_directory)

        # ä¿å­˜æ¨¡å‹æƒé‡
        state_dict = self.state_dict()

        # ä½¿ç”¨safetensorsæ ¼å¼
        from safetensors.torch import save_file
        model_file = save_directory / "model.safetensors"
        save_file(state_dict, model_file)

        print(f"âœ… VLA model saved successfully")

    @classmethod
    def from_pretrained(cls, pretrained_name_or_path, **kwargs):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        print(f"ğŸ“‚ Loading VLA model from {pretrained_name_or_path}")

        # åŠ è½½é…ç½®
        config = VLAConfigWrapper.from_pretrained(pretrained_name_or_path)

        # åˆ›å»ºæ¨¡å‹
        model = cls(config)

        # åŠ è½½æƒé‡
        from safetensors.torch import load_file
        model_file = Path(pretrained_name_or_path) / "model.safetensors"
        if model_file.exists():
            state_dict = load_file(model_file)
            model.load_state_dict(state_dict)
            print(f"âœ… VLA model loaded successfully")
        else:
            print(f"âš ï¸  Model weights not found, using random initialization")

        return model
