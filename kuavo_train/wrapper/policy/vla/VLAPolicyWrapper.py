"""
VLAPolicyWrapper: VLA Transformerç­–ç•¥ä¸»ç±»
"""
import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
from collections import deque

from kuavo_train.wrapper.policy.diffusion.DiffusionPolicyWrapper import CustomDiffusionPolicyWrapper
from .VLAConfigWrapper import VLAConfigWrapper
from .tokenizers.VisionTokenizer import VisionTokenizer
from .tokenizers.StateTokenizer import StateTokenizer
from .decoders.DiffusionDecoder import DiffusionDecoder

# å¯¼å…¥å½’ä¸€åŒ–å·¥å…·
from lerobot.policies.normalize import Normalize, Unnormalize

# å¯¼å…¥å›¾åƒå¤„ç†å·¥å…·
from kuavo_train.utils.augmenter import crop_image, resize_image


class VLAPolicyWrapper(CustomDiffusionPolicyWrapper):
    """
    VLA Transformerç­–ç•¥

    æ¶æ„æµç¨‹ï¼š
    è¾“å…¥ â†’ TokenåŒ– â†’ Transformer Encoder â†’ Tokenç©ºé—´Diffusion â†’ åŠ¨ä½œè¾“å‡º

    æ ¸å¿ƒç‰¹ç‚¹ï¼š
    - æ‰€æœ‰è¾“å…¥ç»´åº¦é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰
    - å®Œæ•´tokenåŒ–æ¶æ„
    - åœ¨tokenç©ºé—´åšdiffusion
    - å¯¹æ ‡OpenVLA/RT-2ç­‰SOTAæ–¹æ³•
    """

    def __init__(
        self,
        config: VLAConfigWrapper,
        dataset_stats: Optional[Dict[str, Dict[str, torch.Tensor]]] = None
    ):
        """
        åˆå§‹åŒ–VLAç­–ç•¥

        Args:
            config: VLAé…ç½®å¯¹è±¡
            dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        """
        # ä¸è°ƒç”¨CustomDiffusionPolicyWrapperçš„__init__ï¼Œå› ä¸ºæ¶æ„å®Œå…¨ä¸åŒ
        # ç›´æ¥åˆå§‹åŒ–nn.Module
        nn.Module.__init__(self)

        self.config = config

        # æ„å»ºå½’ä¸€åŒ–å™¨ï¼ˆä½¿ç”¨lerobotçš„Normalizeç±»ï¼‰
        self.normalize_inputs = Normalize(
            config.input_features,
            config.normalization_mapping,
            dataset_stats
        )
        self.normalize_targets = Normalize(
            config.output_features,
            config.normalization_mapping,
            dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_features,
            config.normalization_mapping,
            dataset_stats
        )

        # ä»é…ç½®è·å–å…³èŠ‚é…ç½®
        self.joint_configs = config.state_config.get('joints', [])
        if not self.joint_configs:
            raise ValueError("state_config must contain 'joints' list")

        # å›¾åƒé¢„å¤„ç†é…ç½®
        self.crop_shape = getattr(config, 'crop_shape', None)
        self.crop_is_random = getattr(config, 'crop_is_random', True)

        # è·å–åŠ¨ä½œç»´åº¦
        if hasattr(config, 'output_features') and 'action' in config.output_features:
            self.action_dim = config.output_features["action"].shape[0]
        else:
            # ä»jointé…ç½®æ¨æ–­
            self.action_dim = len(self.joint_configs)

        print(f"ğŸ¤– Initializing VLA Transformer Policy")
        print(f"   Action dim: {self.action_dim}")
        print(f"   Horizon: {config.horizon}")
        print(f"   Token dim: {config.token_embed_dim}")

        # ==================== Tokenizers ====================
        self.vision_tokenizer = VisionTokenizer(
            patch_size=config.patch_size,
            embed_dim=config.token_embed_dim,
            image_size=config.image_size
        )

        self.state_tokenizer = StateTokenizer(
            embed_dim=config.token_embed_dim,
            max_joints=50
        )

        # ==================== Transformer Encoder ====================
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.token_embed_dim,
            nhead=config.transformer_heads,
            dim_feedforward=config.transformer_dim_feedforward,
            dropout=config.transformer_dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-LNæ¶æ„
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=config.transformer_depth
        )

        # ==================== Diffusion Decoder ====================
        self.diffusion_decoder = DiffusionDecoder(
            action_dim=self.action_dim,
            horizon=config.horizon,
            context_dim=config.token_embed_dim,
            num_train_timesteps=config.num_train_timesteps,
            num_denoiser_layers=config.num_denoiser_layers,
            num_heads=config.denoiser_heads,
            dim_feedforward=config.denoiser_dim_feedforward,
            noise_scheduler_type=config.noise_scheduler_type,
            beta_schedule=config.beta_schedule,
            beta_start=config.beta_start,
            beta_end=config.beta_end,
            prediction_type=config.prediction_type,
            clip_sample=config.clip_sample,
            clip_sample_range=config.clip_sample_range
        )

        # è§‚æµ‹å’ŒåŠ¨ä½œé˜Ÿåˆ—ï¼ˆç”¨äºæ¨ç†ï¼‰
        self._queues = None
        self.reset()

        print(f"âœ… VLA Transformer Policy initialized successfully")

    def _preprocess_images(self, images: torch.Tensor, random_crop: bool = True, image_type: str = "rgb") -> torch.Tensor:
        """
        é¢„å¤„ç†å›¾åƒï¼šcrop + resizeåˆ°VisionTokenizeræœŸæœ›çš„å°ºå¯¸

        Args:
            images: [B, C, H, W] è¾“å…¥å›¾åƒ
            random_crop: æ˜¯å¦éšæœºè£å‰ªï¼ˆè®­ç»ƒæ—¶Trueï¼Œæ¨ç†æ—¶Falseï¼‰
            image_type: å›¾åƒç±»å‹ ("rgb" æˆ– "depth")

        Returns:
            processed_images: [B, C, image_size, image_size] å¤„ç†åçš„å›¾åƒ
        """
        if self.crop_shape is not None:
            # 1. Cropåˆ°crop_shape
            images, _ = crop_image(
                images,
                target_range=self.crop_shape,
                random_crop=(random_crop and self.crop_is_random)
            )

        # 2. Resizeåˆ°VisionTokenizeræœŸæœ›çš„image_size
        target_size = [self.config.image_size, self.config.image_size]
        images = resize_image(images, target_size=target_size, image_type=image_type)

        return images

    def reset(self):
        """é‡ç½®è§‚æµ‹å’ŒåŠ¨ä½œé˜Ÿåˆ—"""
        self._queues = {
            "observation.state": deque(maxlen=self.config.n_obs_steps),
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if self.config.image_features:
            self._queues["observation.images"] = deque(
                maxlen=self.config.n_obs_steps)
        if self.config.use_depth and self.config.depth_features:
            self._queues["observation.depth"] = deque(
                maxlen=self.config.n_obs_steps)

    def forward(self, batch: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Optional[Dict[str, Any]]]:
        """
        è®­ç»ƒforward

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡ï¼ŒåŒ…å«:
                - observation.image.{camera_name}: [B, T, C, H, W] å„ç›¸æœºRGBå›¾åƒ
                - observation.depth.{camera_name}: [B, T, C, H, W] å„ç›¸æœºæ·±åº¦å›¾ï¼ˆå¯é€‰ï¼‰
                - observation.state: [B, T, state_dim] çŠ¶æ€
                - action: [B, horizon, action_dim] ç›®æ ‡åŠ¨ä½œ

        Returns:
            loss: æ ‡é‡tensor
            info: é¢å¤–ä¿¡æ¯å­—å…¸ï¼ˆå¯ä¸ºNoneï¼‰
        """
        # 1. é¢„å¤„ç†å›¾åƒï¼šcrop + resizeï¼ˆè®­ç»ƒæ—¶éšæœºcropï¼‰
        if self.config.image_features:
            batch = dict(batch)  # shallow copy
            for key in self.config.image_features:
                images = batch[key]
                # å¤„ç†æ—¶é—´ç»´åº¦ï¼šå–æœ€åä¸€å¸§
                if len(images.shape) == 5:  # [B, T, C, H, W]
                    images = images[:, -1]  # [B, C, H, W]
                images = self._preprocess_images(images, random_crop=True, image_type="rgb")
                batch[key] = images  # [B, C, 224, 224]

        if self.config.use_depth and self.config.depth_features:
            batch = dict(batch)
            for key in self.config.depth_features:
                depth = batch[key]
                if len(depth.shape) == 5:
                    depth = depth[:, -1]
                depth = self._preprocess_images(depth, random_crop=True, image_type="depth")
                batch[key] = depth

        # 2. å½’ä¸€åŒ–è¾“å…¥
        batch = self.normalize_inputs(batch)
        batch = self.normalize_targets(batch)

        # 3. Stackå„ç›¸æœºå›¾åƒ
        if self.config.image_features:
            # Stackæ‰€æœ‰ç›¸æœº: [B, num_cameras, C, H, W]
            rgb_images = torch.stack([batch[key] for key in self.config.image_features], dim=1)
        else:
            rgb_images = None

        if self.config.use_depth and self.config.depth_features:
            depth_images = torch.stack([batch[key] for key in self.config.depth_features], dim=1)
        else:
            depth_images = None

        # 4. TokenåŒ–æ‰€æœ‰è¾“å…¥
        if rgb_images is not None:
            # rgb_images: [B, num_cameras, C, H, W]
            # VisionTokenizerä¼šå¤„ç†å¤šç›¸æœºè¾“å…¥
            vision_tokens = self.vision_tokenizer(rgb_images, depth_images)
        else:
            # å¦‚æœæ²¡æœ‰è§†è§‰è¾“å…¥ï¼Œåˆ›å»ºé›¶tokens
            batch_size = batch['observation.state'].shape[0]
            device = batch['observation.state'].device
            num_patches = self.vision_tokenizer.num_patches
            vision_tokens = torch.zeros(
                batch_size, num_patches, self.config.token_embed_dim,
                device=device
            )

        # State tokens
        state = batch['observation.state']
        if len(state.shape) == 3:  # [B, T, state_dim]
            state = state[:, -1]  # å–æœ€åä¸€å¸§ [B, state_dim]

        state_tokens = self.state_tokenizer(state, self.joint_configs)

        # 3. æ‹¼æ¥æ‰€æœ‰tokens
        all_tokens = torch.cat([vision_tokens, state_tokens], dim=1)
        # [B, num_vision_tokens + num_state_tokens, token_embed_dim]

        # 4. Transformerç¼–ç 
        context_tokens = self.transformer_encoder(all_tokens)

        # 5. Diffusion loss
        target_actions = batch['action']
        loss = self.diffusion_decoder.compute_loss(
            target_actions, context_tokens)

        # è¿”å›losså’Œç©ºçš„infoå­—å…¸
        return loss, None

    def select_action(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        æ¨ç†forwardï¼šç”ŸæˆåŠ¨ä½œ

        Args:
            batch: è§‚æµ‹æ•°æ®ï¼ŒåŒ…å«å„ç›¸æœºçš„observation keys

        Returns:
            action: [B, action_dim] ä¸‹ä¸€æ­¥åŠ¨ä½œ
        """
        # 1. é¢„å¤„ç†å›¾åƒï¼šcrop + resizeï¼ˆæ¨ç†æ—¶ä¸­å¿ƒcropï¼‰
        if self.config.image_features:
            batch = dict(batch)
            for key in self.config.image_features:
                images = batch[key]
                if len(images.shape) == 5:  # [B, T, C, H, W]
                    images = images[:, -1]  # [B, C, H, W]
                images = self._preprocess_images(images, random_crop=False, image_type="rgb")
                batch[key] = images

        if self.config.use_depth and self.config.depth_features:
            batch = dict(batch)
            for key in self.config.depth_features:
                depth = batch[key]
                if len(depth.shape) == 5:
                    depth = depth[:, -1]
                depth = self._preprocess_images(depth, random_crop=False, image_type="depth")
                batch[key] = depth

        # 2. å½’ä¸€åŒ–è¾“å…¥
        batch = self.normalize_inputs(batch)

        # 3. Stackå„ç›¸æœºå›¾åƒ
        if self.config.image_features:
            rgb_images = torch.stack([batch[key] for key in self.config.image_features], dim=1)
        else:
            rgb_images = None

        if self.config.use_depth and self.config.depth_features:
            depth_images = torch.stack([batch[key] for key in self.config.depth_features], dim=1)
        else:
            depth_images = None

        # 4. TokenåŒ–è¾“å…¥
        if rgb_images is not None:
            vision_tokens = self.vision_tokenizer(rgb_images, depth_images)
        else:
            batch_size = batch['observation.state'].shape[0]
            device = batch['observation.state'].device
            num_patches = self.vision_tokenizer.num_patches
            vision_tokens = torch.zeros(
                batch_size, num_patches, self.config.token_embed_dim,
                device=device
            )

        state = batch['observation.state']
        if len(state.shape) == 3:
            state = state[:, -1]

        state_tokens = self.state_tokenizer(state, self.joint_configs)

        # 3. æ‹¼æ¥å¹¶ç¼–ç 
        all_tokens = torch.cat([vision_tokens, state_tokens], dim=1)
        context_tokens = self.transformer_encoder(all_tokens)

        # 4. Diffusioné‡‡æ ·
        num_inference_steps = self.config.num_inference_steps or 50
        actions = self.diffusion_decoder.sample(
            context_tokens,
            num_inference_steps=num_inference_steps
        )

        # 5. Unnormalizeè¾“å‡ºï¼ˆé‡è¦ï¼é‡‡æ ·å‡ºæ¥çš„åŠ¨ä½œæ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼‰
        unnormalized_actions = self.unnormalize_outputs({'action': actions})['action']

        # 6. è¿”å›ç¬¬ä¸€æ­¥åŠ¨ä½œ
        return unnormalized_actions[:, 0]

    def _save_pretrained(self, save_directory: Path) -> None:
        """ä¿å­˜æ¨¡å‹"""
        print(f"ğŸ’¾ Saving VLA model to {save_directory}")

        save_directory.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜é…ç½®
        self.config._save_pretrained(save_directory)

        # ä¿å­˜æ¨¡å‹æƒé‡
        state_dict = self.state_dict()

        # ä½¿ç”¨safetensorsæ ¼å¼
        from safetensors.torch import save_file
        model_file = save_directory / "model.safetensors"
        save_file(state_dict, model_file)

        print(f"âœ… VLA model saved successfully")

    @classmethod
    def from_pretrained(cls, pretrained_name_or_path, **kwargs):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        print(f"ğŸ“‚ Loading VLA model from {pretrained_name_or_path}")

        # åŠ è½½é…ç½®
        config = VLAConfigWrapper.from_pretrained(pretrained_name_or_path)

        # åˆ›å»ºæ¨¡å‹
        model = cls(config)

        # åŠ è½½æƒé‡
        from safetensors.torch import load_file
        model_file = Path(pretrained_name_or_path) / "model.safetensors"
        if model_file.exists():
            state_dict = load_file(model_file)
            model.load_state_dict(state_dict)
            print(f"âœ… VLA model loaded successfully")
        else:
            print(f"âš ï¸  Model weights not found, using random initialization")

        return model
