"""
åŠ è½½é¢„è®­ç»ƒçš„Patch Embeddingæƒé‡
æ”¯æŒä»DINO, MAE, CLIPç­‰é¢„è®­ç»ƒViTæ¨¡å‹åŠ è½½
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional
import warnings


def load_pretrained_patch_embedding(
    patch_size: int = 32,
    embed_dim: int = 256,
    model_name: str = 'dino'
) -> Optional[torch.Tensor]:
    """
    ä»é¢„è®­ç»ƒViTæ¨¡å‹åŠ è½½patch embeddingæƒé‡

    Args:
        patch_size: ç›®æ ‡patchå¤§å°ï¼ˆ16æˆ–32ï¼‰
        embed_dim: ç›®æ ‡embeddingç»´åº¦
        model_name: é¢„è®­ç»ƒæ¨¡å‹åç§° ('dino', 'mae', 'clip')

    Returns:
        weight: [embed_dim, 3, patch_size, patch_size] å·ç§¯æƒé‡
                å¦‚æœåŠ è½½å¤±è´¥è¿”å›None
    """
    try:
        if model_name.lower() == 'dino':
            return _load_from_dino(patch_size, embed_dim)
        elif model_name.lower() == 'mae':
            return _load_from_mae(patch_size, embed_dim)
        elif model_name.lower() == 'clip':
            return _load_from_clip(patch_size, embed_dim)
        else:
            warnings.warn(f"Unknown model_name: {model_name}, using DINO as default")
            return _load_from_dino(patch_size, embed_dim)
    except Exception as e:
        warnings.warn(f"Failed to load pretrained weights: {e}")
        return None


def _load_from_dino(patch_size: int, embed_dim: int) -> Optional[torch.Tensor]:
    """
    ä»DINO ViTæ¨¡å‹åŠ è½½æƒé‡

    DINOé»˜è®¤é…ç½®ï¼š
    - vit_small: patch=16, dim=384
    - vit_base: patch=16, dim=768
    """
    try:
        # å°è¯•åŠ è½½DINOå°æ¨¡å‹ï¼ˆæ›´è½»é‡ï¼‰
        model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=True)

        # æå–patch_embedçš„æŠ•å½±å±‚æƒé‡
        # shape: [384, 3, 16, 16]
        pretrained_weight = model.patch_embed.proj.weight.data.clone()
        pretrained_dim, in_channels, pretrained_patch_h, pretrained_patch_w = pretrained_weight.shape

        print(f"ğŸ“¦ Loaded DINO pretrained weights: [{pretrained_dim}, {in_channels}, {pretrained_patch_h}, {pretrained_patch_w}]")

        # 1. è°ƒæ•´patch sizeï¼ˆå¦‚æœéœ€è¦ï¼‰
        if patch_size != pretrained_patch_h:
            # ä½¿ç”¨æ’å€¼è°ƒæ•´patchå¤§å°
            # æ–¹æ³•ï¼šå°†å·ç§¯æ ¸reshapeåæ’å€¼ï¼Œå†reshapeå›å»
            weight = F.interpolate(
                pretrained_weight,
                size=(patch_size, patch_size),
                mode='bilinear',
                align_corners=False
            )
            print(f"   âœ“ Interpolated patch size: {pretrained_patch_h}â†’{patch_size}")
        else:
            weight = pretrained_weight

        # 2. è°ƒæ•´embeddingç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if embed_dim != pretrained_dim:
            # æ–¹æ³•1ï¼šå¦‚æœç›®æ ‡ç»´åº¦æ›´å°ï¼Œä½¿ç”¨PCAé™ç»´
            # æ–¹æ³•2ï¼šå¦‚æœç›®æ ‡ç»´åº¦æ›´å¤§ï¼Œç”¨é›¶å¡«å……
            if embed_dim < pretrained_dim:
                # ç®€å•æˆªæ–­ï¼ˆä¿ç•™å‰embed_dimç»´ï¼‰
                weight = weight[:embed_dim]
                print(f"   âœ“ Truncated embed dim: {pretrained_dim}â†’{embed_dim}")
            else:
                # é›¶å¡«å……
                padding = torch.zeros(embed_dim - pretrained_dim, in_channels, patch_size, patch_size)
                weight = torch.cat([weight, padding], dim=0)
                print(f"   âœ“ Padded embed dim: {pretrained_dim}â†’{embed_dim}")

        print(f"âœ… Final weight shape: {list(weight.shape)}")
        return weight

    except Exception as e:
        warnings.warn(f"Failed to load DINO weights: {e}")
        return None


def _load_from_mae(patch_size: int, embed_dim: int) -> Optional[torch.Tensor]:
    """
    ä»MAE (Masked Autoencoder) æ¨¡å‹åŠ è½½æƒé‡

    MAEé»˜è®¤é…ç½®ï¼š
    - vit_base: patch=16, dim=768
    """
    try:
        # MAEéœ€è¦timmåº“
        import timm

        # åŠ è½½é¢„è®­ç»ƒMAEæ¨¡å‹
        model = timm.create_model('vit_base_patch16_224', pretrained=True)

        pretrained_weight = model.patch_embed.proj.weight.data.clone()
        pretrained_dim, in_channels, pretrained_patch_h, pretrained_patch_w = pretrained_weight.shape

        print(f"ğŸ“¦ Loaded MAE pretrained weights: [{pretrained_dim}, {in_channels}, {pretrained_patch_h}, {pretrained_patch_w}]")

        # è°ƒæ•´å°ºå¯¸ï¼ˆåŒDINOï¼‰
        if patch_size != pretrained_patch_h:
            weight = F.interpolate(
                pretrained_weight,
                size=(patch_size, patch_size),
                mode='bilinear',
                align_corners=False
            )
            print(f"   âœ“ Interpolated patch size: {pretrained_patch_h}â†’{patch_size}")
        else:
            weight = pretrained_weight

        if embed_dim != pretrained_dim:
            if embed_dim < pretrained_dim:
                weight = weight[:embed_dim]
                print(f"   âœ“ Truncated embed dim: {pretrained_dim}â†’{embed_dim}")
            else:
                padding = torch.zeros(embed_dim - pretrained_dim, in_channels, patch_size, patch_size)
                weight = torch.cat([weight, padding], dim=0)
                print(f"   âœ“ Padded embed dim: {pretrained_dim}â†’{embed_dim}")

        print(f"âœ… Final weight shape: {list(weight.shape)}")
        return weight

    except ImportError:
        warnings.warn("timm not installed, cannot load MAE weights. Install with: pip install timm")
        return None
    except Exception as e:
        warnings.warn(f"Failed to load MAE weights: {e}")
        return None


def _load_from_clip(patch_size: int, embed_dim: int) -> Optional[torch.Tensor]:
    """
    ä»CLIPæ¨¡å‹åŠ è½½æƒé‡

    CLIPé»˜è®¤é…ç½®ï¼š
    - ViT-B/16: patch=16, dim=768
    - ViT-B/32: patch=32, dim=768
    """
    try:
        import clip

        # æ ¹æ®patch_sizeé€‰æ‹©æ¨¡å‹
        if patch_size == 16:
            model_type = "ViT-B/16"
        elif patch_size == 32:
            model_type = "ViT-B/32"
        else:
            warnings.warn(f"CLIP doesn't have pretrained weights for patch_size={patch_size}, using ViT-B/16")
            model_type = "ViT-B/16"

        model, _ = clip.load(model_type, device="cpu")

        # CLIPçš„patch embeddingåœ¨visual.conv1
        pretrained_weight = model.visual.conv1.weight.data.clone()
        pretrained_dim, in_channels, pretrained_patch_h, pretrained_patch_w = pretrained_weight.shape

        print(f"ğŸ“¦ Loaded CLIP {model_type} weights: [{pretrained_dim}, {in_channels}, {pretrained_patch_h}, {pretrained_patch_w}]")

        # è°ƒæ•´å°ºå¯¸
        if patch_size != pretrained_patch_h:
            weight = F.interpolate(
                pretrained_weight,
                size=(patch_size, patch_size),
                mode='bilinear',
                align_corners=False
            )
            print(f"   âœ“ Interpolated patch size: {pretrained_patch_h}â†’{patch_size}")
        else:
            weight = pretrained_weight

        if embed_dim != pretrained_dim:
            if embed_dim < pretrained_dim:
                weight = weight[:embed_dim]
                print(f"   âœ“ Truncated embed dim: {pretrained_dim}â†’{embed_dim}")
            else:
                padding = torch.zeros(embed_dim - pretrained_dim, in_channels, patch_size, patch_size)
                weight = torch.cat([weight, padding], dim=0)
                print(f"   âœ“ Padded embed dim: {pretrained_dim}â†’{embed_dim}")

        print(f"âœ… Final weight shape: {list(weight.shape)}")
        return weight

    except ImportError:
        warnings.warn("clip not installed, cannot load CLIP weights. Install with: pip install git+https://github.com/openai/CLIP.git")
        return None
    except Exception as e:
        warnings.warn(f"Failed to load CLIP weights: {e}")
        return None


# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    print("Testing pretrained patch embedding loader...")

    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        (32, 256, 'dino'),  # æˆ‘ä»¬çš„é…ç½®
        (16, 384, 'dino'),  # DINOåŸç”Ÿ
        (32, 768, 'clip'),  # CLIP ViT-B/32
    ]

    for patch_size, embed_dim, model_name in configs:
        print(f"\n{'='*60}")
        print(f"Testing: patch_size={patch_size}, embed_dim={embed_dim}, model={model_name}")
        print(f"{'='*60}")

        weight = load_pretrained_patch_embedding(patch_size, embed_dim, model_name)

        if weight is not None:
            print(f"âœ… Success! Weight shape: {weight.shape}")
        else:
            print(f"âŒ Failed to load weights")
