"""
SmolVLA Diffusion Policy Wrapper for Kuavo Project

åŸºäº SmolVLA æ¶æ„ä½†ä½¿ç”¨ Diffusion è¿›è¡ŒåŠ¨ä½œç”Ÿæˆ
å®Œå…¨å†»ç»“è§†è§‰å±‚ï¼Œä¸“æ³¨äºè®­ç»ƒ Action Expert çš„ Diffusion èƒ½åŠ›
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, List
from pathlib import Path
import numpy as np

from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig

from .SmolVLADiffusionConfigWrapper import SmolVLADiffusionConfigWrapper
from .diffusion_scheduler import DDIMScheduler, DDPMScheduler


class SmolVLADiffusionPolicyWrapper(SmolVLAPolicy):
    """
    SmolVLA Diffusion ç­–ç•¥åŒ…è£…å™¨

    ä¸»è¦å˜åŒ–ï¼š
    1. æ›¿æ¢ Flow Matching ä¸º Diffusion
    2. å®Œå…¨å†»ç»“è§†è§‰ç¼–ç å™¨
    3. å®ç°æ‰©æ•£é‡‡æ ·è¿‡ç¨‹
    4. ä¿æŒä¸åŸå§‹ SmolVLA çš„å…¼å®¹æ€§
    """

    def __init__(
        self,
        config: SmolVLADiffusionConfigWrapper,
        dataset_stats: Optional[Dict[str, Dict[str, torch.Tensor]]] = None,
    ):
        """
        åˆå§‹åŒ– SmolVLA Diffusion ç­–ç•¥

        Args:
            config: SmolVLA Diffusion é…ç½®å¯¹è±¡
            dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        """
        # ç¡®ä¿ config æ˜¯ Diffusion é…ç½®
        if not isinstance(config, SmolVLADiffusionConfigWrapper):
            raise TypeError("config å¿…é¡»æ˜¯ SmolVLADiffusionConfigWrapper ç±»å‹")

        # ä¿å­˜é…ç½®
        self.config = config

        # åˆ›å»ºå™ªå£°è°ƒåº¦å™¨
        self.scheduler = config.create_scheduler(device=config.device)

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        # æ³¨æ„ï¼šè¿™é‡Œä¼šè°ƒç”¨ SmolVLAPolicy çš„ __init__ï¼Œå®ƒä½¿ç”¨çš„æ˜¯ VLAFlowMatching
        # æˆ‘ä»¬ç¨åéœ€è¦æ›¿æ¢ä¸º Diffusion
        super().__init__(config, dataset_stats)

        # æ›¿æ¢ Flow Matching ä¸º Diffusion å¤´
        self._replace_flow_matching_with_diffusion()

        # å†»ç»“è§†è§‰å±‚
        self._freeze_vision_encoder()

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()

    def _replace_flow_matching_with_diffusion(self):
        """
        æ›¿æ¢ Flow Matching æ¨¡å—ä¸º Diffusion æ¨¡å—
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ VLAFlowMatching æ¨¡å—
        if hasattr(self.model, 'vlm_with_expert') and hasattr(self.model.vlm_with_expert, 'action_expert'):
            action_expert = self.model.vlm_with_expert.action_expert

            if hasattr(action_expert, 'flow_matching_head'):
                # è·å– Flow Matching å¤´çš„è¾“å…¥ç»´åº¦
                flow_head = action_expert.flow_matching_head
                if hasattr(flow_head, 'linear') or hasattr(flow_head, 'nn'):
                    # è·å–è¾“å…¥ç‰¹å¾ç»´åº¦
                    if hasattr(flow_head, 'linear'):
                        input_dim = flow_head.linear.in_features
                    elif hasattr(flow_head, 'nn') and hasattr(flow_head.nn, '0'):
                        if hasattr(flow_head.nn['0'], 'in_features'):
                            input_dim = flow_head.nn['0'].in_features
                        else:
                            input_dim = 512  # é»˜è®¤å€¼
                    else:
                        input_dim = 512

                    # åˆ›å»º Diffusion UNet å¤´
                    diffusion_head = DiffusionUNetHead(
                        input_dim=input_dim,
                        output_dim=self.config.max_action_dim * self.config.chunk_size,
                        hidden_dim=1024,
                        num_layers=6,
                        time_embedding_dim=128,
                    )

                    # æ›¿æ¢ Flow Matching å¤´
                    action_expert.flow_matching_head = diffusion_head
                    print("âœ… æˆåŠŸæ›¿æ¢ Flow Matching å¤´ä¸º Diffusion UNet å¤´")
                else:
                    print("âš ï¸ æ— æ³•ç¡®å®š Flow Matching å¤´çš„ç»“æ„ï¼Œè·³è¿‡æ›¿æ¢")
            else:
                print("âš ï¸ æœªæ‰¾åˆ° flow_matching_headï¼Œå¯èƒ½å·²ç»æ˜¯ Diffusion ç‰ˆæœ¬")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° action_expertï¼Œè·³è¿‡æ›¿æ¢")

    def _freeze_vision_encoder(self):
        """
        å®Œå…¨å†»ç»“è§†è§‰ç¼–ç å™¨
        """
        frozen_params = 0
        total_params = 0

        # è·å–è§†è§‰ç¼–ç å™¨
        vision_model = self._get_vision_model()
        if vision_model is not None:
            # å†»ç»“æ•´ä¸ªè§†è§‰ç¼–ç å™¨
            for param in vision_model.parameters():
                param.requires_grad = False
                frozen_params += param.numel()

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            vision_model.eval()
            print(f"âœ… å·²å†»ç»“è§†è§‰ç¼–ç å™¨: {frozen_params:,} å‚æ•°")

        # å†»ç»“ VLM çš„å…¶ä»–è§†è§‰ç›¸å…³éƒ¨åˆ†
        if hasattr(self.model, 'vlm_with_expert'):
            vlm_model = self.model.vlm_with_expert
            if hasattr(vlm_model, 'vision_encoder'):
                for param in vlm_model.vision_encoder.parameters():
                    param.requires_grad = False
                    frozen_params += param.numel()
                vlm_model.vision_encoder.eval()

        # ç»Ÿè®¡æ€»å‚æ•°
        for param in self.parameters():
            total_params += param.numel()

        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   - æ€»å‚æ•°: {total_params:,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   - å†»ç»“å‚æ•°: {total_params - trainable_params:,}")
        print(f"   - è®­ç»ƒæ¯”ä¾‹: {trainable_params / total_params * 100:.2f}%")

    def _get_vision_model(self):
        """è·å–è§†è§‰æ¨¡å‹"""
        # å°è¯•å¤šç§è·¯å¾„è·å–è§†è§‰æ¨¡å‹
        if hasattr(self.model, 'vlm_with_expert'):
            vlm_with_expert = self.model.vlm_with_expert
            if hasattr(vlm_with_expert, 'get_vlm_model'):
                vlm_model = vlm_with_expert.get_vlm_model()
                if hasattr(vlm_model, 'vision_model'):
                    return vlm_model.vision_model
        return None

    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        print("\n" + "="*70)
        print("ğŸš€ SmolVLA Diffusion Policy Initialized")
        print("="*70)
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   - VLM Model: {self.config.vlm_model_name}")
        print(f"   - åŠ¨ä½œç”Ÿæˆ: Diffusion (é Flow Matching)")
        print(f"   - è§†è§‰ç¼–ç å™¨: å®Œå…¨å†»ç»“")
        print(f"   - æ¨ç†æ­¥æ•°: {self.config.num_inference_steps}")
        print(f"   - å™ªå£°è°ƒåº¦: {self.config.noise_schedule}")
        print(f"   - é¢„æµ‹ç±»å‹: {self.config.prediction_type}")
        print(f"   - ä½¿ç”¨ DDIM: {self.config.use_ddim_sampling}")
        print("="*70 + "\n")

    def forward(
        self,
        batch: Dict[str, torch.Tensor],
        noise: Optional[torch.Tensor] = None,
        timestep: Optional[torch.Tensor] = None,
    ) -> tuple[torch.Tensor, Dict[str, Any]]:
        """
        Diffusion è®­ç»ƒå‰å‘ä¼ æ’­

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡ï¼Œå¿…é¡»åŒ…å« 'task' å­—æ®µ
            noise: å¯é€‰çš„å™ªå£°ï¼ˆå¦‚æœä¸æä¾›åˆ™éšæœºç”Ÿæˆï¼‰
            timestep: å¯é€‰çš„æ—¶é—´æ­¥ï¼ˆå¦‚æœä¸æä¾›åˆ™éšæœºé‡‡æ ·ï¼‰

        Returns:
            loss: æŸå¤±å€¼
            info: ä¿¡æ¯å­—å…¸
        """
        # ç¡®ä¿æ‰¹æ¬¡åŒ…å«ä»»åŠ¡ä¿¡æ¯
        if 'task' not in batch:
            raise ValueError("æ‰¹æ¬¡å¿…é¡»åŒ…å« 'task' å­—æ®µ")

        batch_size = next(iter(batch.values())).shape[0]

        # å‡†å¤‡è¾“å…¥
        images, img_masks = self.prepare_images(batch)
        state = self.prepare_state(batch)
        lang_tokens, lang_masks = self.prepare_language(batch)

        # è·å–åŠ¨ä½œåºåˆ—
        from lerobot.constants import ACTION
        if ACTION not in batch:
            raise ValueError(f"æ‰¹æ¬¡å¿…é¡»åŒ…å« '{ACTION}' é”®")

        actions = batch[ACTION]  # [B, chunk_size, action_dim]

        # éšæœºé‡‡æ ·æ—¶é—´æ­¥
        if timestep is None:
            timestep = torch.randint(
                0, self.config.num_train_timesteps, (batch_size,),
                device=actions.device
            )

        # ç”Ÿæˆå™ªå£°
        if noise is None:
            noise = torch.randn_like(actions)

        # æ·»åŠ å™ªå£°åˆ°åŠ¨ä½œ
        noisy_actions = self.scheduler.add_noise(
            original_samples=actions,
            noise=noise,
            timesteps=timestep
        )

        # å‡†å¤‡æ—¶é—´åµŒå…¥
        time_embeddings = self._get_time_embeddings(timestep)

        # æ¨¡å‹é¢„æµ‹å™ªå£°
        predicted_noise = self.model.forward(
            images=images,
            image_masks=img_masks,
            language_tokens=lang_tokens,
            language_mask=lang_masks,
            state=state,
            actions=noisy_actions,
            time_embeddings=time_embeddings,
        )

        # è®¡ç®—æŸå¤±
        if self.config.prediction_type == "epsilon":
            # é¢„æµ‹å™ªå£°
            target = noise
        elif self.config.prediction_type == "v_prediction":
            # é¢„æµ‹ v-parameterization
            target = self.scheduler.get_velocity(actions, noise, timestep)
        else:
            # é¢„æµ‹åŸå§‹æ ·æœ¬
            target = actions

        loss = nn.functional.mse_loss(predicted_noise, target)

        # æ”¶é›†ä¿¡æ¯
        info = {
            "loss": loss.item(),
            "timestep_mean": timestep.float().mean().item(),
            "noise_mean": noise.mean().item(),
            "predicted_noise_mean": predicted_noise.mean().item(),
        }

        return loss, info

    def _get_time_embeddings(self, timesteps: torch.Tensor) -> torch.Tensor:
        """
        è·å–æ—¶é—´åµŒå…¥

        Args:
            timesteps: æ—¶é—´æ­¥å¼ é‡

        Returns:
            æ—¶é—´åµŒå…¥
        """
        # ç®€å•çš„æ­£å¼¦ä½ç½®ç¼–ç 
        half_dim = self.config.max_action_dim // 4
        embeddings = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim) * -embeddings)
        embeddings = timesteps[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)

        # æ‰©å±•åˆ°åŒ¹é…åŠ¨ä½œç»´åº¦
        embeddings = embeddings.unsqueeze(1).expand(-1, self.config.chunk_size, -1)

        return embeddings

    def select_action(
        self,
        batch: Dict[str, torch.Tensor],
        noise: Optional[torch.Tensor] = None,
        num_inference_steps: Optional[int] = None,
    ) -> torch.Tensor:
        """
        Diffusion æ¨ç†ï¼šç”ŸæˆåŠ¨ä½œåºåˆ—

        Args:
            batch: è§‚æµ‹æ‰¹æ¬¡
            noise: å¯é€‰çš„åˆå§‹å™ªå£°
            num_inference_steps: å¯é€‰çš„æ¨ç†æ­¥æ•°

        Returns:
            ç”Ÿæˆçš„åŠ¨ä½œåºåˆ— [B, chunk_size, action_dim]
        """
        # ç¡®ä¿æ‰¹æ¬¡åŒ…å«ä»»åŠ¡ä¿¡æ¯
        if 'task' not in batch:
            raise ValueError("æ‰¹æ¬¡å¿…é¡»åŒ…å« 'task' å­—æ®µ")

        batch_size = next(iter(batch.values())).shape[0]

        # è®¾ç½®æ¨ç†æ­¥æ•°
        if num_inference_steps is None:
            num_inference_steps = self.config.num_inference_steps

        # è®¾ç½®è°ƒåº¦å™¨æ—¶é—´æ­¥
        if hasattr(self.scheduler, 'set_timesteps'):
            self.scheduler.set_timesteps(num_inference_steps, device=batch[next(iter(batch.keys()))].device)

        # å‡†å¤‡è¾“å…¥
        images, img_masks = self.prepare_images(batch)
        state = self.prepare_state(batch)
        lang_tokens, lang_masks = self.prepare_language(batch)

        # ä»çº¯å™ªå£°å¼€å§‹
        if noise is None:
            noise = torch.randn(
                batch_size,
                self.config.chunk_size,
                self.config.max_action_dim,
                device=images.device,
                dtype=images.dtype
            )

        # é€æ­¥å»å™ª
        actions = noise
        for i, t in enumerate(self.scheduler.timesteps):
            # è·å–æ—¶é—´åµŒå…¥
            time_embeddings = self._get_time_embeddings(t.unsqueeze(0).expand(batch_size))

            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                predicted_noise = self.model.forward(
                    images=images,
                    image_masks=img_masks,
                    language_tokens=lang_tokens,
                    language_mask=lang_masks,
                    state=state,
                    actions=actions,
                    time_embeddings=time_embeddings,
                )

            # è°ƒåº¦å™¨æ­¥éª¤
            actions = self.scheduler.step(
                model_output=predicted_noise,
                timestep=t,
                sample=actions,
                eta=self.config.ddim_eta,
            )

        # è£å‰ªåŠ¨ä½œåˆ°åŸå§‹ç»´åº¦
        original_action_dim = self.config.action_feature.shape[0]
        actions = actions[:, :, :original_action_dim]

        return actions

    def _get_action_chunk(self, batch: dict[str, torch.Tensor], noise: torch.Tensor | None = None) -> torch.Tensor:
        """
        é‡å†™çˆ¶ç±»æ–¹æ³•ä»¥æ”¯æŒ Diffusion é‡‡æ ·

        ä½¿ç”¨ Diffusion é€æ­¥å»å™ªç”ŸæˆåŠ¨ä½œåºåˆ—
        """
        from lerobot.constants import ACTION

        # å¤åˆ¶é˜Ÿåˆ—
        for k in batch:
            if k in self._queues and k != ACTION:
                batch[k] = torch.stack(list(self._queues[k]), dim=1)

        # ä½¿ç”¨ Diffusion é‡‡æ ·
        actions = self.select_action(batch, noise=noise)

        # åå½’ä¸€åŒ–
        actions = self.unnormalize_outputs({ACTION: actions})[ACTION]

        # è£å‰ªåˆ°åŸå§‹ç»´åº¦
        original_action_dim = self.config.action_feature.shape[0]
        actions = actions[:, :, :original_action_dim]

        if self.config.adapt_to_pi_aloha:
            actions = self._pi_aloha_encode_actions(actions)

        return actions

    @staticmethod
    def _create_identity_stats(config: SmolVLADiffusionConfigWrapper) -> Dict[str, Dict[str, torch.Tensor]]:
        """
        åˆ›å»ºç©ºçš„ dataset_stats
        """
        stats = {}

        # å¤„ç†è¾“å…¥ç‰¹å¾
        for key, feature in config.input_features.items():
            shape = feature.shape
            if 'state' in key.lower():
                shape = (config.max_state_dim,)

            stats[key] = {
                'mean': torch.zeros(shape, dtype=torch.float32),
                'std': torch.ones(shape, dtype=torch.float32),
                'min': torch.zeros(shape, dtype=torch.float32),
                'max': torch.ones(shape, dtype=torch.float32),
            }

        # å¤„ç†è¾“å‡ºç‰¹å¾
        for key, feature in config.output_features.items():
            shape = feature.shape
            if 'action' in key.lower():
                shape = (config.max_action_dim,)

            stats[key] = {
                'mean': torch.zeros(shape, dtype=torch.float32),
                'std': torch.ones(shape, dtype=torch.float32),
                'min': torch.zeros(shape, dtype=torch.float32),
                'max': torch.ones(shape, dtype=torch.float32),
            }

        return stats

    @classmethod
    def from_pretrained(
        cls,
        pretrained_name_or_path: str,
        config: Optional[SmolVLADiffusionConfigWrapper] = None,
        dataset_stats: Optional[Dict[str, Dict[str, torch.Tensor]]] = None,
        **kwargs
    ):
        """
        ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

        æ”¯æŒä» SmolVLA é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¹¶è½¬æ¢ä¸º Diffusion ç‰ˆæœ¬
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“‚ Loading SmolVLA Diffusion from: {pretrained_name_or_path}")
        print(f"{'='*70}")

        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä»é¢„è®­ç»ƒè·¯å¾„åŠ è½½
        if config is None:
            config = SmolVLADiffusionConfigWrapper.from_pretrained(pretrained_name_or_path)

        # å¦‚æœæ²¡æœ‰æä¾› dataset_statsï¼Œåˆ›å»ºä¸´æ—¶çš„
        if dataset_stats is None:
            dataset_stats = cls._create_identity_stats(config)

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = cls(config, dataset_stats)

        # åŠ è½½æƒé‡
        pretrained_path = Path(pretrained_name_or_path)
        if pretrained_path.exists():
            model_file = pretrained_path / "model.safetensors"
            if model_file.exists():
                # åŠ è½½å®Œæ•´çš„çŠ¶æ€å­—å…¸
                from safetensors.torch import load_file
                full_state_dict = load_file(str(model_file))

                # åˆ†ç¦»å½’ä¸€åŒ–å‚æ•°å’Œæ¨¡å‹å‚æ•°
                norm_keys = ("normalize_inputs", "normalize_targets", "unnormalize_outputs")
                norm_state_dict = {
                    k: v for k, v in full_state_dict.items() if k.startswith(norm_keys)
                }
                model_state_dict = {
                    k: v for k, v in full_state_dict.items() if not k.startswith(norm_keys)
                }

                # åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆå…è®¸éƒ¨åˆ†åŠ è½½ï¼‰
                missing, unexpected = model.load_state_dict(model_state_dict, strict=False)
                print(f"âœ… ä»æœ¬åœ° checkpoint åŠ è½½æƒé‡")
                if missing:
                    print(f"   - ç¼ºå¤±çš„é”®ï¼ˆå¯èƒ½æ˜¯ Diffusion ç‰¹å®šçš„ï¼‰: {len(missing)}")
                if unexpected:
                    print(f"   - æ„å¤–çš„é”®: {len(unexpected)}")

                # åŠ è½½å½’ä¸€åŒ–å‚æ•°
                if norm_state_dict:
                    cls._load_normalization_params(model, norm_state_dict)
                    print(f"âœ… åŠ è½½å½’ä¸€åŒ–å‚æ•°")
            else:
                print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_file}")
        else:
            # å°è¯•ä» HuggingFace åŠ è½½
            try:
                from huggingface_hub import hf_hub_download
                model_file = hf_hub_download(
                    repo_id=pretrained_name_or_path,
                    filename="model.safetensors"
                )
                # åŠ è½½é€»è¾‘...
                print(f"âœ… ä» HuggingFace åŠ è½½: {pretrained_name_or_path}")
            except Exception as e:
                print(f"âš ï¸ ä» HuggingFace åŠ è½½å¤±è´¥: {e}")
                print(f"ä½¿ç”¨éšæœºåˆå§‹åŒ–")

        print(f"{'='*70}\n")

        return model

    @staticmethod
    def _load_normalization_params(model, norm_state_dict):
        """åŠ è½½å½’ä¸€åŒ–å‚æ•°"""
        import torch.nn as nn

        for key, value in norm_state_dict.items():
            parts = key.split('.')
            obj = model
            for part in parts[:-1]:
                obj = getattr(obj, part)
            param_name = parts[-1]

            if isinstance(obj, nn.ParameterDict):
                obj[param_name] = nn.Parameter(value, requires_grad=False)
            elif hasattr(obj, param_name) and hasattr(getattr(obj, param_name), 'data'):
                getattr(obj, param_name).data = value


class DiffusionUNetHead(nn.Module):
    """
    ç”¨äºæ›¿æ¢ Flow Matching çš„ Diffusion UNet å¤´
    """

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        hidden_dim: int = 1024,
        num_layers: int = 6,
        time_embedding_dim: int = 128,
    ):
        super().__init__()

        # æ—¶é—´åµŒå…¥æŠ•å½±
        self.time_proj = nn.Sequential(
            nn.Linear(time_embedding_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # UNet é£æ ¼çš„å±‚
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(
                nn.Sequential(
                    nn.Linear(hidden_dim * 3, hidden_dim),  # è¾“å…¥ + æ—¶é—´ + å‰ä¸€å±‚
                    nn.SiLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.SiLU(),
                )
            )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, output_dim),
        )

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

    def forward(self, x, time_embeddings):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥ç‰¹å¾ [B, seq_len, input_dim]
            time_embeddings: æ—¶é—´åµŒå…¥ [B, seq_len, time_dim]

        Returns:
            é¢„æµ‹çš„å™ªå£°æˆ–æ ·æœ¬ [B, seq_len, output_dim]
        """
        # æŠ•å½±è¾“å…¥
        h = self.input_proj(x)  # [B, seq_len, hidden_dim]

        # æŠ•å½±æ—¶é—´åµŒå…¥
        t = self.time_proj(time_embeddings)  # [B, seq_len, hidden_dim]

        # é€šè¿‡ UNet å±‚
        for i, layer in enumerate(self.layers):
            # æ‹¼æ¥è¾“å…¥ã€æ—¶é—´å’Œå‰ä¸€å±‚çš„è¾“å‡º
            if i == 0:
                layer_input = torch.cat([h, t, h], dim=-1)
            else:
                layer_input = torch.cat([h, t, h], dim=-1)
            h = layer(layer_input)

        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(h)

        return output