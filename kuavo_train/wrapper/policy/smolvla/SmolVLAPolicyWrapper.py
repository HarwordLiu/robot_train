"""
SmolVLA Policy Wrapper for Kuavo Project

SmolVLAçš„Kuavoé¡¹ç›®åŒ…è£…å™¨ï¼Œç»§æ‰¿lerobotçš„SmolVLAPolicyï¼Œ
æ·»åŠ Kuavoç‰¹å®šçš„åŠŸèƒ½å’Œå…¼å®¹æ€§å¤„ç†ã€‚
"""

import torch
from typing import Dict, Any, Optional
from pathlib import Path

from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig


class SmolVLAPolicyWrapper(SmolVLAPolicy):
    """
    Kuavoé¡¹ç›®çš„SmolVLAç­–ç•¥åŒ…è£…å™¨

    ç›´æ¥ç»§æ‰¿lerobotçš„SmolVLAPolicyï¼Œæ·»åŠ ï¼š
    1. Kuavoé¡¹ç›®çš„åˆå§‹åŒ–æ—¥å¿—
    2. å…¼å®¹Kuavoæ•°æ®æ ¼å¼
    3. æ”¯æŒå¤šä»»åŠ¡é¡ºåºè®­ç»ƒ

    Usage:
        # è®­ç»ƒæ¨¡å¼
        policy = SmolVLAPolicyWrapper(config, dataset_stats)
        loss, info = policy.forward(batch)

        # æ¨ç†æ¨¡å¼
        action = policy.select_action(batch)
    """

    def __init__(
        self,
        config: SmolVLAConfig,
        dataset_stats: Optional[Dict[str, Dict[str, torch.Tensor]]] = None,
    ):
        """
        åˆå§‹åŒ–SmolVLAç­–ç•¥

        Args:
            config: SmolVLAConfigé…ç½®å¯¹è±¡
            dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»SmolVLAPolicyçš„åˆå§‹åŒ–
        super().__init__(config, dataset_stats)

        # ğŸ†• åº”ç”¨çµæ´»çš„è§†è§‰å±‚å†»ç»“ç­–ç•¥
        self._apply_flexible_vision_freezing()

        # Kuavoé¡¹ç›®ç‰¹å®šæ—¥å¿—
        print("\n" + "="*70)
        print("ğŸ¤– SmolVLA Policy Initialized for Kuavo Project")
        print("="*70)
        print(f"VLM Backbone: {config.vlm_model_name}")
        print(f"Action Dimension: {config.max_action_dim} (Kuavo Dual-Arm)")
        print(f"Chunk Size: {config.chunk_size}")
        print(f"Action Steps per Inference: {config.n_action_steps}")
        print(f"Freeze Vision Encoder: {config.freeze_vision_encoder}")
        print(f"Train Expert Only: {config.train_expert_only}")

        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel()
                               for p in self.parameters() if p.requires_grad)
        print(f"\nModel Parameters:")
        print(f"  Total: {total_params:,}")
        print(f"  Trainable: {trainable_params:,}")
        print(f"  Frozen: {total_params - trainable_params:,}")
        print("="*70 + "\n")

    def prepare_batch_with_language(
        self,
        batch: Dict[str, torch.Tensor],
        language_instruction: str
    ) -> Dict[str, torch.Tensor]:
        """
        ä¸ºbatchæ·»åŠ language instruction

        SmolVLAéœ€è¦language instructionä½œä¸ºä»»åŠ¡æ¡ä»¶ï¼Œ
        è¿™ä¸ªæ–¹æ³•ç¡®ä¿æ¯ä¸ªbatchéƒ½åŒ…å«æ­£ç¡®çš„language field

        Args:
            batch: è¾“å…¥batch
            language_instruction: ä»»åŠ¡çš„language instruction

        Returns:
            åŒ…å«languageå­—æ®µçš„batch
        """
        if 'task' not in batch:
            # ä¸ºbatchä¸­çš„æ¯ä¸ªæ ·æœ¬æ·»åŠ ç›¸åŒçš„language instruction
            batch_size = next(iter(batch.values())).shape[0]
            batch['task'] = [language_instruction] * batch_size

        return batch

    def forward(
        self,
        batch: Dict[str, torch.Tensor],
        noise: Optional[torch.Tensor] = None,
        time: Optional[torch.Tensor] = None
    ) -> tuple[torch.Tensor, Dict[str, Any]]:
        """
        è®­ç»ƒforward

        Args:
            batch: è¾“å…¥batchï¼Œå¿…é¡»åŒ…å«'task'å­—æ®µ
            noise: å¯é€‰çš„å™ªå£°ï¼ˆFlow Matchingä½¿ç”¨ï¼‰
            time: å¯é€‰çš„æ—¶é—´æ­¥ï¼ˆFlow Matchingä½¿ç”¨ï¼‰

        Returns:
            loss: æ ‡é‡tensor
            info: ä¿¡æ¯å­—å…¸
        """
        # ç¡®ä¿batchåŒ…å«taskå­—æ®µ
        if 'task' not in batch:
            raise ValueError(
                "Batch must contain 'task' field for SmolVLA. "
                "Use prepare_batch_with_language() to add language instruction."
            )

        # è°ƒç”¨çˆ¶ç±»forward
        return super().forward(batch, noise, time)

    def select_action(
        self,
        batch: Dict[str, torch.Tensor],
        noise: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        æ¨ç†forwardï¼šç”ŸæˆåŠ¨ä½œ

        Args:
            batch: è§‚æµ‹batchï¼Œå¿…é¡»åŒ…å«'task'å­—æ®µ
            noise: å¯é€‰çš„å™ªå£°ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        Returns:
            action: [B, action_dim] å•æ­¥åŠ¨ä½œ
        """
        # ç¡®ä¿batchåŒ…å«taskå­—æ®µ
        if 'task' not in batch:
            raise ValueError(
                "Batch must contain 'task' field for SmolVLA inference. "
                "Provide language instruction to specify which task to execute."
            )

        # è°ƒç”¨çˆ¶ç±»select_action
        return super().select_action(batch, noise)

    def _get_action_chunk(self, batch: dict[str, torch.Tensor], noise: torch.Tensor | None = None) -> torch.Tensor:
        """
        é‡å†™çˆ¶ç±»æ–¹æ³•ä»¥ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜

        æ­£ç¡®çš„é¡ºåºï¼š
        1. æ¨¡å‹é¢„æµ‹ï¼ˆè¾“å‡º32Då½’ä¸€åŒ–çš„åŠ¨ä½œï¼‰
        2. ç”¨32Då‚æ•°åå½’ä¸€åŒ–
        3. è£å‰ªåˆ°16Dï¼ˆKuavoå®é™…ç»´åº¦ï¼‰

        çˆ¶ç±»çš„å®ç°é¡ºåºé”™è¯¯ï¼ˆå…ˆè£å‰ªå†åå½’ä¸€åŒ–ï¼‰ï¼Œå¯¼è‡´ç»´åº¦ä¸åŒ¹é…ã€‚
        """
        from lerobot.constants import ACTION

        # Copy queues so that we don't modify the originals
        for k in batch:
            if k in self._queues and k != ACTION:
                batch[k] = torch.stack(list(self._queues[k]), dim=1)

        # å‡†å¤‡è¾“å…¥
        images, img_masks = self.prepare_images(batch)
        state = self.prepare_state(batch)
        lang_tokens, lang_masks = self.prepare_language(batch)

        # æ¨¡å‹é‡‡æ ·ï¼ˆè¾“å‡º32Då½’ä¸€åŒ–çš„åŠ¨ä½œï¼‰
        actions = self.model.sample_actions(
            images, img_masks, lang_tokens, lang_masks, state, noise=noise)

        # å…ˆåœ¨32Dç©ºé—´åå½’ä¸€åŒ–ï¼ˆä½¿ç”¨32Dçš„mean/stdï¼‰
        actions = self.unnormalize_outputs({ACTION: actions})[ACTION]

        # ç„¶åè£å‰ªåˆ°åŸå§‹ç»´åº¦ï¼ˆ16Dï¼‰
        original_action_dim = self.config.action_feature.shape[0]
        actions = actions[:, :, :original_action_dim]

        if self.config.adapt_to_pi_aloha:
            actions = self._pi_aloha_encode_actions(actions)

        return actions

    @staticmethod
    def _create_identity_stats(config: SmolVLAConfig) -> Dict[str, Dict[str, torch.Tensor]]:
        """
        åˆ›å»º"ç©º"çš„dataset_statsï¼Œä½¿å½’ä¸€åŒ–æˆä¸ºæ’ç­‰å˜æ¢

        å¯¹äºæ¯ä¸ªfeatureï¼š
        - mean = 0ï¼ˆå‡å»0ä¸æ”¹å˜æ•°æ®ï¼‰
        - std = 1ï¼ˆé™¤ä»¥1ä¸æ”¹å˜æ•°æ®ï¼‰

        æ³¨æ„ï¼šå¯¹äº state å’Œ actionï¼Œä½¿ç”¨ max_state_dim å’Œ max_action_dimï¼ˆ32ç»´ï¼‰
        è€Œä¸æ˜¯å®é™…çš„ç»´åº¦ï¼ˆ16ç»´ï¼‰ï¼Œä»¥åŒ¹é…è®­ç»ƒæ—¶çš„å¡«å……ç»´åº¦ã€‚

        å¯¹äºæ·±åº¦ç‰¹å¾çš„shapeä¸åŒ¹é…é—®é¢˜ï¼Œä¼šåœ¨åŠ è½½checkpointæ—¶é€šè¿‡broadcastingè‡ªåŠ¨è§£å†³ã€‚

        Args:
            config: SmolVLAé…ç½®å¯¹è±¡

        Returns:
            åŒ…å«æ‰€æœ‰featuresçš„identity statså­—å…¸
        """
        stats = {}

        # å¤„ç†input featuresï¼ˆobservationsï¼‰
        for key, feature in config.input_features.items():
            shape = feature.shape

            # å¯¹äºstateï¼Œä½¿ç”¨max_state_dimè€Œä¸æ˜¯å®é™…ç»´åº¦
            if 'state' in key.lower():
                shape = (config.max_state_dim,)

            stats[key] = {
                'mean': torch.zeros(shape, dtype=torch.float32),
                'std': torch.ones(shape, dtype=torch.float32),
                'min': torch.zeros(shape, dtype=torch.float32),
                'max': torch.ones(shape, dtype=torch.float32),
            }

        # å¤„ç†output featuresï¼ˆactionsï¼‰
        for key, feature in config.output_features.items():
            shape = feature.shape

            # å¯¹äºactionï¼Œä½¿ç”¨max_action_dimè€Œä¸æ˜¯å®é™…ç»´åº¦
            if 'action' in key.lower():
                shape = (config.max_action_dim,)

            stats[key] = {
                'mean': torch.zeros(shape, dtype=torch.float32),
                'std': torch.ones(shape, dtype=torch.float32),
                'min': torch.zeros(shape, dtype=torch.float32),
                'max': torch.ones(shape, dtype=torch.float32),
            }

        return stats

    @classmethod
    def from_pretrained(
        cls,
        pretrained_name_or_path: str,
        config: Optional[SmolVLAConfig] = None,
        dataset_stats: Optional[Dict[str, Dict[str, torch.Tensor]]] = None,
        apply_freezing: bool = False,  # ğŸ†• é»˜è®¤ä¸åº”ç”¨å†»ç»“ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        **kwargs
    ):
        """
        ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

        Args:
            pretrained_name_or_path:
                - HuggingFaceæ¨¡å‹IDï¼ˆå¦‚'lerobot/smolvla_base'ï¼‰
                - æœ¬åœ°è·¯å¾„ï¼ˆå¦‚'outputs/train/.../best'ï¼‰
            config: å¯é€‰çš„é…ç½®å¯¹è±¡ï¼ˆå¦‚æœæä¾›ï¼Œä¼šoverrideé¢„è®­ç»ƒé…ç½®ï¼‰
            dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            apply_freezing: æ˜¯å¦åº”ç”¨è§†è§‰å±‚å†»ç»“ç­–ç•¥
                - True: åº”ç”¨å†»ç»“ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
                - False: ä¸åº”ç”¨å†»ç»“ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼Œé»˜è®¤ï¼‰

        Returns:
            åŠ è½½çš„SmolVLAPolicyWrapperå®ä¾‹
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“‚ Loading SmolVLA from: {pretrained_name_or_path}")
        print(f"{'='*70}")

        # å¦‚æœæ²¡æœ‰æä¾›configï¼Œä»é¢„è®­ç»ƒè·¯å¾„åŠ è½½
        if config is None:
            from .SmolVLAConfigWrapper import SmolVLAConfigWrapper
            config = SmolVLAConfigWrapper.from_pretrained(
                pretrained_name_or_path)

        # å¦‚æœæ²¡æœ‰æä¾›dataset_statsï¼Œåˆ›å»ºä¸´æ—¶çš„identity statsç”¨äºåˆå§‹åŒ–
        # çœŸå®çš„å½’ä¸€åŒ–å‚æ•°ä¼šä»checkpointä¸­åŠ è½½
        if dataset_stats is None:
            print(
                "âš ï¸  No dataset_stats provided. Will load normalization params from checkpoint.")
            dataset_stats = cls._create_identity_stats(config)

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = cls(config, dataset_stats)

        # åŠ è½½æƒé‡
        pretrained_path = Path(pretrained_name_or_path)
        if pretrained_path.exists():
            # æœ¬åœ°checkpoint
            model_file = pretrained_path / "model.safetensors"
            if model_file.exists():
                # åŠ è½½å®Œæ•´çš„ state_dictï¼ˆåŒ…æ‹¬å½’ä¸€åŒ–å‚æ•°ï¼‰
                from safetensors.torch import load_file
                full_state_dict = load_file(str(model_file))

                # åˆ†ç¦»å½’ä¸€åŒ–å‚æ•°å’Œæ¨¡å‹å‚æ•°
                norm_keys = ("normalize_inputs",
                             "normalize_targets", "unnormalize_outputs")
                norm_state_dict = {
                    k: v for k, v in full_state_dict.items() if k.startswith(norm_keys)}
                model_state_dict = {
                    k: v for k, v in full_state_dict.items() if not k.startswith(norm_keys)}

                # å…ˆåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆä¸åŒ…æ‹¬å½’ä¸€åŒ–ï¼‰
                missing, unexpected = model.load_state_dict(
                    model_state_dict, strict=False)
                print(f"âœ… Loaded model weights from local checkpoint")

                # å†åŠ è½½å½’ä¸€åŒ–å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if norm_state_dict:
                    # ä¿®å¤æ·±åº¦ç‰¹å¾å½’ä¸€åŒ–å‚æ•°çš„shapeä¸åŒ¹é…é—®é¢˜
                    # checkpointä¸­æ·±åº¦ç‰¹å¾çš„å½’ä¸€åŒ–å‚æ•°æ˜¯(1,1,1)ï¼Œä½†æ¨¡å‹åˆå§‹åŒ–æ—¶åˆ›å»ºçš„æ˜¯(1,480,640)
                    # æˆ‘ä»¬éœ€è¦ä¿æŒ(1,1,1)ä»¥ä¾¿åœ¨forwardæ—¶è‡ªåŠ¨broadcaståˆ°ä»»æ„åˆ†è¾¨ç‡

                    import torch.nn as nn

                    # ç›´æ¥è®¿é—®å¹¶æ›¿æ¢å½’ä¸€åŒ–æ¨¡å—ä¸­çš„å‚æ•°
                    for key, value in norm_state_dict.items():
                        # é€šè¿‡åç§°è®¿é—®åµŒå¥—çš„å‚æ•°
                        # ä¾‹å¦‚: normalize_inputs.buffer_observation_depth_h.mean
                        parts = key.split('.')
                        obj = model

                        # å¯¼èˆªåˆ°ç›®æ ‡å¯¹è±¡ï¼ˆä¾‹å¦‚ParameterDictï¼‰
                        for part in parts[:-1]:
                            obj = getattr(obj, part)

                        # è·å–æœ€åä¸€ä¸ªå±æ€§åï¼ˆä¾‹å¦‚'mean'ï¼‰
                        param_name = parts[-1]

                        # å¦‚æœæ˜¯ParameterDictï¼Œç›´æ¥æ›¿æ¢å…¶ä¸­çš„Parameter
                        if isinstance(obj, nn.ParameterDict):
                            current_param = obj[param_name]
                            checkpoint_shape = value.shape
                            current_shape = current_param.shape

                            if checkpoint_shape != current_shape:
                                print(
                                    f"ğŸ”§ Keeping compact shape for {key}: {checkpoint_shape} (model had {current_shape})")

                            # åˆ›å»ºæ–°çš„Parameterå¯¹è±¡ï¼Œä¿æŒcheckpointçš„shape
                            obj[param_name] = nn.Parameter(
                                value, requires_grad=False)
                        else:
                            # å…¶ä»–æƒ…å†µï¼Œå°è¯•ç›´æ¥èµ‹å€¼
                            if hasattr(obj, param_name):
                                current_param = getattr(obj, param_name)
                                if hasattr(current_param, 'data'):
                                    current_param.data = value

                    print(f"âœ… Loaded normalization parameters from checkpoint")
                    print(
                        f"   - {len([k for k in norm_state_dict.keys() if 'normalize_inputs' in k])} input norm params")
                    print(
                        f"   - {len([k for k in norm_state_dict.keys() if 'normalize_targets' in k])} target norm params")
                    print(
                        f"   - {len([k for k in norm_state_dict.keys() if 'unnormalize_outputs' in k])} unnorm params")
                else:
                    print(
                        f"âš ï¸  No normalization parameters found in checkpoint. Using identity normalization.")
            else:
                print(f"âš ï¸  Model file not found: {model_file}")
        else:
            # HuggingFaceæ¨¡å‹
            try:
                from huggingface_hub import hf_hub_download
                model_file = hf_hub_download(
                    repo_id=pretrained_name_or_path,
                    filename="model.safetensors"
                )
                from lerobot.policies.smolvla.modeling_smolvla import load_smolvla
                model = load_smolvla(
                    model,
                    model_file,
                    device='cpu',
                    checkpoint_keys_mapping="model._orig_mod.//model."
                )
                print(
                    f"âœ… Loaded weights from HuggingFace: {pretrained_name_or_path}")
            except Exception as e:
                print(f"âš ï¸  Failed to load from HuggingFace: {e}")
                print(f"Using random initialization")

        print(f"{'='*70}\n")

        # ğŸ†• åœ¨åŠ è½½æƒé‡åé‡æ–°åº”ç”¨çµæ´»å†»ç»“ç­–ç•¥ï¼ˆä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
        # å› ä¸ºæœ‰äº›å±‚å¯èƒ½åœ¨æƒé‡åŠ è½½åæ‰å®Œå…¨åˆå§‹åŒ–
        if apply_freezing and (config.unfreeze_vision_layers is not None or
                               config.freeze_vision_layers is not None or
                               config.freeze_vision_ratio is not None):
            print("\nğŸ”§ é‡æ–°åº”ç”¨çµæ´»è§†è§‰å±‚å†»ç»“ç­–ç•¥ï¼ˆåœ¨æƒé‡åŠ è½½åï¼‰...")
            model._apply_flexible_vision_freezing()
        elif not apply_freezing:
            print("\nğŸ’¡ æ¨ç†æ¨¡å¼ï¼šè·³è¿‡è§†è§‰å±‚å†»ç»“ç­–ç•¥åº”ç”¨ï¼ˆæ‰€æœ‰å±‚æ­£å¸¸å·¥ä½œï¼‰")

        return model

    def save_pretrained(self, save_directory: Path) -> None:
        """
        ä¿å­˜æ¨¡å‹

        Args:
            save_directory: ä¿å­˜ç›®å½•è·¯å¾„

        æ³¨æ„ï¼šä¾èµ– SmolVLAConfigWrapper å·²å°†æ‰€æœ‰ OmegaConf å¯¹è±¡è½¬æ¢ä¸ºåŸç”Ÿ Python å¯¹è±¡ï¼Œ
        å› æ­¤å¯ä»¥ç›´æ¥ä½¿ç”¨ lerobot çš„æ ‡å‡†ä¿å­˜æ–¹å¼ã€‚
        """
        save_directory = Path(save_directory)
        save_directory.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ’¾ Saving SmolVLA model to {save_directory}")

        # ä¿å­˜é…ç½®ï¼ˆä½¿ç”¨ lerobot æ ‡å‡†æ–¹å¼ï¼‰
        self.config._save_pretrained(save_directory)

        # ä¿å­˜æ¨¡å‹æƒé‡
        from safetensors.torch import save_file
        model_file = save_directory / "model.safetensors"
        save_file(self.state_dict(), str(model_file))

        print(f"âœ… Model saved successfully")
        print(f"   Config: {save_directory / 'config.json'}")
        print(f"   Weights: {model_file}")

    def _apply_flexible_vision_freezing(self):
        """
        åº”ç”¨çµæ´»çš„è§†è§‰å±‚å†»ç»“ç­–ç•¥

        æ”¯æŒä¸‰ç§é…ç½®æ–¹å¼ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
        1. unfreeze_vision_layers: æŒ‡å®šè¦è§£å†»çš„å±‚ç´¢å¼•ï¼ˆæ”¯æŒè´Ÿæ•°ç´¢å¼•ï¼‰
        2. freeze_vision_layers: æŒ‡å®šè¦å†»ç»“çš„å±‚ç´¢å¼•
        3. freeze_vision_ratio: æŒ‰æ¯”ä¾‹å†»ç»“å‰N%çš„å±‚

        å¦‚æœæ²¡æœ‰é…ç½®ä»¥ä¸Šä»»ä½•é€‰é¡¹ï¼Œä½¿ç”¨é»˜è®¤çš„ freeze_vision_encoder è¡Œä¸ºã€‚
        """
        config = self.config

        # å¦‚æœæ²¡æœ‰é…ç½®çµæ´»å†»ç»“ç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤è¡Œä¸º
        if (config.unfreeze_vision_layers is None and
            config.freeze_vision_layers is None and
                config.freeze_vision_ratio is None):
            return

        # è·å– vision_modelï¼ˆSmolVLMçš„è§†è§‰ç¼–ç å™¨ï¼‰
        # å®Œæ•´è·¯å¾„: self.model.vlm_with_expert.vlm.model.vision_model
        try:
            vision_model = None

            # è·¯å¾„1: é€šè¿‡ model.vlm_with_expert
            if hasattr(self, 'model') and hasattr(self.model, 'vlm_with_expert'):
                vlm_with_expert = self.model.vlm_with_expert
                if hasattr(vlm_with_expert, 'get_vlm_model'):
                    vlm_model = vlm_with_expert.get_vlm_model()
                    if hasattr(vlm_model, 'vision_model'):
                        vision_model = vlm_model.vision_model

            # è·¯å¾„2: ç›´æ¥é€šè¿‡ model çš„ get_vlm_model
            if vision_model is None and hasattr(self, 'model') and hasattr(self.model, 'get_vlm_model'):
                vlm_model = self.model.get_vlm_model()
                if hasattr(vlm_model, 'vision_model'):
                    vision_model = vlm_model.vision_model

            # è·¯å¾„3: å¦‚æœ self æœ¬èº«æ˜¯ VLAFlowMatching
            if vision_model is None and hasattr(self, 'vlm_with_expert'):
                if hasattr(self.vlm_with_expert, 'get_vlm_model'):
                    vlm_model = self.vlm_with_expert.get_vlm_model()
                    if hasattr(vlm_model, 'vision_model'):
                        vision_model = vlm_model.vision_model

            if vision_model is None:
                print("âš ï¸  æ— æ³•æ‰¾åˆ° vision_modelï¼Œè·³è¿‡çµæ´»å†»ç»“ç­–ç•¥")
                print(f"   DEBUG: self ç±»å‹: {type(self).__name__}")
                if hasattr(self, 'model'):
                    print(
                        f"   DEBUG: self.model ç±»å‹: {type(self.model).__name__}")
                    if hasattr(self.model, 'vlm_with_expert'):
                        print(f"   DEBUG: self.model.vlm_with_expert å­˜åœ¨")
                return

        except Exception as e:
            print(f"âš ï¸  è®¿é—® vision_model æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

        # è·å–è§†è§‰ç¼–ç å™¨çš„æ‰€æœ‰å±‚
        vision_layers = vision_model.encoder.layers
        total_layers = len(vision_layers)

        print(f"\n{'='*70}")
        print(f"ğŸ”§ åº”ç”¨çµæ´»è§†è§‰å±‚å†»ç»“ç­–ç•¥")
        print(f"{'='*70}")

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ“Š Vision Model ä¿¡æ¯:")
        print(f"   - Vision Model ç±»å‹: {type(vision_model).__name__}")
        print(f"   - æ˜¯å¦æœ‰ encoder: {hasattr(vision_model, 'encoder')}")
        if hasattr(vision_model, 'config'):
            print(f"   - Config: {type(vision_model.config).__name__}")

        print(f"\nVision Encoder æ€»å±‚æ•°: {total_layers}")

        # ç¡®å®šè¦å†»ç»“/è§£å†»çš„å±‚
        frozen_layers = set()
        unfrozen_layers = set()

        # ä¼˜å…ˆçº§1: unfreeze_vision_layers
        if config.unfreeze_vision_layers is not None:
            print(f"\nç­–ç•¥: è§£å†»æŒ‡å®šå±‚ {config.unfreeze_vision_layers}")

            # é»˜è®¤æ‰€æœ‰å±‚éƒ½å†»ç»“
            frozen_layers = set(range(total_layers))

            # è§£å†»æŒ‡å®šçš„å±‚ï¼ˆæ”¯æŒè´Ÿæ•°ç´¢å¼•ï¼‰
            for idx in config.unfreeze_vision_layers:
                if idx < 0:
                    actual_idx = total_layers + idx
                else:
                    actual_idx = idx

                if 0 <= actual_idx < total_layers:
                    frozen_layers.discard(actual_idx)
                    unfrozen_layers.add(actual_idx)
                else:
                    print(
                        f"âš ï¸  è­¦å‘Š: å±‚ç´¢å¼• {idx} (å®é™…: {actual_idx}) è¶…å‡ºèŒƒå›´ [0, {total_layers-1}]")

        # ä¼˜å…ˆçº§2: freeze_vision_layers
        elif config.freeze_vision_layers is not None:
            print(f"\nç­–ç•¥: å†»ç»“æŒ‡å®šå±‚ {config.freeze_vision_layers}")

            # é»˜è®¤æ‰€æœ‰å±‚éƒ½è§£å†»
            unfrozen_layers = set(range(total_layers))

            # å†»ç»“æŒ‡å®šçš„å±‚
            for idx in config.freeze_vision_layers:
                if 0 <= idx < total_layers:
                    frozen_layers.add(idx)
                    unfrozen_layers.discard(idx)
                else:
                    print(f"âš ï¸  è­¦å‘Š: å±‚ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {total_layers-1}]")

        # ä¼˜å…ˆçº§3: freeze_vision_ratio
        elif config.freeze_vision_ratio is not None:
            ratio = config.freeze_vision_ratio
            if not 0.0 <= ratio <= 1.0:
                print(
                    f"âš ï¸  è­¦å‘Š: freeze_vision_ratio={ratio} ä¸åœ¨ [0.0, 1.0] èŒƒå›´å†…ï¼Œä½¿ç”¨é»˜è®¤è¡Œä¸º")
                return

            num_frozen = int(total_layers * ratio)
            print(f"\nç­–ç•¥: æŒ‰æ¯”ä¾‹å†»ç»“å‰ {ratio:.1%} çš„å±‚ (å‰ {num_frozen} å±‚)")

            frozen_layers = set(range(num_frozen))
            unfrozen_layers = set(range(num_frozen, total_layers))

        # åº”ç”¨å†»ç»“ç­–ç•¥
        for layer_idx in range(total_layers):
            layer = vision_layers[layer_idx]

            if layer_idx in frozen_layers:
                # å†»ç»“å±‚
                layer.eval()
                for param in layer.parameters():
                    param.requires_grad = False
            else:
                # è§£å†»å±‚
                for param in layer.parameters():
                    param.requires_grad = True

        # æ‰“å°ç»“æœæ‘˜è¦
        print(f"\nâœ… å†»ç»“ç­–ç•¥åº”ç”¨å®Œæˆ:")
        print(f"   ğŸ”’ å†»ç»“å±‚æ•°: {len(frozen_layers)} / {total_layers}")
        print(f"   ğŸ”“ è§£å†»å±‚æ•°: {len(unfrozen_layers)} / {total_layers}")

        if frozen_layers:
            frozen_list = sorted(list(frozen_layers))
            if len(frozen_list) <= 10:
                print(f"   ğŸ”’ å†»ç»“å±‚ç´¢å¼•: {frozen_list}")
            else:
                print(f"   ğŸ”’ å†»ç»“å±‚ç´¢å¼•: [{frozen_list[0]}...{frozen_list[-1]}]")

        if unfrozen_layers:
            unfrozen_list = sorted(list(unfrozen_layers))
            if len(unfrozen_list) <= 10:
                print(f"   ğŸ”“ è§£å†»å±‚ç´¢å¼•: {unfrozen_list}")
            else:
                print(
                    f"   ğŸ”“ è§£å†»å±‚ç´¢å¼•: [{unfrozen_list[0]}...{unfrozen_list[-1]}]")

        print(f"{'='*70}\n")
