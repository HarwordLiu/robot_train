"""
SmolVLA Configuration Wrapper for Kuavo Project

æ‰©å±•lerobotçš„SmolVLAConfigä»¥æ”¯æŒKuavoé¡¹ç›®çš„ç‰¹å®šéœ€æ±‚
"""

from dataclasses import dataclass, fields
from pathlib import Path
from copy import deepcopy
from typing import TypeVar, List, Tuple
import torch
from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig
from lerobot.configs.policies import PreTrainedConfig, PolicyFeature

T = TypeVar("T", bound="SmolVLAConfigWrapper")


@PreTrainedConfig.register_subclass("smolvla_kuavo")
@dataclass
class SmolVLAConfigWrapper(SmolVLAConfig):
    """
    Kuavoé¡¹ç›®çš„SmolVLAé…ç½®æ‰©å±•ç±» - æ”¯æŒæ·±åº¦ç›¸æœº

    ç»§æ‰¿SmolVLAConfigçš„æ‰€æœ‰åŠŸèƒ½ï¼Œæ·»åŠ æ·±åº¦ç›¸æœºæ”¯æŒï¼š
    - æ·±åº¦ç›¸æœºé…ç½®
    - å¤šç›¸æœºèåˆå‚æ•°
    - æ·±åº¦å›¾åƒé¢„å¤„ç†è®¾ç½®

    é‡è¦ï¼šè‡ªåŠ¨å°†æ‰€æœ‰ OmegaConf å¯¹è±¡è½¬æ¢ä¸ºåŸç”Ÿ Python å¯¹è±¡ï¼Œ
    ç¡®ä¿å¯ä»¥ä½¿ç”¨ lerobot çš„æ ‡å‡†ä¿å­˜æ–¹å¼ï¼Œæ— éœ€ä¾èµ– omegaconfã€‚
    """

    # æ·±åº¦ç›¸æœºæ”¯æŒ
    use_depth: bool = True
    depth_features: List[str] = None

    # æ·±åº¦å›¾åƒé¢„å¤„ç†
    depth_resize_with_padding: List[int] = None
    depth_normalization_range: List[float] = None
    use_depth_padding: bool = True  # æ·±åº¦å›¾æ˜¯å¦ä½¿ç”¨paddingæ–¹å¼ä¿æŒé•¿å®½æ¯”

    # åˆ†å±‚å­¦ä¹ ç‡æ”¯æŒï¼ˆè§£é”VLMæ—¶ä½¿ç”¨ï¼‰
    use_layerwise_lr: bool = False  # æ˜¯å¦å¯ç”¨åˆ†å±‚å­¦ä¹ ç‡
    vision_encoder_lr: float = None  # è§†è§‰ç¼–ç å™¨å­¦ä¹ ç‡ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨optimizer_lrï¼‰
    expert_lr: float = None  # Action Expertå­¦ä¹ ç‡ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨optimizer_lrï¼‰

    # ğŸ†• çµæ´»çš„è§†è§‰å±‚å†»ç»“é…ç½®
    # æ–¹å¼1: æŒ‡å®šè¦è§£å†»çš„å±‚ç´¢å¼•åˆ—è¡¨ï¼ˆæ¨èï¼‰
    unfreeze_vision_layers: List[int] = None  # ä¾‹å¦‚: [-1, -2, -3] è§£å†»æœ€å3å±‚
    # æ–¹å¼2: æŒ‡å®šè¦å†»ç»“çš„å±‚ç´¢å¼•åˆ—è¡¨
    freeze_vision_layers: List[int] = None  # ä¾‹å¦‚: [0, 1, 2, 3, 4] å†»ç»“å‰5å±‚
    # æ–¹å¼3: ä½¿ç”¨æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰
    freeze_vision_ratio: float = None  # ä¾‹å¦‚: 0.75 è¡¨ç¤ºå†»ç»“å‰75%çš„å±‚

    def __post_init__(self):
        """
        ååˆå§‹åŒ–å¤„ç†

        1. è®¾ç½®é»˜è®¤å€¼
        2. è½¬æ¢ OmegaConf å¯¹è±¡ä¸ºåŸç”Ÿ Python å¯¹è±¡
        3. é‡æ–°å°† input_features å’Œ output_features è½¬æ¢ä¸º PolicyFeature å¯¹è±¡
        4. æ‰§è¡Œçˆ¶ç±»çš„éªŒè¯é€»è¾‘
        5. æ‰§è¡Œ Kuavo ç‰¹å®šçš„é…ç½®éªŒè¯
        """
        # è®¾ç½®é»˜è®¤å€¼
        if self.depth_features is None:
            self.depth_features = [
                "observation.depth_h", "observation.depth_l", "observation.depth_r"
            ]

        if self.depth_resize_with_padding is None:
            self.depth_resize_with_padding = [512, 512]

        if self.depth_normalization_range is None:
            self.depth_normalization_range = [0.0, 1000.0]

        # åˆ†å±‚å­¦ä¹ ç‡é»˜è®¤å€¼
        if self.vision_encoder_lr is None:
            self.vision_encoder_lr = getattr(self, 'optimizer_lr', 5e-5)
        if self.expert_lr is None:
            self.expert_lr = getattr(self, 'optimizer_lr', 5e-5)

        # ç¬¬ä¸€æ­¥ï¼šè½¬æ¢ OmegaConf å¯¹è±¡ï¼ˆå¿…é¡»åœ¨çˆ¶ç±» __post_init__ ä¹‹å‰ï¼‰
        self._convert_omegaconf_to_native()

        # ç¬¬äºŒæ­¥ï¼šé‡æ–°å°† features è½¬æ¢ä¸º PolicyFeature å¯¹è±¡
        # è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸º _convert_omegaconf_to_native ä¼šå°†å®ƒä»¬è½¬æ¢ä¸ºå­—å…¸
        if hasattr(self, 'input_features') and self.input_features is not None:
            self.input_features = self._normalize_feature_dict(
                self.input_features)
        if hasattr(self, 'output_features') and self.output_features is not None:
            self.output_features = self._normalize_feature_dict(
                self.output_features)

        # ç¬¬ä¸‰æ­¥ï¼šè°ƒç”¨çˆ¶ç±»çš„ååˆå§‹åŒ–
        super().__post_init__()

        # éªŒè¯æ·±åº¦é…ç½®
        if self.use_depth and not self.depth_features:
            raise ValueError("use_depth=True but no depth_features specified")

        # æ³¨æ„ï¼šä¸ºäº†ä½¿ç”¨SmolVLAé¢„è®­ç»ƒæƒé‡ï¼Œmax_action_dimå’Œmax_state_dimåº”è¯¥ä¸º32ï¼ˆä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€è‡´ï¼‰
        # Kuavoå®é™…æ˜¯16ç»´ï¼Œæ•°æ®ä¼šåœ¨åŠ è½½æ—¶è‡ªåŠ¨å¡«å……åˆ°32ç»´
        if self.max_action_dim == 32 and self.max_state_dim == 32:
            print(
                "âœ… Using SmolVLA pretrained dimensions (32D). Kuavo 16D data will be auto-padded.")
        elif self.max_action_dim != 32 or self.max_state_dim != 32:
            print(
                f"âš ï¸  Warning: max_action_dim={self.max_action_dim}, max_state_dim={self.max_state_dim}")
            print(
                f"   For pretrained SmolVLA, both should be 32. Current config may not load pretrained weights.")

        # æ‰“å°SmolVLAé…ç½®æ‘˜è¦
        print(f"ğŸ“‹ SmolVLA Config Summary (Kuavo with Depth):")
        print(f"   - VLM Model: {self.vlm_model_name}")
        print(
            f"   - Max Action Dim: {self.max_action_dim} (Kuavo actual: 16, auto-padded)")
        print(
            f"   - Max State Dim: {self.max_state_dim} (Kuavo actual: 16, auto-padded)")
        print(f"   - Chunk Size: {self.chunk_size}")
        print(f"   - Action Steps: {self.n_action_steps}")
        print(f"   - Freeze Vision: {self.freeze_vision_encoder}")
        print(f"   - Train Expert Only: {self.train_expert_only}")

        # ğŸ†• æ‰“å°çµæ´»å†»ç»“ç­–ç•¥é…ç½®
        if self.unfreeze_vision_layers is not None:
            print(
                f"   - ğŸ”“ Unfreeze Vision Layers: {self.unfreeze_vision_layers} (çµæ´»ç­–ç•¥)")
        elif self.freeze_vision_layers is not None:
            print(
                f"   - ğŸ”’ Freeze Vision Layers: {self.freeze_vision_layers} (çµæ´»ç­–ç•¥)")
        elif self.freeze_vision_ratio is not None:
            print(
                f"   - ğŸ”’ Freeze Vision Ratio: {self.freeze_vision_ratio:.1%} (æ¯”ä¾‹ç­–ç•¥)")

        print(f"   - Use Layerwise LR: {self.use_layerwise_lr}")
        if self.use_layerwise_lr:
            print(f"     - Vision Encoder LR: {self.vision_encoder_lr:.2e}")
            print(f"     - Expert LR: {self.expert_lr:.2e}")
        print(f"   - Use Depth: {self.use_depth}")
        print(f"   - Depth Features: {self.depth_features}")
        print(
            f"   - Use Depth Padding: {self.use_depth_padding} {'(é«˜ç²¾åº¦æ¨¡å¼)' if self.use_depth_padding else '(å¿«é€Ÿæ¨¡å¼)'}")

    def _convert_omegaconf_to_native(self):
        """
        å°†é…ç½®ä¸­æ‰€æœ‰ OmegaConf å¯¹è±¡è½¬æ¢ä¸ºåŸç”Ÿ Python å¯¹è±¡

        è¿™ç¡®ä¿äº†é…ç½®å¯ä»¥è¢« JSON åºåˆ—åŒ–ï¼Œæ”¯æŒ lerobot çš„æ ‡å‡†ä¿å­˜/åŠ è½½æ–¹å¼ã€‚
        åªåœ¨éœ€è¦æ—¶å¯¼å…¥ omegaconfï¼Œé¿å…ä¸å¿…è¦çš„ä¾èµ–ã€‚
        """
        try:
            from omegaconf import DictConfig, ListConfig, OmegaConf
        except ImportError:
            # å¦‚æœæ²¡æœ‰å®‰è£… omegaconfï¼Œè¯´æ˜é…ç½®å·²ç»æ˜¯åŸç”Ÿå¯¹è±¡ï¼Œæ— éœ€è½¬æ¢
            return

        # éå†æ‰€æœ‰ dataclass å­—æ®µ
        for field in fields(self):
            value = getattr(self, field.name)

            # è½¬æ¢ OmegaConf å¯¹è±¡ä¸ºåŸç”Ÿ Python å¯¹è±¡
            if isinstance(value, (DictConfig, ListConfig)):
                # OmegaConf.to_container ä¼šé€’å½’è½¬æ¢æ‰€æœ‰åµŒå¥—çš„ DictConfig/ListConfig
                native_value = OmegaConf.to_container(value, resolve=True)
                setattr(self, field.name, native_value)

    def _normalize_feature_dict(self, d):
        """
        å°†å­—å…¸æ ¼å¼çš„ features è½¬æ¢ä¸º PolicyFeature å¯¹è±¡

        å½“ OmegaConf é…ç½®è¢«è½¬æ¢ä¸ºåŸç”Ÿ Python å¯¹è±¡åï¼Œinput_features å’Œ output_features
        ä¼šå˜æˆå­—å…¸ï¼Œéœ€è¦é‡æ–°è½¬æ¢ä¸º PolicyFeature å¯¹è±¡ä»¥ä¾›ç­–ç•¥æ¨¡å‹ä½¿ç”¨ã€‚

        Args:
            d: å­—å…¸æˆ–åŒ…å«å­—å…¸çš„å­—å…¸

        Returns:
            åŒ…å« PolicyFeature å¯¹è±¡çš„å­—å…¸
        """
        if not isinstance(d, dict):
            return d

        return {
            k: PolicyFeature(**v) if isinstance(v,
                                                dict) and not isinstance(v, PolicyFeature) else v
            for k, v in d.items()
        }

    def _save_pretrained(self, save_directory: Path) -> None:
        """
        ä¿å­˜é…ç½®åˆ°æŒ‡å®šç›®å½•

        åœ¨ä¿å­˜å‰ï¼Œå°†ä¸èƒ½è¢« JSON åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¦‚ torch.deviceï¼‰è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼ã€‚

        Args:
            save_directory: ä¿å­˜ç›®å½•è·¯å¾„
        """
        import draccus
        from lerobot.configs.policies import CONFIG_NAME

        # åˆ›å»ºæ·±æ‹·è´ä»¥é¿å…ä¿®æ”¹åŸå§‹é…ç½®
        cfg_copy = deepcopy(self)

        # å°† torch.device è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if hasattr(cfg_copy, 'device') and isinstance(cfg_copy.device, torch.device):
            cfg_copy.device = str(cfg_copy.device)

        # ä½¿ç”¨ draccus ä¿å­˜é…ç½®
        save_directory = Path(save_directory)
        save_directory.mkdir(parents=True, exist_ok=True)

        with open(save_directory / CONFIG_NAME, "w") as f, draccus.config_type("json"):
            draccus.dump(cfg_copy, f, indent=4)

    @classmethod
    def from_pretrained(
        cls: type[T],
        pretrained_name_or_path: str | Path,
        *,
        force_download: bool = False,
        resume_download: bool = None,
        proxies: dict | None = None,
        token: str | bool | None = None,
        cache_dir: str | Path | None = None,
        local_files_only: bool = False,
        revision: str | None = None,
        **policy_kwargs,
    ) -> T:
        """
        ä»é¢„è®­ç»ƒè·¯å¾„åŠ è½½é…ç½®

        è¿™ä¸ªæ–¹æ³•è°ƒç”¨çˆ¶ç±»çš„ from_pretrainedï¼Œç¡®ä¿æ­£ç¡®å¤„ç†é…ç½®æ–‡ä»¶ä¸­çš„ type å­—æ®µã€‚

        Args:
            pretrained_name_or_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹ ID
            å…¶ä»–å‚æ•°åŒ PreTrainedConfig.from_pretrained

        Returns:
            åŠ è½½çš„é…ç½®å¯¹è±¡
        """
        # è°ƒç”¨çˆ¶ç±» PreTrainedConfig çš„ from_pretrainedï¼Œè§¦å‘ Choice æœºåˆ¶è¯†åˆ«å­ç±»
        parent_cls = PreTrainedConfig

        return parent_cls.from_pretrained(
            pretrained_name_or_path,
            force_download=force_download,
            resume_download=resume_download,
            proxies=proxies,
            token=token,
            cache_dir=cache_dir,
            local_files_only=local_files_only,
            revision=revision,
            **policy_kwargs,
        )
