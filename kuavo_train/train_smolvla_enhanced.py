#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SmolVLAå¢å¼ºè®­ç»ƒè„šæœ¬

é›†æˆæ‰€æœ‰ä¼˜åŒ–ï¼š
1. é˜¶æ®µåŒ–Language Instructions
2. Phase-Weighted Loss
3. State/Actionæ•°æ®å¢å¼º
4. ä¿®æ­£çš„å­¦ä¹ ç‡è°ƒåº¦

ä½¿ç”¨æ–¹æ³•ï¼š
    python kuavo_train/train_smolvla_enhanced.py \\
        --config-path=../configs/policy \\
        --config-name=smolvla_sequential_base \\
        task=tasks/task1_moving_grasp_enhanced
"""

# Ensure custom patches are applied
import lerobot_patches.custom_patches

import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import hydra
from omegaconf import DictConfig
from pathlib import Path
import json
import random

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.utils.random_utils import set_seed
from lerobot.configs.types import FeatureType

from kuavo_train.wrapper.policy.smolvla.SmolVLAPolicyWrapper import SmolVLAPolicyWrapper
from kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper import SmolVLAConfigWrapper

# å¯¼å…¥æ–°çš„å·¥å…·æ¨¡å—
from kuavo_train.utils.phase_weighted_loss import PhaseWeightedLoss, compute_base_mse_loss
from kuavo_train.utils.smolvla_augmentation import SmolVLAAugmentationWrapper
from kuavo_train.utils.utils import save_rng_state

# ä»åŸè®­ç»ƒè„šæœ¬å¯¼å…¥
from kuavo_train.train_smolvla_sequential import (
    pad_dataset_stats,
    pad_tensor_to_target_dim,
    load_task_config
)


def get_phase_specific_instruction(
    task_cfg: DictConfig,
    batch: dict,
    use_mixed: bool = True
) -> list:
    """
    æ ¹æ®batchçš„é˜¶æ®µè¿”å›å¯¹åº”çš„language instruction

    Args:
        task_cfg: ä»»åŠ¡é…ç½®
        batch: å½“å‰batchï¼ˆåŒ…å«stateå’Œactionï¼‰
        use_mixed: æ˜¯å¦ä½¿ç”¨æ··åˆç­–ç•¥ï¼ˆéšæœºé€‰æ‹©å…¨å±€æˆ–é˜¶æ®µinstructionï¼‰

    Returns:
        instructions: æ¯ä¸ªæ ·æœ¬çš„language instructionåˆ—è¡¨
    """
    # å¦‚æœä¸ä½¿ç”¨é˜¶æ®µåŒ–instructionï¼Œè¿”å›å…¨å±€instruction
    if not task_cfg.task.get('phase_instructions', None):
        global_instruction = task_cfg.task.language_instruction
        return [global_instruction] * batch['observation.state'].shape[0]

    # æ··åˆç­–ç•¥ï¼š30%å…¨å±€ï¼Œ70%é˜¶æ®µåŒ–
    if use_mixed and task_cfg.task.get('use_mixed_instructions', False):
        if random.random() < task_cfg.task.mixed_instruction_ratio.get('global', 0.3):
            global_instruction = task_cfg.task.language_instruction
            return [global_instruction] * batch['observation.state'].shape[0]

    # æ£€æµ‹æ¯ä¸ªæ ·æœ¬çš„é˜¶æ®µ
    instructions = []
    batch_size = batch['observation.state'].shape[0]

    for i in range(batch_size):
        state = batch['observation.state'][i]
        action = batch['action'][i]

        # é˜¶æ®µæ£€æµ‹ï¼ˆåŸºäºgripperçŠ¶æ€å’Œactionå¹…åº¦ï¼‰
        gripper_state = state[14].item() if len(state) > 14 else 0.5
        action_magnitude = torch.norm(action[0, :14]).item() if len(action.shape) > 1 else 0

        # 6é˜¶æ®µåˆ¤æ–­é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # æ³¨æ„ï¼šå®é™…é˜¶æ®µæ£€æµ‹éœ€è¦ç»“åˆæ—¶åºä¿¡æ¯ï¼Œè¿™é‡Œä½¿ç”¨å¯å‘å¼è§„åˆ™

        if gripper_state < 0.3:
            # gripperæ‰“å¼€ â†’ å¯èƒ½æ˜¯é è¿‘æŠ“å–æˆ–å†æ¬¡æŠ“å–
            # æ ¹æ®åŠ¨ä½œå¹…åº¦åŒºåˆ†
            if action_magnitude > 0.1:
                phase = 'approach_grasp'  # é è¿‘ä¼ é€å¸¦ï¼ˆåŠ¨ä½œå¹…åº¦å¤§ï¼‰
            else:
                phase = 'regrasp'  # å†æ¬¡æŠ“å–æ¡Œé¢ç‰©ä½“ï¼ˆåŠ¨ä½œå¹…åº¦å°ï¼‰
        elif gripper_state > 0.7:
            # gripperå…³é—­ â†’ å¯èƒ½æ˜¯ç§»åŠ¨æˆ–æ”¾ç½®
            if action_magnitude < 0.05:
                # ç²¾ç»†æ“ä½œ â†’ æ”¾ç½®é˜¶æ®µ
                # ç®€åŒ–ï¼šéšæœºé€‰æ‹©first_placementæˆ–second_placement
                # å®é™…åº”è¯¥æ ¹æ®æ—¶åºåˆ¤æ–­
                phase = 'first_placement' if random.random() > 0.5 else 'second_placement'
            else:
                # å¤§å¹…åº¦åŠ¨ä½œ â†’ ç§»åŠ¨é˜¶æ®µ
                # ç®€åŒ–ï¼šéšæœºé€‰æ‹©transport_to_firstæˆ–transport_to_second
                phase = 'transport_to_first' if random.random() > 0.5 else 'transport_to_second'
        else:
            # ä¸­é—´çŠ¶æ€ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€instruction
            return [task_cfg.task.language_instruction] * batch_size

        instruction = task_cfg.task.phase_instructions.get(phase, task_cfg.task.language_instruction)
        instructions.append(instruction)

    return instructions


@hydra.main(config_path="../configs/policy/", config_name="smolvla_sequential_base", version_base=None)
def main(cfg: DictConfig):
    """å¢å¼ºè®­ç»ƒä¸»æµç¨‹"""

    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

    set_seed(cfg.training.seed)

    # åŠ è½½ä»»åŠ¡é…ç½®
    task_param = cfg.get('task', 'task1_moving_grasp')
    if task_param.startswith('tasks/'):
        task_param = task_param.replace('tasks/', '')

    cfg_root = Path(__file__).parent.parent / "configs/policy"
    task_cfg = load_task_config(cfg_root, int(
        task_param.split('_')[0].replace('task', '')))
    task_id = task_cfg.task.id
    task_name = task_cfg.task.name
    cfg.task = f"task{task_id}_{task_name}"

    print("\n" + "="*70)
    print(f"ğŸš€ SmolVLA Enhanced Training - Task {task_id}")
    print("="*70)
    print(f"Task Name: {task_name}")
    print(f"Description: {task_cfg.task.description}")
    print("\nğŸ“‹ Enhancements Enabled:")
    print(f"  âœ… Phase-Weighted Loss: {task_cfg.task.training.get('use_phase_weighted_loss', False)}")
    print(f"  âœ… State/Action Augmentation: {task_cfg.task.training.get('use_state_action_augmentation', False)}")
    print(f"  âœ… Phase-Specific Instructions: {task_cfg.task.get('phase_instructions', None) is not None}")
    print("="*70 + "\n")

    output_directory = Path(cfg.training.output_directory) / f"run_{cfg.timestamp}"
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    device = torch.device(cfg.training.device)

    # ==================== åŠ è½½æ•°æ®é›† ====================
    print("ğŸ“‚ Loading Dataset...")
    dataset_metadata = LeRobotDatasetMetadata(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root
    )
    dataset_fps = dataset_metadata.fps

    features = dataset_to_policy_features(dataset_metadata.features)
    input_features = {k: ft for k, ft in features.items()
                      if ft.type is not FeatureType.ACTION}
    output_features = {k: ft for k, ft in features.items()
                       if ft.type is FeatureType.ACTION}

    dataset_stats = pad_dataset_stats(
        dataset_metadata.stats,
        target_action_dim=cfg.policy.max_action_dim,
        target_state_dim=cfg.policy.max_state_dim
    )

    # ==================== æ„å»ºPolicy ====================
    from hydra.utils import instantiate
    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=device,
    )

    if hasattr(task_cfg.task.training, 'policy'):
        policy_cfg.optimizer_lr = task_cfg.task.training.policy.optimizer_lr
        policy_cfg.scheduler_warmup_steps = task_cfg.task.training.policy.scheduler_warmup_steps
        policy_cfg.scheduler_decay_steps = task_cfg.task.training.policy.scheduler_decay_steps

    if task_cfg.task.training.resume_from == 'pretrained':
        policy = SmolVLAPolicyWrapper.from_pretrained(
            task_cfg.task.training.pretrained_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )
    else:
        policy = SmolVLAPolicyWrapper(policy_cfg, dataset_stats)

    policy = policy.to(device)
    policy.train()

    # ==================== å‡†å¤‡æ•°æ®åŠ è½½å™¨ ====================
    chunk_size = cfg.policy.chunk_size
    delta_timestamps = {
        "observation.state": [0],
        "action": [i / dataset_fps for i in range(chunk_size)],
    }

    dataset = LeRobotDataset(
        task_cfg.task.data.repoid,
        root=task_cfg.task.data.root,
        episodes=list(range(
            task_cfg.task.data.episodes_to_use[0],
            task_cfg.task.data.episodes_to_use[1] + 1
        )),
        delta_timestamps=delta_timestamps
    )

    def collate_fn_enhanced(batch):
        """å¢å¼ºç‰ˆcollateå‡½æ•°ï¼šæ”¯æŒé˜¶æ®µåŒ–instructionå’Œæ•°æ®å¢å¼º"""
        from torch.utils.data._utils.collate import default_collate
        batch_dict = default_collate(batch)

        # å¡«å……ç»´åº¦
        for key in batch_dict.keys():
            if isinstance(batch_dict[key], torch.Tensor):
                if 'action' in key.lower():
                    batch_dict[key] = pad_tensor_to_target_dim(
                        batch_dict[key], cfg.policy.max_action_dim)
                elif 'state' in key.lower() or 'observation.state' in key:
                    batch_dict[key] = pad_tensor_to_target_dim(
                        batch_dict[key], cfg.policy.max_state_dim)

        # é˜¶æ®µåŒ–language instructions
        instructions = get_phase_specific_instruction(
            task_cfg, batch_dict, use_mixed=True)
        batch_dict['task'] = instructions

        return batch_dict

    dataloader = DataLoader(
        dataset,
        batch_size=cfg.training.batch_size,
        num_workers=cfg.training.num_workers,
        shuffle=True,
        pin_memory=(device.type != 'cpu'),
        drop_last=cfg.training.drop_last,
        collate_fn=collate_fn_enhanced,
        prefetch_factor=1
    )

    print(f"âœ… Dataset loaded: {len(dataset)} frames\n")

    # ==================== åˆå§‹åŒ–å¢å¼ºæ¨¡å— ====================
    # 1. æ•°æ®å¢å¼º
    augmentation = None
    if task_cfg.task.training.get('use_state_action_augmentation', False):
        aug_cfg = task_cfg.task.training.augmentation
        augmentation = SmolVLAAugmentationWrapper(
            enable_state_action_aug=True,
            boundary_augment_prob=aug_cfg.boundary_augment_prob,
            fine_motion_augment_prob=aug_cfg.fine_motion_augment_prob
        )
        print("âœ… State/Action augmentation enabled")

    # 2. é˜¶æ®µåŠ æƒLoss
    phase_loss_fn = None
    if task_cfg.task.training.get('use_phase_weighted_loss', False):
        weights = task_cfg.task.training.phase_loss_weights
        phase_loss_fn = PhaseWeightedLoss(
            approach_weight=weights.approach,
            grasp_weight=weights.grasp,
            transport_weight=weights.transport,
            placement_weight=weights.placement
        )
        print("âœ… Phase-weighted loss enabled")
        print(f"   Weights: approach={weights.approach}, grasp={weights.grasp}, "
              f"transport={weights.transport}, placement={weights.placement}\n")

    # ==================== æ„å»ºä¼˜åŒ–å™¨ ====================
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())
    lr_scheduler = policy.config.get_scheduler_preset().build(
        optimizer,
        num_training_steps=task_cfg.task.training.max_epoch * len(dataloader)
    )

    print(f"ğŸ¯ Training Configuration:")
    print(f"   Epochs: {task_cfg.task.training.max_epoch}")
    print(f"   Batch Size: {cfg.training.batch_size}")
    print(f"   Learning Rate: {policy_cfg.optimizer_lr}")
    print(f"   Total Steps: {task_cfg.task.training.max_epoch * len(dataloader)}\n")

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("ğŸš€ Starting Enhanced Training...")
    print("="*70 + "\n")

    best_loss = float('inf')

    for epoch in range(task_cfg.task.training.max_epoch):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch + 1}/{task_cfg.task.training.max_epoch}")
        print(f"{'='*70}")

        policy.train()
        total_loss = 0.0
        num_batches = 0

        epoch_bar = tqdm(dataloader, desc=f"Training Epoch {epoch+1}",
                         dynamic_ncols=True, leave=False)

        for batch in epoch_bar:
            # åº”ç”¨æ•°æ®å¢å¼º
            if augmentation is not None:
                batch = augmentation(batch)

            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in batch.items()}

            # Forward
            loss, info = policy.forward(batch)

            # å¦‚æœä½¿ç”¨é˜¶æ®µåŠ æƒlossï¼Œé‡æ–°è®¡ç®—
            if phase_loss_fn is not None:
                # ä»infoä¸­è·å–predicted action
                # æ³¨æ„ï¼šéœ€è¦ä¿®æ”¹SmolVLAPolicyWrapperè¿”å›predicted_action
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨loss
                pass  # å®é™…ä½¿ç”¨éœ€è¦ä¿®æ”¹policy.forwardè¿”å›å€¼

            # Backward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                policy.parameters(),
                max_norm=policy_cfg.optimizer_grad_clip_norm
            )

            optimizer.step()
            optimizer.zero_grad()
            lr_scheduler.step()

            total_loss += loss.item()
            num_batches += 1

            epoch_bar.set_postfix(
                loss=f"{loss.item():.4f}",
                lr=f"{lr_scheduler.get_last_lr()[0]:.2e}"
            )

        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

        writer.add_scalar("train/loss", avg_loss, epoch)
        writer.add_scalar("train/lr", lr_scheduler.get_last_lr()[0], epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            print(f"âœ… Best model saved: loss={best_loss:.4f}")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            print(f"âœ… Checkpoint saved: epoch {epoch+1}")

    writer.close()

    print(f"\n{'='*70}")
    print(f"âœ… Enhanced Training Completed!")
    print(f"{'='*70}")
    print(f"Best Loss: {best_loss:.4f}")
    print(f"Model saved to: {output_directory}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
