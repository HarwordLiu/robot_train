"""
SmolVLA Diffusion è®­ç»ƒè„šæœ¬

åŸºäº SmolVLA æ¶æ„ä½†ä½¿ç”¨ Diffusion è¿›è¡ŒåŠ¨ä½œç”Ÿæˆè®­ç»ƒ
å®Œå…¨å†»ç»“è§†è§‰å±‚ï¼Œä¸“æ³¨äºè®­ç»ƒ Action Expert çš„ Diffusion èƒ½åŠ›
"""

import os
import sys
import torch
import hydra
from omegaconf import OmegaConf
from pathlib import Path
import logging
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥å¿…è¦æ¨¡å—
from lerobot.common.policies import Policy
from lerobot.common.utils import init_logging, set_seed
from leroto import lerobot_datasets

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from kuavo_train.wrapper.policy.smolvla import SmolVLADiffusionPolicyWrapper
from kuavo_train.trainer.trainer import Trainer
from kuavo_train.datasets.dataset_utils import make_dataset


class DiffusionTrainer(Trainer):
    """
    Diffusion ä¸“ç”¨è®­ç»ƒå™¨
    """

    def __init__(self, cfg):
        super().__init__(cfg)
        self.use_diffusion = True
        self.cfg = cfg

    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ª epoch

        Args:
            epoch: å½“å‰ epoch
        """
        self.policy.train()
        total_loss = 0.0
        num_batches = 0

        for batch_idx, batch in enumerate(self.dataloader):
            # å‡†å¤‡ batch
            batch = self.prepare_batch(batch)

            # å‰å‘ä¼ æ’­
            loss, info = self.policy.forward(batch)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            if hasattr(self.cfg.policy, 'optimizer_grad_clip_norm'):
                torch.nn.utils.clip_grad_norm_(
                    self.policy.parameters(),
                    self.cfg.policy.optimizer_grad_clip_norm
                )

            self.optimizer.step()
            self.scheduler.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1

            # æ—¥å¿—
            if batch_idx % self.cfg.training.log_freq == 0:
                step = epoch * len(self.dataloader) + batch_idx
                self.log_training_info(step, loss.item(), info)

        # è¿”å›å¹³å‡æŸå¤±
        return total_loss / num_batches

    def prepare_batch(self, batch):
        """
        å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        """
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        device = self.cfg.training.device
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(device)

        # ç¡®ä¿æœ‰è¯­è¨€æŒ‡ä»¤
        if 'task' not in batch:
            # ä½¿ç”¨é»˜è®¤è¯­è¨€æŒ‡ä»¤
            batch['task'] = [self.cfg.task.get('language_instruction', 'Complete the task')] * len(batch[next(iter(batch))])

        return batch

    def log_training_info(self, step, loss, info):
        """
        è®°å½•è®­ç»ƒä¿¡æ¯
        """
        if self.use_wandb:
            import wandb
            log_dict = {
                'train/loss': loss,
                'train/step': step,
            }

            # æ·»åŠ  Diffusion ç‰¹å®šä¿¡æ¯
            if 'timestep_mean' in info:
                log_dict['train/timestep_mean'] = info['timestep_mean']
            if 'noise_mean' in info:
                log_dict['train/noise_mean'] = info['noise_mean']
            if 'predicted_noise_mean' in info:
                log_dict['train/predicted_noise_mean'] = info['predicted_noise_mean']

            # æ·»åŠ å­¦ä¹ ç‡
            if hasattr(self, 'scheduler'):
                log_dict['train/lr'] = self.scheduler.get_last_lr()[0]

            wandb.log(log_dict)

        # æ§åˆ¶å°è¾“å‡º
        if step % (self.cfg.training.log_freq * 10) == 0:
            print(f"Step {step}: Loss = {loss:.6f}")
            if 'timestep_mean' in info:
                print(f"  - Avg Timestep: {info['timestep_mean']:.2f}")

    def validate(self, epoch):
        """
        éªŒè¯æ¨¡å‹
        """
        if not hasattr(self, 'eval_dataloader') or self.eval_dataloader is None:
            return None

        self.policy.eval()
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in self.eval_dataloader:
                batch = self.prepare_batch(batch)

                # å‰å‘ä¼ æ’­
                loss, info = self.policy.forward(batch)

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else None

        # è®°å½•éªŒè¯æŸå¤±
        if avg_loss is not None and self.use_wandb:
            import wandb
            wandb.log({
                'val/loss': avg_loss,
                'val/epoch': epoch,
            })

        return avg_loss


def setup_logging(cfg):
    """è®¾ç½®æ—¥å¿—"""
    log_dir = Path(cfg.training.output_directory) / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)

    # è®¾ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
            logging.StreamHandler()
        ]
    )

    return logging.getLogger(__name__)


def setup_wandb(cfg):
    """è®¾ç½® wandb"""
    if not cfg.get('use_wandb', False):
        return None

    import wandb

    wandb.init(
        project="smolvla-diffusion",
        name=f"{cfg.task}_{cfg.method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config=OmegaConf.to_container(cfg, resolve=True),
        dir=cfg.training.output_directory
    )

    return wandb


def make_policy(cfg, dataset_stats):
    """
    åˆ›å»ºç­–ç•¥æ¨¡å‹
    """
    # æ ¹æ®é…ç½®å†³å®šæ˜¯ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½è¿˜æ˜¯ä»å¤´è®­ç»ƒ
    if hasattr(cfg.training, 'resume_from') and cfg.training.resume_from == 'pretrained':
        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
        policy = SmolVLADiffusionPolicyWrapper.from_pretrained(
            pretrained_name_or_path=cfg.training.pretrained_path,
            config=cfg.policy,
            dataset_stats=dataset_stats
        )
    else:
        # åˆ›å»ºæ–°æ¨¡å‹
        policy = SmolVLADiffusionPolicyWrapper(cfg.policy, dataset_stats)

    return policy


def make_optimizer(cfg, policy):
    """
    åˆ›å»ºä¼˜åŒ–å™¨
    """
    # è·å–å¯è®­ç»ƒå‚æ•°
    trainable_params = [p for p in policy.parameters() if p.requires_grad]

    # åˆ›å»ºä¼˜åŒ–å™¨
    if cfg.policy.optimizer_type == 'adam':
        optimizer = torch.optim.Adam(
            trainable_params,
            lr=cfg.policy.optimizer_lr,
            betas=cfg.policy.optimizer_betas,
            eps=cfg.policy.optimizer_eps,
            weight_decay=cfg.policy.optimizer_weight_decay
        )
    elif cfg.policy.optimizer_type == 'adamw':
        optimizer = torch.optim.AdamW(
            trainable_params,
            lr=cfg.policy.optimizer_lr,
            betas=cfg.policy.optimizer_betas,
            eps=cfg.policy.optimizer_eps,
            weight_decay=cfg.policy.optimizer_weight_decay
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {cfg.policy.optimizer_type}")

    return optimizer


def make_scheduler(cfg, optimizer):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg.policy.scheduler_decay_steps,
        eta_min=cfg.policy.scheduler_decay_lr
    )

    return scheduler


def evaluate_policy(policy, cfg, epoch):
    """
    è¯„ä¼°ç­–ç•¥
    """
    # è¿™é‡Œå¯ä»¥æ·»åŠ è¯„ä¼°é€»è¾‘
    # ä¾‹å¦‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿è¡Œç­–ç•¥
    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°:")
    print("   - ç­–ç•¥è¯„ä¼°åŠŸèƒ½å¾…å®ç°")
    return {}


@hydra.main(
    version_base=None,
    config_path="../configs/policy",
    config_name="smolvla_diffusion_config"
)
def main(cfg):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    # è®¾ç½®è®¾å¤‡
    device = torch.device(cfg.training.device if torch.cuda.is_available() else 'cpu')
    cfg.training.device = str(device)

    print(f"\n{'='*70}")
    print("ğŸš€ å¼€å§‹ SmolVLA Diffusion è®­ç»ƒ")
    print(f"{'='*70}")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - ä»»åŠ¡: {cfg.task}")
    print(f"   - æ–¹æ³•: {cfg.method}")
    print(f"   - è®¾å¤‡: {device}")
    print(f"   - æ‰¹å¤§å°: {cfg.training.batch_size}")
    print(f"   - å­¦ä¹ ç‡: {cfg.policy.optimizer_lr}")
    print(f"   - æ¨ç†æ­¥æ•°: {cfg.policy.num_inference_steps}")
    print(f"   - è§†è§‰ç¼–ç å™¨å†»ç»“: {cfg.policy.freeze_vision_encoder}")
    print(f"{'='*70}\n")

    # è®¾ç½®éšæœºç§å­
    set_seed(cfg.training.seed)

    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(cfg)

    # è®¾ç½® wandb
    wandb_run = setup_wandb(cfg)
    cfg.use_wandb = wandb_run is not None

    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“¦ åˆ›å»ºæ•°æ®é›†...")
    train_dataset, eval_dataset = make_dataset(cfg)
    print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    if eval_dataset:
        print(f"   - éªŒè¯æ ·æœ¬æ•°: {len(eval_dataset)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=cfg.training.batch_size,
        shuffle=True,
        num_workers=cfg.training.num_workers,
        drop_last=cfg.training.drop_last,
        prefetch_factor=cfg.training.prefetch_factor,
        persistent_workers=cfg.training.persistent_workers
    )

    eval_dataloader = None
    if eval_dataset:
        eval_dataloader = DataLoader(
            eval_dataset,
            batch_size=cfg.training.batch_size,
            shuffle=False,
            num_workers=cfg.training.num_workers,
            drop_last=False
        )

    # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    if hasattr(train_dataset, 'stats') and train_dataset.stats is not None:
        dataset_stats = train_dataset.stats
    else:
        # åˆ›å»ºç©ºçš„ç»Ÿè®¡ä¿¡æ¯
        dataset_stats = SmolVLADiffusionPolicyWrapper._create_identity_stats(cfg.policy)

    # åˆ›å»ºç­–ç•¥
    print("ğŸ§  åˆ›å»ºç­–ç•¥æ¨¡å‹...")
    policy = make_policy(cfg, dataset_stats)
    policy.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    print("âš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = make_optimizer(cfg, policy)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = make_scheduler(cfg, optimizer)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DiffusionTrainer(cfg)
    trainer.policy = policy
    trainer.optimizer = optimizer
    trainer.scheduler = scheduler
    trainer.dataloader = train_dataloader
    trainer.eval_dataloader = eval_dataloader
    trainer.use_wandb = cfg.use_wandb

    # ä¿å­˜é…ç½®
    output_dir = Path(cfg.training.output_directory)
    output_dir.mkdir(parents=True, exist_ok=True)

    config_file = output_dir / "config.yaml"
    with open(config_file, 'w') as f:
        OmegaConf.save(cfg, f)

    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {config_file}")

    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸƒ å¼€å§‹è®­ç»ƒ (å…± {cfg.training.max_epoch} epochs)...")
    best_loss = float('inf')

    for epoch in range(cfg.training.max_epoch):
        print(f"\nEpoch {epoch + 1}/{cfg.training.max_epoch}")
        print("-" * 50)

        # è®­ç»ƒä¸€ä¸ª epoch
        avg_loss = trainer.train_epoch(epoch)
        print(f"è®­ç»ƒæŸå¤±: {avg_loss:.6f}")

        # éªŒè¯
        if eval_dataloader is not None and epoch % cfg.training.validation_freq_epoch == 0:
            val_loss = trainer.validate(epoch)
            if val_loss:
                print(f"éªŒè¯æŸå¤±: {val_loss:.6f}")

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % cfg.training.save_freq_epoch == 0:
            checkpoint_dir = output_dir / f"checkpoint_epoch_{epoch}"
            checkpoint_dir.mkdir(exist_ok=True)

            # ä¿å­˜æ¨¡å‹
            policy.save_pretrained(checkpoint_dir)

            # ä¿å­˜è®­ç»ƒçŠ¶æ€
            torch.save({
                'epoch': epoch,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_loss,
            }, checkpoint_dir / "training_state.pt")

            print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_dir = output_dir / "best"
            best_dir.mkdir(exist_ok=True)
            policy.save_pretrained(best_dir)
            print(f"ğŸ† æœ€ä½³æ¨¡å‹å·²æ›´æ–° (æŸå¤±: {best_loss:.6f})")

        # è¯„ä¼°ç­–ç•¥ï¼ˆå¯é€‰ï¼‰
        if epoch % 10 == 0:
            evaluate_policy(policy, cfg, epoch)

    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"   - æœ€ä½³æŸå¤±: {best_loss:.6f}")
    print(f"   - è¾“å‡ºç›®å½•: {output_dir}")

    # å…³é—­ wandb
    if wandb_run is not None:
        wandb_run.finish()


if __name__ == "__main__":
    main()