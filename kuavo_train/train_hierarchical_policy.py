# -*- coding: utf-8 -*-
"""
åˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyç»Ÿä¸€è®­ç»ƒè„šæœ¬

æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
1. åŸºç¡€åˆ†å±‚è®­ç»ƒï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ç›´æ¥å®šä¹‰çš„è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
2. ä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼šä½¿ç”¨TaskSpecificTrainingManagerç®¡ç†å¤šä»»åŠ¡åœºæ™¯

æ¨¡å¼é€‰æ‹©ç”± task_specific_training.enable é…ç½®å†³å®š

ä½¿ç”¨æ–¹æ³•ï¼š
# åŸºç¡€æ¨¡å¼
python kuavo_train/train_hierarchical_policy.py --config-name=humanoid_diffusion_config

# ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼ˆåœ¨é…ç½®ä¸­è®¾ç½® task_specific_training.enable=Trueï¼‰
python kuavo_train/train_hierarchical_policy.py --config-name=humanoid_diffusion_config
"""

# Ensure custom patches are applied, DON'T REMOVE THIS LINE!
import lerobot_patches.custom_patches
from lerobot.configs.policies import PolicyFeature
from typing import Any, Dict, List, Optional, Tuple

import os
import hydra
from omegaconf import DictConfig, OmegaConf, ListConfig
from pathlib import Path
from functools import partial

import torch
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, WeightedRandomSampler, ConcatDataset
from tqdm import tqdm
import shutil
from hydra.utils import instantiate
from diffusers.optimization import get_scheduler
import numpy as np
import logging

from lerobot.configs.types import FeatureType, NormalizationMode
from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.utils.random_utils import set_seed

# å¯¼å…¥åˆ†å±‚æ¶æ„æ¨¡å—
from kuavo_train.wrapper.policy.humanoid.HumanoidDiffusionPolicy import HumanoidDiffusionPolicy
from kuavo_train.wrapper.policy.humanoid.TaskSpecificTrainingManager import TaskSpecificTrainingManager
from kuavo_train.wrapper.dataset.LeRobotDatasetWrapper import CustomLeRobotDataset
from kuavo_train.utils.augmenter import crop_image, resize_image, DeterministicAugmenterColor
from kuavo_train.utils.utils import save_rng_state, load_rng_state
from utils.transforms import ImageTransforms, ImageTransformsConfig, ImageTransformConfig

from functools import partial
from contextlib import nullcontext


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('hierarchical_training.log', encoding='utf-8')
        ]
    )
    return logging.getLogger("HierarchicalTraining")


def build_augmenter(cfg):
    """æ„å»ºå›¾åƒå¢å¼ºå™¨"""
    img_tf_cfg = ImageTransformsConfig(
        enable=cfg.get("enable", False),
        max_num_transforms=cfg.get("max_num_transforms", 3),
        random_order=cfg.get("random_order", False),
        tfs={}
    )

    if "tfs" in cfg:
        for name, tf_dict in cfg["tfs"].items():
            img_tf_cfg.tfs[name] = ImageTransformConfig(
                weight=tf_dict.get("weight", 1.0),
                type=tf_dict.get("type", "Identity"),
                kwargs=tf_dict.get("kwargs", {}),
            )
    return ImageTransforms(img_tf_cfg)


def build_delta_timestamps(dataset_metadata, policy_cfg):
    """æ„å»ºdelta timestamps"""
    obs_indices = getattr(policy_cfg, "observation_delta_indices", None)
    act_indices = getattr(policy_cfg, "action_delta_indices", None)
    if obs_indices is None and act_indices is None:
        return None

    delta_timestamps = {}
    for key in dataset_metadata.info["features"]:
        if "observation" in key and obs_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in obs_indices]
        elif "action" in key and act_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in act_indices]

    return delta_timestamps if delta_timestamps else None


def build_optimizer_and_scheduler(policy, cfg, total_frames):
    """æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())

    if cfg.training.max_training_step is None:
        updates_per_epoch = (
            total_frames // (cfg.training.batch_size * cfg.training.accumulation_steps)) + 1
        num_training_steps = cfg.training.max_epoch * updates_per_epoch
    else:
        num_training_steps = cfg.training.max_training_step

    lr_scheduler = policy.config.get_scheduler_preset()
    if lr_scheduler is not None:
        lr_scheduler = lr_scheduler.build(optimizer, num_training_steps)
    else:
        lr_scheduler = get_scheduler(
            name=cfg.training.scheduler_name,
            optimizer=optimizer,
            num_warmup_steps=cfg.training.scheduler_warmup_steps,
            num_training_steps=num_training_steps,
        )

    return optimizer, lr_scheduler


def build_policy_config(cfg, input_features, output_features):
    """æ„å»ºpolicyé…ç½®"""
    def _normalize_feature_dict(d: Any) -> dict[str, PolicyFeature]:
        if isinstance(d, DictConfig):
            d = OmegaConf.to_container(d, resolve=True)
        if not isinstance(d, dict):
            raise TypeError(
                "Expected dict or DictConfig, got {}".format(type(d)))

        return {
            k: PolicyFeature(**v) if isinstance(v,
                                                dict) and not isinstance(v, PolicyFeature) else v
            for k, v in d.items()
        }

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=cfg.training.device,
    )

    policy_cfg.input_features = _normalize_feature_dict(
        policy_cfg.input_features)
    policy_cfg.output_features = _normalize_feature_dict(
        policy_cfg.output_features)
    return policy_cfg


def build_hierarchical_policy(policy_cfg, dataset_stats):
    """æ„å»ºåˆ†å±‚æ¶æ„çš„policy"""
    print("ğŸ¤– æ„å»ºHumanoidDiffusionPolicyåˆ†å±‚æ¶æ„...")
    return HumanoidDiffusionPolicy(policy_cfg, dataset_stats)


def load_task_dataset(task_id: int, cfg: DictConfig, policy_cfg, image_transforms) -> Tuple[Optional[LeRobotDataset], Optional[LeRobotDatasetMetadata]]:
    """åŠ è½½ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ï¼ˆä»»åŠ¡ç‰¹å®šæ¨¡å¼ä½¿ç”¨ï¼‰"""
    # ä»»åŠ¡1ç›´æ¥ä»rooté…ç½®è¯»å–
    if task_id == 1:
        task_data_path = cfg.get('root', '')
        if not task_data_path:
            print(f"âš ï¸  rooté…ç½®æœªè®¾ç½®")
            return None, None

        if not os.path.exists(task_data_path):
            print(f"âš ï¸  rootæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {task_data_path}")
            return None, None
    else:
        # å…¶ä»–ä»»åŠ¡ä½¿ç”¨task_specific_trainingé…ç½®
        task_config = cfg.get('task_specific_training', {})
        data_config = task_config.get('data_config', {})

        base_path = data_config.get('base_path', '/robot/data')
        task_dir = data_config.get('task_directories', {}).get(task_id)

        if not task_dir:
            print(f"âš ï¸  ä»»åŠ¡{task_id}çš„æ•°æ®ç›®å½•æœªé…ç½®")
            return None, None

        task_data_path = os.path.join(base_path, task_dir)
        if not os.path.exists(task_data_path):
            print(f"âš ï¸  ä»»åŠ¡{task_id}æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {task_data_path}")
            return None, None

    try:
        task_repoid = f"lerobot/task_{task_id}"
        dataset_metadata = LeRobotDatasetMetadata(
            task_repoid, root=task_data_path)

        delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)

        max_episodes = cfg.task_specific_training.memory_management.get(
            'max_episodes_per_task', 300)
        episodes_to_use = list(
            range(min(max_episodes, dataset_metadata.info["total_episodes"])))

        dataset = LeRobotDataset(
            task_repoid,
            delta_timestamps=delta_timestamps,
            root=task_data_path,
            episodes=episodes_to_use,
            image_transforms=image_transforms,
        )

        print(f"âœ… ä»»åŠ¡{task_id}æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(episodes_to_use)}ä¸ªepisodes")
        return dataset, dataset_metadata

    except Exception as e:
        print(f"âŒ ä»»åŠ¡{task_id}æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None, None


def create_task_specific_dataloader(datasets: Dict[int, LeRobotDataset], task_manager: TaskSpecificTrainingManager,
                                    cfg: DictConfig, device: torch.device) -> DataLoader:
    """åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„æ•°æ®åŠ è½½å™¨ï¼ˆå¤šä»»åŠ¡åŠ æƒé‡‡æ ·ï¼‰"""
    if len(datasets) == 1:
        # å•ä»»åŠ¡æƒ…å†µ
        task_id = list(datasets.keys())[0]
        dataset = datasets[task_id]
        return DataLoader(
            dataset,
            batch_size=cfg.training.batch_size,
            shuffle=True,
            num_workers=cfg.training.num_workers,
            pin_memory=(device.type != "cpu"),
            drop_last=cfg.training.drop_last,
            prefetch_factor=1,
        )

    # å¤šä»»åŠ¡æƒ…å†µ - ä½¿ç”¨åŠ æƒé‡‡æ ·
    sampling_strategy = task_manager.get_task_data_sampling_strategy()
    task_weights = sampling_strategy.get("task_weights", {})

    combined_datasets = []
    sample_weights = []

    for task_id, dataset in datasets.items():
        task_weight = task_weights.get(task_id, 1.0 / len(datasets))
        dataset_size = len(dataset)

        combined_datasets.append(dataset)
        sample_weights.extend([task_weight] * dataset_size)

    combined_dataset = ConcatDataset(combined_datasets)

    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(combined_dataset),
        replacement=True
    )

    return DataLoader(
        combined_dataset,
        batch_size=cfg.training.batch_size,
        sampler=sampler,
        num_workers=cfg.training.num_workers,
        pin_memory=(device.type != "cpu"),
        drop_last=cfg.training.drop_last,
        prefetch_factor=1,
    )


def run_curriculum_learning_stage(policy, stage_config, dataset, cfg, device, writer, current_step,
                                  optimizer=None, lr_scheduler=None, scaler=None, output_directory=None,
                                  amp_enabled=False, task_manager=None, dataloader=None):
    """
    è¿è¡Œè¯¾ç¨‹å­¦ä¹ çš„å•ä¸ªé˜¶æ®µ

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. åŸºç¡€æ¨¡å¼ï¼šdatasetå‚æ•°ä¼ å…¥ï¼Œå†…éƒ¨åˆ›å»ºdataloader
    2. ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šdataloaderå‚æ•°ä¼ å…¥ï¼Œä½¿ç”¨task_manager
    """
    stage_name = stage_config.get("name", "unknown")
    enabled_layers = stage_config.get("layers", [])
    stage_epochs = stage_config.get("epochs", 10)
    target_task = stage_config.get("target_task")  # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ä½¿ç”¨

    # æµ‹è¯•è®­ç»ƒæ¨¡å¼
    test_training_mode = cfg.training.get('test_training_mode', False)
    if test_training_mode:
        original_epochs = stage_epochs
        test_epochs = cfg.training.get('test_training_epochs', 1)
        stage_epochs = test_epochs
        print(f"ğŸ§ª TEST MODE: Overriding {stage_name} stage epochs from {original_epochs} to {test_epochs}")

    print(f"ğŸ“ å¼€å§‹è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: {stage_name}")
    print(f"   æ¿€æ´»å±‚: {enabled_layers}")
    print(f"   è®­ç»ƒè½®æ¬¡: {stage_epochs}")
    if target_task:
        print(f"   ç›®æ ‡ä»»åŠ¡: {target_task}")

    # æ¿€æ´»æŒ‡å®šçš„å±‚
    if hasattr(policy, 'set_curriculum_stage'):
        policy.set_curriculum_stage(enabled_layers)

    # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šé…ç½®ä»»åŠ¡å±‚æƒé‡
    if task_manager and target_task and target_task != "all":
        layer_weights = task_manager.get_task_specific_layer_weights(target_task)
        if hasattr(policy, 'set_task_layer_weights'):
            policy.set_task_layer_weights(layer_weights)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if dataloader is None:
        dataloader = DataLoader(
            dataset,
            num_workers=cfg.training.num_workers,
            batch_size=cfg.training.batch_size,
            shuffle=True,
            pin_memory=(device.type != "cpu"),
            drop_last=cfg.training.drop_last,
            prefetch_factor=1,
        )

    # AMP autocast
    has_torch_autocast = hasattr(torch, "autocast")

    def make_autocast(enabled: bool):
        if not enabled:
            return nullcontext()
        if device.type == "cuda":
            if has_torch_autocast:
                return torch.autocast(device_type="cuda")
            else:
                from torch.cuda.amp import autocast as cuda_autocast
                return cuda_autocast()
        return nullcontext()

    stage_steps = 0
    best_stage_loss = float('inf')

    for epoch in range(stage_epochs):
        print(f"ğŸš€ å¼€å§‹ Epoch {epoch+1}/{stage_epochs}")
        epoch_bar = tqdm(
            dataloader,
            desc=f"Stage {stage_name} Epoch {epoch+1}/{stage_epochs}",
            dynamic_ncols=True,
            leave=False)

        total_epoch_loss = 0.0
        epoch_samples = 0

        for batch in epoch_bar:
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            # æ„å»ºè¯¾ç¨‹å­¦ä¹ ä¿¡æ¯
            curriculum_info = {
                'stage': stage_name,
                'enabled_layers': enabled_layers
            }
            if target_task:
                curriculum_info['target_task'] = target_task

            with make_autocast(amp_enabled):
                loss, hierarchical_info = policy.forward(
                    batch, curriculum_info=curriculum_info)

            scaled_loss = loss / cfg.training.accumulation_steps

            if amp_enabled:
                scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            if stage_steps % cfg.training.accumulation_steps == 0:
                if amp_enabled:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                lr_scheduler.step()

            # è®°å½•æ—¥å¿—
            if stage_steps % cfg.training.log_freq == 0:
                writer.add_scalar(f"curriculum/{stage_name}/loss",
                                  scaled_loss.item(), current_step + stage_steps)

                # è®°å½•åˆ†å±‚ä¿¡æ¯
                if isinstance(hierarchical_info, dict):
                    for key, value in hierarchical_info.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(
                                f"curriculum/{stage_name}/{key}", value, current_step + stage_steps)

                epoch_bar.set_postfix(
                    loss=f"{scaled_loss.item():.3f}",
                    stage=stage_name
                )

            total_epoch_loss += scaled_loss.item()
            epoch_samples += 1
            stage_steps += 1

        # è®¡ç®—å¹³å‡epochæŸå¤±
        avg_epoch_loss = total_epoch_loss / max(epoch_samples, 1)
        print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}, æœ€ä½³æŸå¤±: {best_stage_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_stage_loss and output_directory is not None:
            print(f"ğŸ¯ å‘ç°æ›´å¥½çš„æ¨¡å‹! {best_stage_loss:.4f} â†’ {avg_epoch_loss:.4f}")
            best_stage_loss = avg_epoch_loss
            best_save_path = output_directory / f"curriculum_{stage_name}_best"
            try:
                policy.save_pretrained(best_save_path)
                save_rng_state(best_save_path / "rng_state.pth")
                print(f"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜æˆåŠŸ: {best_save_path}")
            except Exception as e:
                print(f"âŒ æœ€ä½³æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if output_directory is not None and (epoch + 1) % cfg.training.save_freq_epoch == 0:
            checkpoint_save_path = output_directory / f"curriculum_{stage_name}_epoch{epoch + 1}"
            try:
                policy.save_pretrained(checkpoint_save_path)
                save_rng_state(checkpoint_save_path / "rng_state.pth")
                print(f"âœ… å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {checkpoint_save_path}")
            except Exception as e:
                print(f"âŒ å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜è¯¦ç»†çŠ¶æ€
            if optimizer is not None and lr_scheduler is not None:
                try:
                    save_hierarchical_checkpoint(
                        policy, optimizer, lr_scheduler, scaler,
                        current_step + stage_steps, epoch + 1, best_stage_loss,
                        output_directory, amp_enabled, task_manager
                    )
                    print(f"âœ… åˆ†å±‚æ¶æ„çŠ¶æ€ä¿å­˜æˆåŠŸ")
                except Exception as e:
                    print(f"âŒ åˆ†å±‚æ¶æ„çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")

    # æµ‹è¯•è®­ç»ƒæ¨¡å¼ï¼šè‡ªåŠ¨ä¿å­˜
    if test_training_mode and output_directory is not None:
        test_save_path = output_directory / f"test_stage_{stage_name}_complete"
        print(f"ğŸ§ª TEST MODE: Auto-saving stage completion to {test_save_path}")
        try:
            policy.save_pretrained(test_save_path)
            print(f"âœ… Test stage model saved successfully")
        except Exception as e:
            print(f"âŒ Test stage model save failed: {e}")

    print(f"âœ… è¯¾ç¨‹é˜¶æ®µ {stage_name} å®Œæˆï¼Œæœ€ä½³æŸå¤±: {best_stage_loss:.4f}")
    return current_step + stage_steps


def save_hierarchical_checkpoint(policy, optimizer, lr_scheduler, scaler, steps, epoch, best_loss,
                                 output_directory, amp_enabled, task_manager=None):
    """ä¿å­˜åˆ†å±‚æ¶æ„æ£€æŸ¥ç‚¹ï¼ˆæ”¯æŒä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼‰"""
    print(f"ğŸ”§ å¼€å§‹ä¿å­˜åˆ†å±‚æ¶æ„æ£€æŸ¥ç‚¹...")
    print(f"   - steps: {steps}, epoch: {epoch}, best_loss: {best_loss:.4f}")

    # ä¿å­˜policy
    try:
        policy.save_pretrained(output_directory)
        save_rng_state(output_directory / "rng_state.pth")
        print(f"âœ… Policy ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Policy ä¿å­˜å¤±è´¥: {e}")
        return

    # ä¿å­˜è®­ç»ƒçŠ¶æ€
    try:
        checkpoint = {
            "optimizer": optimizer.state_dict(),
            "lr_scheduler": lr_scheduler.state_dict(),
            "scaler": scaler.state_dict() if amp_enabled else None,
            "steps": steps,
            "epoch": epoch,
            "best_loss": best_loss,
            "hierarchical_stats": policy.get_performance_stats() if hasattr(policy, 'get_performance_stats') else {},
            "layer_states": policy.get_layer_states() if hasattr(policy, 'get_layer_states') else {}
        }

        # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šä¿å­˜ä»»åŠ¡ç®¡ç†å™¨çŠ¶æ€
        if task_manager is not None:
            checkpoint["available_tasks"] = task_manager.available_tasks
            checkpoint["current_phase"] = task_manager.current_training_phase
            state_file = output_directory / "task_specific_learning_state.pth"
        else:
            state_file = output_directory / "hierarchical_learning_state.pth"

        torch.save(checkpoint, state_file)
        print(f"âœ… è®­ç»ƒçŠ¶æ€ä¿å­˜æˆåŠŸ: {state_file}")

        # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šä¿å­˜ä»»åŠ¡ç®¡ç†å™¨è¯¦ç»†çŠ¶æ€
        if task_manager is not None:
            task_manager.save_training_state(output_directory, epoch,
                                             policy.get_performance_stats() if hasattr(policy, 'get_performance_stats') else {})
            print(f"âœ… ä»»åŠ¡ç®¡ç†å™¨çŠ¶æ€ä¿å­˜æˆåŠŸ")

    except Exception as e:
        print(f"âŒ è®­ç»ƒçŠ¶æ€ä¿å­˜å¤±è´¥: {e}")


@hydra.main(config_path="../configs/policy/", config_name="humanoid_diffusion_config", version_base=None)
def main(cfg: DictConfig):
    """ç»Ÿä¸€åˆ†å±‚æ¶æ„è®­ç»ƒä¸»å‡½æ•°"""
    logger = setup_logging()
    set_seed(cfg.training.seed)

    # æ£€æŸ¥è®­ç»ƒæ¨¡å¼
    use_task_specific = cfg.get('task_specific_training', {}).get('enable', False)
    test_training_mode = cfg.training.get('test_training_mode', False)

    print("ğŸ¤– åˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyè®­ç»ƒ")
    print("=" * 70)
    print(f"ä»»åŠ¡: {cfg.task}")
    print(f"æ–¹æ³•: {cfg.method}")
    print(f"åˆ†å±‚æ¶æ„: {cfg.policy.get('use_hierarchical', False)}")
    print(f"è®­ç»ƒæ¨¡å¼: {'ä»»åŠ¡ç‰¹å®šæ¨¡å¼' if use_task_specific else 'åŸºç¡€æ¨¡å¼'}")

    if test_training_mode:
        test_epochs = cfg.training.get('test_training_epochs', 1)
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼å¯ç”¨ - æ¯ä¸ªé˜¶æ®µè¿è¡Œ{test_epochs}ä¸ªepoch")

    # éªŒè¯é…ç½®
    if not cfg.policy.get('use_hierarchical', False):
        logger.warning("âš ï¸  use_hierarchicalä¸ºFalseï¼Œå»ºè®®è®¾ä¸ºTrueä»¥ä½¿ç”¨åˆ†å±‚æ¶æ„")

    # è®¾ç½®è¾“å‡ºç›®å½•
    if use_task_specific:
        output_directory = Path(cfg.training.output_directory) / f"task_specific_run_{cfg.timestamp}"
    else:
        output_directory = Path(cfg.training.output_directory) / f"run_{cfg.timestamp}"

    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    device = torch.device(cfg.training.device)

    # åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨ï¼ˆä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼‰
    task_manager = None
    if use_task_specific:
        task_manager = TaskSpecificTrainingManager(cfg)

    # =================
    # æ•°æ®é›†åŠ è½½
    # =================
    datasets = {}
    dataset_metadatas = {}
    image_transforms = build_augmenter(cfg.training.RGB_Augmenter)

    if use_task_specific:
        # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šåŠ è½½ä»»åŠ¡æ•°æ®
        print("ğŸ” ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šæ£€æµ‹å¯ç”¨ä»»åŠ¡æ•°æ®...")

        # ä¸´æ—¶ï¼šåªå¤„ç†ä»»åŠ¡1ï¼Œä»rooté…ç½®è¯»å–
        task_id = 1
        task_data_path = cfg.get('root', '')

        if task_data_path and os.path.exists(task_data_path):
            try:
                task_repoid = f"lerobot/task_{task_id}"
                temp_dataset_metadata = LeRobotDatasetMetadata(task_repoid, root=task_data_path)

                task_manager.register_available_task(
                    task_id, temp_dataset_metadata.info["total_episodes"], task_data_path)
                print(f"âœ… æ£€æµ‹åˆ°ä»»åŠ¡{task_id}æ•°æ®: {temp_dataset_metadata.info['total_episodes']}ä¸ªepisodes")
            except Exception as e:
                print(f"âš ï¸  ä»»åŠ¡{task_id}æ•°æ®æ£€æµ‹å¤±è´¥: {e}")

        if not task_manager.available_tasks:
            logger.error("âŒ æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨çš„ä»»åŠ¡æ•°æ®")
            return

        # éªŒè¯è®­ç»ƒå‡†å¤‡çŠ¶æ€
        ready, issues = task_manager.validate_training_readiness()
        if not ready:
            logger.error("âŒ è®­ç»ƒå‡†å¤‡æœªå®Œæˆ:")
            for issue in issues:
                logger.error(f"   - {issue}")
            return

        task_manager.print_training_plan()

        # åŠ è½½ç¬¬ä¸€ä¸ªä»»åŠ¡çš„æ•°æ®é›†ï¼ˆè·å–ç‰¹å¾ï¼‰
        first_task_id = task_manager.available_tasks[0]
        temp_dataset_metadata = LeRobotDatasetMetadata(
            f"lerobot/task_{first_task_id}", root=task_data_path)

        features = dataset_to_policy_features(temp_dataset_metadata.features)
        input_features = {k: ft for k, ft in features.items() if ft.type is not FeatureType.ACTION}
        output_features = {k: ft for k, ft in features.items() if ft.type is FeatureType.ACTION}

        # æ„å»ºpolicyé…ç½®
        policy_cfg = build_policy_config(cfg, input_features, output_features)

        # åŠ è½½å®é™…æ•°æ®é›†
        first_dataset, first_metadata = load_task_dataset(
            first_task_id, cfg, policy_cfg, image_transforms)

        if first_dataset is None:
            logger.error(f"âŒ æ— æ³•åŠ è½½ä»»åŠ¡{first_task_id}æ•°æ®")
            return

        datasets[first_task_id] = first_dataset
        dataset_metadatas[first_task_id] = first_metadata

        # è®¡ç®—æ€»å¸§æ•°
        total_frames = sum(metadata.info["total_frames"] for metadata in dataset_metadatas.values())
        dataset_stats = first_metadata.stats

    else:
        # åŸºç¡€æ¨¡å¼ï¼šç›´æ¥åŠ è½½æ•°æ®é›†
        print("ğŸ“‚ åŸºç¡€æ¨¡å¼ï¼šåŠ è½½æ•°æ®é›†...")
        dataset_metadata = LeRobotDatasetMetadata(cfg.repoid, root=cfg.root)
        print("Camera keys:", dataset_metadata.camera_keys)
        print("Original dataset features:", dataset_metadata.features)

        features = dataset_to_policy_features(dataset_metadata.features)
        input_features = {k: ft for k, ft in features.items() if ft.type is not FeatureType.ACTION}
        output_features = {k: ft for k, ft in features.items() if ft.type is FeatureType.ACTION}

        # æ„å»ºpolicyé…ç½®
        policy_cfg = build_policy_config(cfg, input_features, output_features)

        # Episodeé™åˆ¶
        episodes_to_use = getattr(cfg, 'episodes_to_use', None)
        if episodes_to_use is not None:
            if isinstance(episodes_to_use, int):
                episodes_to_use = list(range(episodes_to_use))
            elif hasattr(episodes_to_use, '__len__') and len(episodes_to_use) == 2:
                start, end = int(episodes_to_use[0]), int(episodes_to_use[1])
                episodes_to_use = list(range(start, end + 1))
                print(f"ä½¿ç”¨episodesèŒƒå›´ [{start}, {end}]: {len(episodes_to_use)}ä¸ªepisodes")
            elif hasattr(episodes_to_use, '__iter__'):
                episodes_to_use = list(episodes_to_use)
            print(f"ä½¿ç”¨é™å®šepisodes: {len(episodes_to_use)}ä¸ª")
        else:
            print("ä½¿ç”¨å…¨éƒ¨episodes")

        delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)
        dataset = LeRobotDataset(
            cfg.repoid,
            delta_timestamps=delta_timestamps,
            root=cfg.root,
            episodes=episodes_to_use,
            image_transforms=image_transforms,
        )

        total_frames = dataset_metadata.info["total_frames"]
        dataset_stats = dataset_metadata.stats

    # =================
    # æ„å»ºPolicy
    # =================
    policy = build_hierarchical_policy(policy_cfg, dataset_stats)
    optimizer, lr_scheduler = build_optimizer_and_scheduler(policy, cfg, total_frames)

    # AMPæ”¯æŒ
    amp_requested = bool(getattr(cfg.policy, "use_amp", False))
    amp_enabled = amp_requested and device.type == "cuda"

    has_torch_autocast = hasattr(torch, "autocast")

    def make_autocast(enabled: bool):
        if not enabled:
            return nullcontext()
        if device.type == "cuda":
            if has_torch_autocast:
                return torch.autocast(device_type="cuda")
            else:
                from torch.cuda.amp import autocast as cuda_autocast
                return cuda_autocast()
        return nullcontext()

    scaler = torch.amp.GradScaler(device=device.type, enabled=amp_enabled) if hasattr(
        torch, "amp") else torch.cuda.amp.GradScaler(enabled=amp_enabled)

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    steps = 0
    best_loss = float('inf')

    # æ¢å¤è®­ç»ƒé€»è¾‘
    if cfg.training.resume and cfg.training.resume_timestamp:
        resume_path = Path(cfg.training.output_directory) / cfg.training.resume_timestamp
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_path}")
        try:
            load_rng_state(resume_path / "rng_state.pth")
            policy = policy.from_pretrained(resume_path, strict=True)

            optimizer, lr_scheduler = build_optimizer_and_scheduler(policy, cfg, total_frames)

            # æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_file = resume_path / "task_specific_learning_state.pth"
            if not checkpoint_file.exists():
                checkpoint_file = resume_path / "hierarchical_learning_state.pth"
            if not checkpoint_file.exists():
                checkpoint_file = resume_path / "learning_state.pth"

            checkpoint = torch.load(checkpoint_file, map_location=device)
            optimizer.load_state_dict(checkpoint["optimizer"])

            if "lr_scheduler" in checkpoint:
                lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
            if "scaler" in checkpoint and amp_enabled:
                scaler.load_state_dict(checkpoint["scaler"])
            if "steps" in checkpoint:
                steps = checkpoint["steps"]
            if "epoch" in checkpoint:
                start_epoch = checkpoint["epoch"]
            if "best_loss" in checkpoint:
                best_loss = checkpoint["best_loss"]

            # æ¢å¤åˆ†å±‚æ¶æ„çŠ¶æ€
            if "layer_states" in checkpoint and hasattr(policy, 'load_layer_states'):
                policy.load_layer_states(checkpoint["layer_states"])

            # æ¢å¤ä»»åŠ¡ç‰¹å®šçŠ¶æ€
            if use_task_specific and task_manager:
                if "available_tasks" in checkpoint:
                    for task_id in checkpoint["available_tasks"]:
                        if task_id not in task_manager.available_tasks:
                            task_manager.available_tasks.append(task_id)
                if "current_phase" in checkpoint:
                    task_manager.current_training_phase = checkpoint["current_phase"]

                state_file = resume_path / "task_training_state.json"
                if state_file.exists():
                    task_manager.load_training_state(state_file)

            for file in resume_path.glob("events.*"):
                shutil.copy(file, output_directory)

            print(f"å·²æ¢å¤è®­ç»ƒä»epoch {start_epoch}, step {steps}")
        except Exception as e:
            print(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return
    else:
        print("ä»å¤´å¼€å§‹è®­ç»ƒ!")

    policy.train().to(device)
    print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in policy.parameters()):,}")
    print(f"ä½¿ç”¨AMP: {amp_enabled}")

    # æ‰“å°åˆ†å±‚æ¶æ„ä¿¡æ¯
    if hasattr(policy, 'print_architecture_summary'):
        policy.print_architecture_summary()

    # =================
    # è¯¾ç¨‹å­¦ä¹ 
    # =================
    if use_task_specific:
        # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šä½¿ç”¨TaskManagerçš„è¯¾ç¨‹å­¦ä¹ é…ç½®
        curriculum_stages = task_manager.get_current_curriculum_stages()
        use_curriculum = bool(curriculum_stages)
    else:
        # åŸºç¡€æ¨¡å¼ï¼šä»é…ç½®è¯»å–è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
        curriculum_config = cfg.policy.hierarchical.get('curriculum_learning', {}) if hasattr(
            cfg.policy, 'hierarchical') else {}
        use_curriculum = curriculum_config.get('enable', False)
        stages_config = curriculum_config.get('stages') or curriculum_config.get('universal_stages')
        curriculum_stages = stages_config if use_curriculum else {}

    if use_curriculum and curriculum_stages:
        print(f"ğŸ“ å¯åŠ¨è¯¾ç¨‹å­¦ä¹ ï¼Œå…±{len(curriculum_stages)}ä¸ªé˜¶æ®µ")

        current_step = steps
        for stage_name, stage_config in curriculum_stages.items():
            if use_task_specific:
                # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šä½¿ç”¨ä»»åŠ¡ç‰¹å®šæ•°æ®åŠ è½½å™¨
                dataloader = create_task_specific_dataloader(datasets, task_manager, cfg, device)
                current_step = run_curriculum_learning_stage(
                    policy, stage_config, None, cfg, device, writer, current_step,
                    optimizer, lr_scheduler, scaler, output_directory, amp_enabled,
                    task_manager=task_manager, dataloader=dataloader
                )
            else:
                # åŸºç¡€æ¨¡å¼ï¼šä½¿ç”¨å•ä¸€æ•°æ®é›†
                current_step = run_curriculum_learning_stage(
                    policy, stage_config, dataset, cfg, device, writer, current_step,
                    optimizer, lr_scheduler, scaler, output_directory, amp_enabled
                )

        print("âœ… è¯¾ç¨‹å­¦ä¹ å®Œæˆï¼Œå¼€å§‹å®Œæ•´è®­ç»ƒ...")
        steps = current_step

    # =================
    # ä¸»è®­ç»ƒå¾ªç¯
    # =================
    print("ğŸš€ å¼€å§‹ä¸»è¦è®­ç»ƒå¾ªç¯...")
    for epoch in range(start_epoch, cfg.training.max_epoch):
        if use_task_specific:
            dataloader = create_task_specific_dataloader(datasets, task_manager, cfg, device)
        else:
            dataloader = DataLoader(
                dataset,
                num_workers=cfg.training.num_workers,
                batch_size=cfg.training.batch_size,
                shuffle=True,
                pin_memory=(device.type != "cpu"),
                drop_last=cfg.training.drop_last,
                prefetch_factor=1,
            )

        epoch_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch+1}/{cfg.training.max_epoch}",
            dynamic_ncols=True,
            leave=False)

        total_loss = 0.0
        for batch in epoch_bar:
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šè·å–ä»»åŠ¡æƒé‡
            if use_task_specific and task_manager:
                task_loss_weights = task_manager.get_task_specific_loss_weights(batch)
                with make_autocast(amp_enabled):
                    loss, hierarchical_info = policy.forward(batch, task_weights=task_loss_weights)
            else:
                with make_autocast(amp_enabled):
                    loss, hierarchical_info = policy.forward(batch)

            scaled_loss = loss / cfg.training.accumulation_steps

            if amp_enabled:
                scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            if steps % cfg.training.accumulation_steps == 0:
                if amp_enabled:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                lr_scheduler.step()

            # è®°å½•è®­ç»ƒæ—¥å¿—
            if steps % cfg.training.log_freq == 0:
                writer.add_scalar("train/loss", scaled_loss.item(), steps)
                writer.add_scalar("train/lr", lr_scheduler.get_last_lr()[0], steps)

                # è®°å½•åˆ†å±‚æ¶æ„ç»Ÿè®¡ä¿¡æ¯
                if isinstance(hierarchical_info, dict):
                    for key, value in hierarchical_info.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(f"hierarchical/{key}", value, steps)

                # è®°å½•å±‚æ€§èƒ½ç»Ÿè®¡
                if hasattr(policy, 'get_performance_stats'):
                    perf_stats = policy.get_performance_stats()
                    for layer_name, stats in perf_stats.items():
                        if isinstance(stats, dict):
                            for stat_name, stat_value in stats.items():
                                if isinstance(stat_value, (int, float)):
                                    writer.add_scalar(
                                        f"performance/{layer_name}/{stat_name}", stat_value, steps)

                epoch_bar.set_postfix(
                    loss=f"{scaled_loss.item():.3f}",
                    step=steps,
                    lr=lr_scheduler.get_last_lr()[0]
                )

            steps += 1
            total_loss += scaled_loss.item()

        # æ›´æ–°æœ€ä½³æŸå¤±
        if total_loss < best_loss:
            best_loss = total_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            save_rng_state(best_path / "rng_state.pth")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / f"epoch{epoch+1}"
            policy.save_pretrained(epoch_path)
            save_rng_state(epoch_path / "rng_state.pth")

        # ä¿å­˜æœ€æ–°çš„åˆ†å±‚æ¶æ„æ£€æŸ¥ç‚¹
        save_hierarchical_checkpoint(
            policy, optimizer, lr_scheduler, scaler, steps, epoch + 1, best_loss,
            output_directory, amp_enabled, task_manager
        )

    writer.close()

    # è®­ç»ƒå®Œæˆåæ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 70)

    if hasattr(policy, 'get_performance_stats'):
        final_stats = policy.get_performance_stats()
        print("æœ€ç»ˆå±‚æ€§èƒ½ç»Ÿè®¡:")
        for layer_name, stats in final_stats.items():
            print(f"  {layer_name}: {stats}")

    # ä»»åŠ¡ç‰¹å®šæ¨¡å¼ï¼šæ‰“å°ä»»åŠ¡è®­ç»ƒæ€»ç»“
    if use_task_specific and task_manager:
        training_summary = task_manager.get_training_summary()
        print(f"\nğŸ“Š ä»»åŠ¡è®­ç»ƒæ€»ç»“:")
        print(f"   è®­ç»ƒé˜¶æ®µ: {training_summary['current_phase']}")
        print(f"   å¤„ç†ä»»åŠ¡: {len(training_summary['available_tasks'])}")
        print(f"   æ€»episodes: {training_summary['total_episodes']}")

    print(f"   æœ€ç»ˆæŸå¤±: {best_loss:.4f}")


if __name__ == "__main__":
    main()
