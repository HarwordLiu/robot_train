# -*- coding: utf-8 -*-
"""
åˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyä¸“ç”¨è®­ç»ƒè„šæœ¬

è¿™ä¸ªæ–‡ä»¶ä¸“é—¨ç”¨äºè®­ç»ƒåˆ†å±‚æ¶æ„çš„Diffusion Policyï¼Œä¸ä¼ ç»Ÿtrain_policy.pyåˆ†ç¦»ï¼Œ
é¿å…å¹²æ‰°ç°æœ‰çš„è®­ç»ƒæ¨¡å¼ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
python kuavo_train/train_hierarchical_policy.py --config-name=humanoid_diffusion_config
"""

# Ensure custom patches are applied, DON'T REMOVE THIS LINE!
import lerobot_patches.custom_patches
from lerobot.configs.policies import PolicyFeature
from typing import Any

import hydra
from omegaconf import DictConfig, OmegaConf, ListConfig
from pathlib import Path
from functools import partial

import torch
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader
from tqdm import tqdm
import shutil
from hydra.utils import instantiate
from diffusers.optimization import get_scheduler

from lerobot.configs.types import FeatureType, NormalizationMode
from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata, LeRobotDataset
from lerobot.datasets.utils import dataset_to_policy_features
from lerobot.utils.random_utils import set_seed

# å¯¼å…¥åˆ†å±‚æ¶æ„æ¨¡å—
from kuavo_train.wrapper.policy.humanoid.HumanoidDiffusionPolicy import HumanoidDiffusionPolicy
from kuavo_train.wrapper.dataset.LeRobotDatasetWrapper import CustomLeRobotDataset
from kuavo_train.utils.augmenter import crop_image, resize_image, DeterministicAugmenterColor
from kuavo_train.utils.utils import save_rng_state, load_rng_state
from diffusers.optimization import get_scheduler
from utils.transforms import ImageTransforms, ImageTransformsConfig, ImageTransformConfig

from functools import partial
from contextlib import nullcontext


def build_augmenter(cfg):
    """æ„å»ºå›¾åƒå¢å¼ºå™¨ - å¤ç”¨train_policy.pyçš„å®ç°"""
    img_tf_cfg = ImageTransformsConfig(
        enable=cfg.get("enable", False),
        max_num_transforms=cfg.get("max_num_transforms", 3),
        random_order=cfg.get("random_order", False),
        tfs={}
    )

    if "tfs" in cfg:
        for name, tf_dict in cfg["tfs"].items():
            img_tf_cfg.tfs[name] = ImageTransformConfig(
                weight=tf_dict.get("weight", 1.0),
                type=tf_dict.get("type", "Identity"),
                kwargs=tf_dict.get("kwargs", {}),
            )
    return ImageTransforms(img_tf_cfg)


def build_delta_timestamps(dataset_metadata, policy_cfg):
    """æ„å»ºdelta timestamps - å¤ç”¨train_policy.pyçš„å®ç°"""
    obs_indices = getattr(policy_cfg, "observation_delta_indices", None)
    act_indices = getattr(policy_cfg, "action_delta_indices", None)
    if obs_indices is None and act_indices is None:
        return None

    delta_timestamps = {}
    for key in dataset_metadata.info["features"]:
        if "observation" in key and obs_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in obs_indices]
        elif "action" in key and act_indices is not None:
            delta_timestamps[key] = [
                i / dataset_metadata.fps for i in act_indices]

    return delta_timestamps if delta_timestamps else None


def build_optimizer_and_scheduler(policy, cfg, total_frames):
    """æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ - å¤ç”¨train_policy.pyçš„å®ç°"""
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())

    if cfg.training.max_training_step is None:
        updates_per_epoch = (
            total_frames // (cfg.training.batch_size * cfg.training.accumulation_steps)) + 1
        num_training_steps = cfg.training.max_epoch * updates_per_epoch
    else:
        num_training_steps = cfg.training.max_training_step

    lr_scheduler = policy.config.get_scheduler_preset()
    if lr_scheduler is not None:
        lr_scheduler = lr_scheduler.build(optimizer, num_training_steps)
    else:
        lr_scheduler = get_scheduler(
            name=cfg.training.scheduler_name,
            optimizer=optimizer,
            num_warmup_steps=cfg.training.scheduler_warmup_steps,
            num_training_steps=num_training_steps,
        )

    return optimizer, lr_scheduler


def build_policy_config(cfg, input_features, output_features):
    """æ„å»ºpolicyé…ç½® - å¤ç”¨train_policy.pyçš„å®ç°"""
    def _normalize_feature_dict(d: Any) -> dict[str, PolicyFeature]:
        if isinstance(d, DictConfig):
            d = OmegaConf.to_container(d, resolve=True)
        if not isinstance(d, dict):
            raise TypeError(
                "Expected dict or DictConfig, got {}".format(type(d)))

        return {
            k: PolicyFeature(**v) if isinstance(v,
                                                dict) and not isinstance(v, PolicyFeature) else v
            for k, v in d.items()
        }

    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=cfg.training.device,
    )

    policy_cfg.input_features = _normalize_feature_dict(
        policy_cfg.input_features)
    policy_cfg.output_features = _normalize_feature_dict(
        policy_cfg.output_features)
    return policy_cfg


def build_hierarchical_policy(policy_cfg, dataset_stats):
    """æ„å»ºåˆ†å±‚æ¶æ„çš„policy"""
    print("ğŸ¤– Building HumanoidDiffusionPolicy with hierarchical architecture...")
    return HumanoidDiffusionPolicy(policy_cfg, dataset_stats)


def run_curriculum_learning_stage(policy, stage_config, dataset, cfg, device, writer, current_step,
                                  optimizer=None, lr_scheduler=None, scaler=None, output_directory=None, amp_enabled=False):
    """è¿è¡Œè¯¾ç¨‹å­¦ä¹ çš„å•ä¸ªé˜¶æ®µ"""
    stage_name = stage_config.get("name", "unknown")
    enabled_layers = stage_config.get("layers", [])
    stage_epochs = stage_config.get("epochs", 10)

    # æµ‹è¯•è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æµ‹è¯•epochæ•°é‡
    test_training_mode = cfg.training.get('test_training_mode', False)
    if test_training_mode:
        original_epochs = stage_epochs
        test_epochs = cfg.training.get('test_training_epochs', 1)
        stage_epochs = test_epochs
        print(f"ğŸ§ª TEST MODE: Overriding {stage_name} stage epochs from {original_epochs} to {test_epochs}")

    print("ğŸ“ Starting curriculum stage: {} (layers: {}, epochs: {})".format(
        stage_name, enabled_layers, stage_epochs))
    print(f"ğŸ”§ è¯¾ç¨‹å­¦ä¹ é˜¶æ®µå‚æ•°:")
    print(f"   - output_directory: {output_directory}")
    print(f"   - optimizer: {optimizer is not None}")
    print(f"   - lr_scheduler: {lr_scheduler is not None}")
    print(f"   - scaler: {scaler is not None}")
    print(f"   - amp_enabled: {amp_enabled}")
    print(f"   - save_freq_epoch: {cfg.training.save_freq_epoch}")

    # æ¿€æ´»æŒ‡å®šçš„å±‚
    if hasattr(policy, 'set_curriculum_stage'):
        policy.set_curriculum_stage(enabled_layers)

    # ä¸ºè¿™ä¸ªé˜¶æ®µåˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        num_workers=cfg.training.num_workers,
        batch_size=cfg.training.batch_size,
        shuffle=True,
        pin_memory=(device.type != "cpu"),
        drop_last=cfg.training.drop_last,
        prefetch_factor=1,
    )

    stage_steps = 0
    best_stage_loss = float('inf')

    for epoch in range(stage_epochs):
        print(f"ğŸš€ å¼€å§‹ Epoch {epoch+1}/{stage_epochs}")
        epoch_bar = tqdm(
            dataloader,
            desc="Stage {} Epoch {}/{}".format(stage_name, epoch+1, stage_epochs),
            dynamic_ncols=True,
            leave=False)

        total_epoch_loss = 0.0
        epoch_samples = 0

        for batch in epoch_bar:
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            # å‰å‘ä¼ æ’­æ—¶åŒ…å«è¯¾ç¨‹å­¦ä¹ ä¿¡æ¯
            loss, _ = policy.forward(batch, curriculum_info={
                'stage': stage_name,
                'enabled_layers': enabled_layers
            })

            # è®°å½•æ—¥å¿—
            if stage_steps % cfg.training.log_freq == 0:
                writer.add_scalar("curriculum/{}/loss".format(stage_name),
                                  loss.item(), current_step + stage_steps)
                epoch_bar.set_postfix(loss="{:.3f}".format(
                    loss.item()), stage=stage_name)

            total_epoch_loss += loss.item()
            epoch_samples += 1
            stage_steps += 1

        print(f"ğŸ Epoch {epoch+1} è®­ç»ƒå®Œæˆï¼Œå¼€å§‹ä¿å­˜æ£€æŸ¥ç‚¹...")
        # è®¡ç®—å¹³å‡epochæŸå¤±
        avg_epoch_loss = total_epoch_loss / max(epoch_samples, 1)
        print(
            f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}, å½“å‰æœ€ä½³æŸå¤±: {best_stage_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_stage_loss and output_directory is not None:
            print(
                f"ğŸ¯ å‘ç°æ›´å¥½çš„æ¨¡å‹! æŸå¤±ä» {best_stage_loss:.4f} æ”¹å–„åˆ° {avg_epoch_loss:.4f}")
            best_stage_loss = avg_epoch_loss
            best_save_path = output_directory / \
                "curriculum_{}_best".format(stage_name)
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_save_path}")
            try:
                policy.save_pretrained(best_save_path)
                save_rng_state(best_save_path / "rng_state.pth")
                print(f"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜æˆåŠŸ: {best_save_path}")
            except Exception as e:
                print(f"âŒ æœ€ä½³æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        elif avg_epoch_loss >= best_stage_loss:
            print(
                f"ğŸ“‰ æŸå¤±æœªæ”¹å–„ï¼Œè·³è¿‡æœ€ä½³æ¨¡å‹ä¿å­˜ (å½“å‰: {avg_epoch_loss:.4f}, æœ€ä½³: {best_stage_loss:.4f})")
        elif output_directory is None:
            print(f"âš ï¸  output_directory ä¸º Noneï¼Œè·³è¿‡æœ€ä½³æ¨¡å‹ä¿å­˜")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        print(
            f"ğŸ” æ£€æŸ¥å®šæœŸä¿å­˜æ¡ä»¶: output_directory={output_directory is not None}, epoch={epoch+1}, save_freq_epoch={cfg.training.save_freq_epoch}")
        if output_directory is not None and (epoch + 1) % cfg.training.save_freq_epoch == 0:
            checkpoint_save_path = output_directory / \
                "curriculum_{}_epoch{}".format(stage_name, epoch + 1)
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹åˆ°: {checkpoint_save_path}")
            try:
                policy.save_pretrained(checkpoint_save_path)
                save_rng_state(checkpoint_save_path / "rng_state.pth")
                print(f"âœ… å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {checkpoint_save_path}")
            except Exception as e:
                print(f"âŒ å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜è¯¾ç¨‹å­¦ä¹ é˜¶æ®µçš„è¯¦ç»†çŠ¶æ€
            if optimizer is not None and lr_scheduler is not None:
                print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†å±‚æ¶æ„è¯¦ç»†çŠ¶æ€...")
                try:
                    save_hierarchical_checkpoint(
                        policy, optimizer, lr_scheduler, scaler,
                        current_step + stage_steps, epoch + 1, best_stage_loss,
                        output_directory, amp_enabled
                    )
                    print(f"âœ… åˆ†å±‚æ¶æ„çŠ¶æ€ä¿å­˜æˆåŠŸ")
                except Exception as e:
                    print(f"âŒ åˆ†å±‚æ¶æ„çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
            else:
                print(f"âš ï¸  optimizer æˆ– lr_scheduler ä¸º Noneï¼Œè·³è¿‡è¯¦ç»†çŠ¶æ€ä¿å­˜")
        else:
            print(
                f"â­ï¸  è·³è¿‡å®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜ (epoch {epoch+1} ä¸æ˜¯ {cfg.training.save_freq_epoch} çš„å€æ•°)")

        print(f"âœ… Epoch {epoch+1} æ£€æŸ¥ç‚¹ä¿å­˜å®Œæˆ")

    # æµ‹è¯•è®­ç»ƒæ¨¡å¼ï¼šåœ¨æ¯ä¸ªé˜¶æ®µç»“æŸåè‡ªåŠ¨ä¿å­˜æ¨¡å‹
    if test_training_mode and output_directory is not None:
        test_save_path = output_directory / f"test_stage_{stage_name}_complete"
        print(f"ğŸ§ª TEST MODE: Auto-saving stage completion to {test_save_path}")
        try:
            policy.save_pretrained(test_save_path)
            print(f"âœ… Test stage model saved successfully: {test_save_path}")
        except Exception as e:
            print(f"âŒ Test stage model save failed: {e}")

    print("âœ… Completed curriculum stage: {} (best loss: {:.4f})".format(
        stage_name, best_stage_loss))
    return current_step + stage_steps


def save_hierarchical_checkpoint(policy, optimizer, lr_scheduler, scaler, steps, epoch, best_loss,
                                 output_directory, amp_enabled):
    """ä¿å­˜åˆ†å±‚æ¶æ„ä¸“ç”¨çš„æ£€æŸ¥ç‚¹"""
    print(f"ğŸ”§ å¼€å§‹ä¿å­˜åˆ†å±‚æ¶æ„æ£€æŸ¥ç‚¹...")
    print(f"   - output_directory: {output_directory}")
    print(f"   - steps: {steps}, epoch: {epoch}, best_loss: {best_loss:.4f}")
    print(f"   - amp_enabled: {amp_enabled}")

    # ä¿å­˜policy
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ policy åˆ°: {output_directory}")
    try:
        policy.save_pretrained(output_directory)
        save_rng_state(output_directory / "rng_state.pth")
        print(f"âœ… Policy ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Policy ä¿å­˜å¤±è´¥: {e}")
        return

    # ä¿å­˜åˆ†å±‚æ¶æ„ç‰¹æœ‰çš„çŠ¶æ€
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜è®­ç»ƒçŠ¶æ€...")
    try:
        checkpoint = {
            "optimizer": optimizer.state_dict(),
            "lr_scheduler": lr_scheduler.state_dict(),
            "scaler": scaler.state_dict() if amp_enabled else None,
            "steps": steps,
            "epoch": epoch,
            "best_loss": best_loss,
            # ä¿å­˜åˆ†å±‚æ¶æ„ç‰¹æœ‰çš„çŠ¶æ€
            "hierarchical_stats": policy.get_performance_stats() if hasattr(policy, 'get_performance_stats') else {},
            "layer_states": policy.get_layer_states() if hasattr(policy, 'get_layer_states') else {}
        }

        state_file = output_directory / "hierarchical_learning_state.pth"
        torch.save(checkpoint, state_file)
        print(f"âœ… è®­ç»ƒçŠ¶æ€ä¿å­˜æˆåŠŸ: {state_file}")

        # ä¿å­˜éšæœºæ•°çŠ¶æ€
        rng_file = output_directory / "rng_state.pth"
        save_rng_state(rng_file)
        print(f"âœ… éšæœºæ•°çŠ¶æ€ä¿å­˜æˆåŠŸ: {rng_file}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒçŠ¶æ€ä¿å­˜å¤±è´¥: {e}")


@hydra.main(config_path="../configs/policy/", config_name="humanoid_diffusion_config", version_base=None)
def main(cfg: DictConfig):
    """åˆ†å±‚æ¶æ„è®­ç»ƒä¸»å‡½æ•°"""
    set_seed(cfg.training.seed)

    # æ£€æŸ¥æµ‹è¯•è®­ç»ƒæ¨¡å¼
    test_training_mode = cfg.training.get('test_training_mode', False)

    print("ğŸ¤– Hierarchical Humanoid Diffusion Policy Training")
    print("=" * 60)
    print("Config: {}".format(cfg.defaults))
    print("Use hierarchical: {}".format(
        cfg.policy.get('use_hierarchical', False)))

    if test_training_mode:
        test_epochs = cfg.training.get('test_training_epochs', 1)
        print(f"ğŸ§ª TEST TRAINING MODE ENABLED - Running {test_epochs} epoch(s) per stage for quick validation")
        print(f"âš¡ All curriculum stages will be reduced to {test_epochs} epoch(s)")
        print("ğŸ’¾ Automatic saving enabled after each stage")

    # éªŒè¯åˆ†å±‚æ¶æ„é…ç½®
    if not cfg.policy.get('use_hierarchical', False):
        print("âš ï¸  Warning: use_hierarchical is False. Set it to True in config to use hierarchical architecture.")

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_directory = Path(cfg.training.output_directory) / \
        "run_{}".format(cfg.timestamp)
    output_directory.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(output_directory))

    device = torch.device(cfg.training.device)

    # åŠ è½½æ•°æ®é›†å…ƒæ•°æ®å’Œç‰¹å¾
    dataset_metadata = LeRobotDatasetMetadata(cfg.repoid, root=cfg.root)
    print("Camera keys:", dataset_metadata.camera_keys)
    print("Original dataset features:", dataset_metadata.features)

    features = dataset_to_policy_features(dataset_metadata.features)
    input_features = {k: ft for k, ft in features.items(
    ) if ft.type is not FeatureType.ACTION}
    output_features = {k: ft for k,
                       ft in features.items() if ft.type is FeatureType.ACTION}

    print("Input features: {}".format(input_features))
    print("Output features: {}".format(output_features))

    # æ„å»ºåˆ†å±‚policyé…ç½®
    policy_cfg = build_policy_config(cfg, input_features, output_features)
    print("Policy config:", policy_cfg)

    # æ„å»ºåˆ†å±‚architectureçš„policy
    policy = build_hierarchical_policy(
        policy_cfg, dataset_stats=dataset_metadata.stats)
    optimizer, lr_scheduler = build_optimizer_and_scheduler(
        policy, cfg, dataset_metadata.info["total_frames"])

    # AMPæ”¯æŒ - å¤ç”¨train_policy.pyçš„å®ç°
    amp_requested = bool(getattr(cfg.policy, "use_amp", False))
    amp_enabled = amp_requested and device.type == "cuda"

    has_torch_autocast = hasattr(torch, "autocast")

    def make_autocast(enabled: bool):
        if not enabled:
            return nullcontext()
        if device.type == "cuda":
            if has_torch_autocast:
                return torch.autocast(device_type="cuda")
            else:
                from torch.cuda.amp import autocast as cuda_autocast
                return cuda_autocast()
        return nullcontext()

    scaler = torch.amp.GradScaler(device=device.type, enabled=amp_enabled) if hasattr(
        torch, "amp") else torch.cuda.amp.GradScaler(device=device.type, enabled=amp_enabled)

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    steps = 0
    best_loss = float('inf')

    # æ¢å¤è®­ç»ƒé€»è¾‘ - ä¸“ç”¨äºåˆ†å±‚æ¶æ„
    if cfg.training.resume and cfg.training.resume_timestamp:
        resume_path = Path(cfg.training.output_directory) / \
            cfg.training.resume_timestamp
        print("Resuming hierarchical training from:", resume_path)
        try:
            load_rng_state(resume_path / "rng_state.pth")
            policy = policy.from_pretrained(resume_path, strict=True)

            optimizer, lr_scheduler = build_optimizer_and_scheduler(
                policy, cfg, dataset_metadata.info["total_frames"])

            # åŠ è½½åˆ†å±‚æ¶æ„ä¸“ç”¨çš„çŠ¶æ€
            checkpoint_file = resume_path / "hierarchical_learning_state.pth"
            if not checkpoint_file.exists():
                checkpoint_file = resume_path / "learning_state.pth"  # å›é€€åˆ°æ ‡å‡†æ–‡ä»¶

            checkpoint = torch.load(checkpoint_file, map_location=device)
            optimizer.load_state_dict(checkpoint["optimizer"])

            if "lr_scheduler" in checkpoint:
                lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
            if "scaler" in checkpoint and amp_enabled:
                scaler.load_state_dict(checkpoint["scaler"])
            if "steps" in checkpoint:
                steps = checkpoint["steps"]
            if "epoch" in checkpoint:
                start_epoch = checkpoint["epoch"]
            if "best_loss" in checkpoint:
                best_loss = checkpoint["best_loss"]

            # æ¢å¤åˆ†å±‚æ¶æ„ç‰¹æœ‰çš„çŠ¶æ€
            if "layer_states" in checkpoint and hasattr(policy, 'load_layer_states'):
                policy.load_layer_states(checkpoint["layer_states"])

            for file in resume_path.glob("events.*"):
                shutil.copy(file, output_directory)

            print("Resumed hierarchical training from epoch {}, step {}".format(
                start_epoch, steps))
        except Exception as e:
            print("Failed to load hierarchical checkpoint:", e)
            return
    else:
        print("Training hierarchical architecture from scratch!")
        # åˆå§‹åŒ–optimizerå’Œlr_schedulerï¼ˆéresumeæƒ…å†µï¼‰
        optimizer, lr_scheduler = build_optimizer_and_scheduler(
            policy, cfg, dataset_metadata.info["total_frames"])

    policy.train().to(device)
    print("Total parameters: {:,}".format(
        sum(p.numel() for p in policy.parameters())))
    print("Using AMP: {}".format(amp_enabled))

    # æ‰“å°åˆ†å±‚æ¶æ„ä¿¡æ¯
    if hasattr(policy, 'print_architecture_summary'):
        policy.print_architecture_summary()

    # æ„å»ºæ•°æ®é›†
    delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)
    image_transforms = build_augmenter(cfg.training.RGB_Augmenter)

    # Episodeé™åˆ¶é€»è¾‘ - å¤ç”¨train_policy.pyçš„å®ç°
    episodes_to_use = getattr(cfg, 'episodes_to_use', None)
    print("Raw episodes_to_use from config: {}, type: {}".format(
        episodes_to_use, type(episodes_to_use)))
    if episodes_to_use is not None:
        if isinstance(episodes_to_use, int):
            episodes_to_use = list(range(episodes_to_use))
        elif hasattr(episodes_to_use, '__len__') and len(episodes_to_use) == 2:
            start, end = int(episodes_to_use[0]), int(episodes_to_use[1])
            episodes_to_use = list(range(start, end + 1))
            print("Converted range [{}, {}] to {} episodes".format(
                start, end, len(episodes_to_use)))
        elif hasattr(episodes_to_use, '__iter__'):
            episodes_to_use = list(episodes_to_use)
        print("Using limited episodes for memory efficiency: {} episodes".format(
            len(episodes_to_use)))
    else:
        episodes_to_use = None
        print("Using all available episodes")

    dataset = LeRobotDataset(
        cfg.repoid,
        delta_timestamps=delta_timestamps,
        root=cfg.root,
        episodes=episodes_to_use,
        image_transforms=image_transforms,
    )

    # è¯¾ç¨‹å­¦ä¹ æ”¯æŒ
    curriculum_config = cfg.get('curriculum_learning', {})
    use_curriculum = curriculum_config.get('enable', False)

    # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ é…ç½®ï¼Œæ”¯æŒ 'stages' æˆ– 'universal_stages'
    stages_config = curriculum_config.get(
        'stages') or curriculum_config.get('universal_stages')

    if use_curriculum and stages_config:
        print("ğŸ“ Starting curriculum learning with {} stages".format(
            len(stages_config)))

        # è¿è¡Œè¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
        current_step = steps
        for stage_name, stage_config in stages_config.items():
            current_step = run_curriculum_learning_stage(
                policy, stage_config, dataset, cfg, device, writer, current_step,
                optimizer, lr_scheduler, scaler, output_directory, amp_enabled
            )

        print("âœ… Curriculum learning completed. Starting full training...")
        steps = current_step

    # ä¸»è¦è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, cfg.training.max_epoch):
        dataloader = DataLoader(
            dataset,
            num_workers=cfg.training.num_workers,
            batch_size=cfg.training.batch_size,
            shuffle=True,
            pin_memory=(device.type != "cpu"),
            drop_last=cfg.training.drop_last,
            prefetch_factor=1,
        )

        epoch_bar = tqdm(
            dataloader,
            desc="Epoch {}/{}".format(epoch+1, cfg.training.max_epoch),
            dynamic_ncols=True,
            leave=False)

        total_loss = 0.0
        for batch in epoch_bar:
            batch = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                     for k, v in batch.items()}

            with make_autocast(amp_enabled):
                loss, hierarchical_info = policy.forward(batch)

            scaled_loss = loss / cfg.training.accumulation_steps

            if amp_enabled:
                scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            if steps % cfg.training.accumulation_steps == 0:
                if amp_enabled:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                lr_scheduler.step()

            # è®°å½•è®­ç»ƒæ—¥å¿— - åŒ…å«åˆ†å±‚æ¶æ„ç‰¹æœ‰çš„ä¿¡æ¯
            if steps % cfg.training.log_freq == 0:
                writer.add_scalar("train/loss", scaled_loss.item(), steps)
                writer.add_scalar(
                    "train/lr", lr_scheduler.get_last_lr()[0], steps)

                # è®°å½•åˆ†å±‚æ¶æ„ç‰¹æœ‰çš„ç»Ÿè®¡ä¿¡æ¯
                if isinstance(hierarchical_info, dict):
                    for key, value in hierarchical_info.items():
                        if isinstance(value, (int, float)):
                            writer.add_scalar(
                                "hierarchical/{}".format(key), value, steps)

                # è®°å½•å„å±‚æ€§èƒ½ç»Ÿè®¡
                if hasattr(policy, 'get_performance_stats'):
                    perf_stats = policy.get_performance_stats()
                    for layer_name, stats in perf_stats.items():
                        if isinstance(stats, dict):
                            for stat_name, stat_value in stats.items():
                                if isinstance(stat_value, (int, float)):
                                    writer.add_scalar(
                                        "performance/{}/{}".format(layer_name, stat_name), stat_value, steps)

                epoch_bar.set_postfix(
                    loss="{:.3f}".format(scaled_loss.item()),
                    step=steps,
                    lr=lr_scheduler.get_last_lr()[0]
                )

            steps += 1
            total_loss += scaled_loss.item()

        # æ›´æ–°æœ€ä½³æŸå¤±
        if total_loss < best_loss:
            best_loss = total_loss
            best_path = output_directory / "best"
            policy.save_pretrained(best_path)
            save_rng_state(best_path / "rng_state.pth")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % cfg.training.save_freq_epoch == 0:
            epoch_path = output_directory / "epoch{}".format(epoch+1)
            policy.save_pretrained(epoch_path)
            save_rng_state(epoch_path / "rng_state.pth")

        # ä¿å­˜æœ€æ–°çš„åˆ†å±‚æ¶æ„æ£€æŸ¥ç‚¹
        save_hierarchical_checkpoint(
            policy, optimizer, lr_scheduler, scaler, steps, epoch + 1, best_loss,
            output_directory, amp_enabled
        )

    writer.close()

    # è®­ç»ƒå®Œæˆåæ‰“å°åˆ†å±‚æ¶æ„ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ‰ Hierarchical training completed!")
    if hasattr(policy, 'get_performance_stats'):
        final_stats = policy.get_performance_stats()
        print("Final layer performance statistics:")
        for layer_name, stats in final_stats.items():
            print("  {}: {}".format(layer_name, stats))


if __name__ == "__main__":
    main()
