# SmolVLA å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥è¯¦è§£

## 1. å¤šä»»åŠ¡å­¦ä¹ æ¦‚è¿°

SmolVLA é‡‡ç”¨é¡ºåºå¾®è°ƒï¼ˆSequential Fine-tuningï¼‰ç­–ç•¥å®ç°å¤šä»»åŠ¡å­¦ä¹ ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é˜²é—å¿˜æŠ€æœ¯ï¼Œä½¿ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œ4ä¸ªä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡ã€‚

### 1.1 ä»»åŠ¡å®šä¹‰

#### ä»»åŠ¡1: ç§»åŠ¨ç›®æ ‡æŠ“å– (Moving Grasp)
- **æè¿°**: æœºå™¨äººä»ç§»åŠ¨çš„ä¼ é€å¸¦ä¸ŠæŠ“å–ç‰©ä½“ï¼Œæ”¾ç½®äºæ¡Œé¢ç¬¬ä¸€ä¸ªç›®æ ‡ä½ç½®åï¼Œå†å°†å…¶æ‹¿èµ·è‡³ç¬¬äºŒä¸ªç›®æ ‡ä½ç½®
- **è¯­è¨€æŒ‡ä»¤**: "Grasp the object from the conveyor belt using visual guidance. Place it precisely at the first marked target location on the table. Then grasp it again and place it precisely at the second marked target location on the table."
- **è®­ç»ƒè½®æ•°**: 100 epochs
- **å­¦ä¹ ç‡**: 5e-5 (ä»é¢„è®­ç»ƒå¼€å§‹)

#### ä»»åŠ¡2: å¿«é€’è¢‹ç§°é‡ (Weighing)
- **æè¿°**: æœºå™¨äººä»ç§»åŠ¨çš„ä¼ é€å¸¦ä¸Šæ‹¾å–å¿«é€’è¢‹ï¼Œå…ˆæ”¾ç½®åœ¨ç”µå­ç§¤ä¸Šå®Œæˆç§°é‡ï¼Œéšåå†æ¬¡æ‹¾èµ·å¹¶æ”¾å…¥æŒ‡å®šæ”¶çº³ç­ä¸­
- **è¯­è¨€æŒ‡ä»¤**: "Pick up the package from the conveyor belt, weigh it on the electronic scale, then pick it up again and place it in the designated storage container"
- **è®­ç»ƒè½®æ•°**: 25 epochs
- **å­¦ä¹ ç‡**: 3.5e-5 (é™ä½30%)

#### ä»»åŠ¡3: æ—¥åŒ–äº§å“å®šå§¿æ‘†æ”¾ (Placement)
- **æè¿°**: æœºå™¨äººä»æ‚ä¹±æ‘†æ”¾çš„æ—¥åŒ–ç“¶ä¸­éšæœºæ‹¾å–ä¸€ç“¶ï¼Œä¼ é€’è‡³å¦ä¸€åªæ‰‹åï¼Œå†æŒ‰ç…§æŒ‡å®šå§¿æ€å°†å…¶æ‘†æ”¾åœ¨ç›®æ ‡ä½ç½®
- **è¯­è¨€æŒ‡ä»¤**: "Pick up a bottle from the cluttered daily chemical bottles, transfer it to the other hand, and place it in the specified pose with the label facing up in the yellow area. Requirements: bottle mouth outside the yellow area, most of the bottle body inside the yellow area, label facing up"
- **è®­ç»ƒè½®æ•°**: 30 epochs
- **å­¦ä¹ ç‡**: 2.5e-5 (è¿›ä¸€æ­¥é™ä½)

#### ä»»åŠ¡4: å…¨æµç¨‹åˆ†æ‹£ (Sorting)
- **æè¿°**: æœºå™¨äººä»æŒ‡å®šèµ·å§‹ç‚¹å‡ºå‘ï¼Œç§»åŠ¨è‡³æµåˆ©æ¶å‰æ‹¾å–å·¥ä»¶ï¼Œéšåè½¬èº«ç§»åŠ¨è‡³æ”¾ç½®æ¶ï¼Œå°†å·¥ä»¶æ”¾ç½®åœ¨ç‰©æ–™ç­å†…æŒ‡å®šä½ç½®
- **è¯­è¨€æŒ‡ä»¤**: "Move from the starting point to the rack, pick up the workpiece, turn around, move to the placement rack, and place it in the designated position in the material container"
- **è®­ç»ƒè½®æ•°**: 35 epochs
- **å­¦ä¹ ç‡**: 2e-5 (æœ€ä½å­¦ä¹ ç‡)

## 2. é¡ºåºè®­ç»ƒç­–ç•¥

### 2.1 è®­ç»ƒæµç¨‹

```
é¢„è®­ç»ƒæ¨¡å‹ (lerobot/smolvla_base)
    â†“
ä»»åŠ¡1æ¨¡å‹ (ç§»åŠ¨æŠ“å–) - 100 epochs, lr=5e-5
    â†“
ä»»åŠ¡2æ¨¡å‹ (å¿«é€’ç§°é‡) - 25 epochs, lr=3.5e-5
    â†“
ä»»åŠ¡3æ¨¡å‹ (å®šå§¿æ‘†æ”¾) - 30 epochs, lr=2.5e-5
    â†“
ä»»åŠ¡4æ¨¡å‹ (å…¨æµç¨‹åˆ†æ‹£) - 35 epochs, lr=2e-5
    â†“
æœ€ç»ˆå¤šä»»åŠ¡æ¨¡å‹ (æ”¯æŒæ‰€æœ‰4ä¸ªä»»åŠ¡)
```

### 2.2 è®­ç»ƒé…ç½®

#### åŸºç¡€é…ç½®
```yaml
# smolvla_sequential_base.yaml
policy:
  vlm_model_name: 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'
  freeze_vision_encoder: True  # å†»ç»“è§†è§‰ç¼–ç å™¨
  train_expert_only: True      # åªè®­ç»ƒAction Expert
  train_state_proj: True       # è®­ç»ƒçŠ¶æ€æŠ•å½±å±‚

  max_state_dim: 32           # é¢„è®­ç»ƒæ¨¡å‹ç»´åº¦
  max_action_dim: 32          # é¢„è®­ç»ƒæ¨¡å‹ç»´åº¦
  chunk_size: 50              # Flow Matchingç”Ÿæˆ50æ­¥
  n_action_steps: 8           # æ¯æ¬¡æ‰§è¡Œ8æ­¥åŠ¨ä½œ
```

#### ä»»åŠ¡ç‰¹å®šé…ç½®
```yaml
# task1_moving_grasp.yaml
task:
  training:
    max_epoch: 100
    resume_from: 'pretrained'
    pretrained_path: 'lerobot/smolvla_base'
    policy:
      optimizer_lr: 5e-5
      scheduler_warmup_steps: 1500
      scheduler_decay_steps: 25000

# task2_weighing.yaml
task:
  training:
    max_epoch: 25
    resume_from: 'task'
    resume_task_id: 1
    policy:
      optimizer_lr: 3.5e-5  # é™ä½30%
      scheduler_warmup_steps: 800
      scheduler_decay_steps: 20000
```

## 3. é˜²é—å¿˜æŠ€æœ¯

### 3.1 Replay Buffer ç­–ç•¥

#### æ¯”ä¾‹æ··åˆé…ç½®
```yaml
sequential:
  use_replay_buffer: True
  replay_strategy: 'proportional'

  # Stage 2: è®­ç»ƒä»»åŠ¡2æ—¶çš„æ•°æ®æ··åˆæ¯”ä¾‹
  stage2_replay:
    task1: 0.2  # 20% ä»»åŠ¡1æ•°æ®
    task2: 0.8  # 80% ä»»åŠ¡2æ•°æ®

  # Stage 3: è®­ç»ƒä»»åŠ¡3æ—¶çš„æ•°æ®æ··åˆæ¯”ä¾‹
  stage3_replay:
    task1: 0.1  # 10% ä»»åŠ¡1æ•°æ®
    task2: 0.2  # 20% ä»»åŠ¡2æ•°æ®
    task3: 0.7  # 70% ä»»åŠ¡3æ•°æ®

  # Stage 4: è®­ç»ƒä»»åŠ¡4æ—¶çš„æ•°æ®æ··åˆæ¯”ä¾‹
  stage4_replay:
    task1: 0.1  # 10% ä»»åŠ¡1æ•°æ®
    task2: 0.1  # 10% ä»»åŠ¡2æ•°æ®
    task3: 0.2  # 20% ä»»åŠ¡3æ•°æ®
    task4: 0.6  # 60% ä»»åŠ¡4æ•°æ®
```

#### Replay Buffer å®ç°
```python
class ReplayDatasetManager:
    def load_replay_tasks(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦replayçš„ä»»åŠ¡æ•°æ®"""
        if self.current_task_id == 1:
            return {}, {}  # ä»»åŠ¡1ä¸éœ€è¦replay

        # è·å–å½“å‰stageçš„replayé…ç½®
        stage_key = f"stage{self.current_task_id}_replay"
        replay_config = self.cfg.sequential.get(stage_key, {})

        for task_key, weight in replay_config.items():
            if 'task' in task_key:
                task_id = int(task_key.replace('task', ''))

                # åªåŠ è½½ä¹‹å‰çš„ä»»åŠ¡
                if task_id < self.current_task_id:
                    # åŠ è½½ä»»åŠ¡é…ç½®
                    task_cfg = load_task_config(self.cfg_root, task_id)

                    # åŠ è½½æ•°æ®é›†
                    dataset = LeRobotDataset(
                        task_cfg.task.data.repoid,
                        root=task_cfg.task.data.root,
                        episodes=list(range(
                            task_cfg.task.data.episodes_to_use[0],
                            task_cfg.task.data.episodes_to_use[1] + 1
                        )),
                        delta_timestamps=delta_timestamps
                    )

                    self.replay_datasets[task_id] = dataset
                    self.replay_weights[task_id] = weight
```

### 3.2 å†»ç»“ç­–ç•¥

#### å‚æ•°å†»ç»“æœºåˆ¶
```python
# å†»ç»“è§†è§‰ç¼–ç å™¨
if config.freeze_vision_encoder:
    for param in self.vision_encoder.parameters():
        param.requires_grad = False

# åªè®­ç»ƒAction Expert
if config.train_expert_only:
    for name, param in self.named_parameters():
        if 'action_expert' not in name:
            param.requires_grad = False
```

#### å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
```python
def get_task_learning_rate(task_id):
    """è·å–ä»»åŠ¡ç‰¹å®šå­¦ä¹ ç‡"""
    base_lr = 5e-5
    decay_factors = [1.0, 0.7, 0.5, 0.4]  # å¯¹åº”ä»»åŠ¡1-4
    return base_lr * decay_factors[task_id - 1]

# ä»»åŠ¡1: 5e-5 (ä»é¢„è®­ç»ƒå¼€å§‹)
# ä»»åŠ¡2: 3.5e-5 (é™ä½30%ï¼Œä¿æŠ¤ä»»åŠ¡1çŸ¥è¯†)
# ä»»åŠ¡3: 2.5e-5 (è¿›ä¸€æ­¥é™ä½ï¼Œä¿æŠ¤ä»»åŠ¡1+2çŸ¥è¯†)
# ä»»åŠ¡4: 2e-5 (æœ€ä½å­¦ä¹ ç‡ï¼Œç²¾ç»†è°ƒæ•´å¤šä»»åŠ¡æ¨¡å‹)
```

## 4. å¤šä»»åŠ¡éªŒè¯æœºåˆ¶

### 4.1 éªŒè¯æµç¨‹

```python
def validate_all_tasks(policy, cfg, current_task_id, device, cfg_root):
    """éªŒè¯æ‰€æœ‰ä¹‹å‰çš„ä»»åŠ¡ï¼ˆæ£€æµ‹é—å¿˜ï¼‰"""
    print(f"ğŸ” Multi-Task Validation (Tasks 1-{current_task_id})")

    policy.eval()
    validation_results = {}

    for task_id in range(1, current_task_id + 1):
        print(f"ğŸ“Š Validating Task {task_id}...")

        # åŠ è½½ä»»åŠ¡é…ç½®
        task_cfg = load_task_config(cfg_root, task_id)

        # åŠ è½½éªŒè¯é›†
        val_dataset = LeRobotDataset(...)
        val_loader = create_dataloader_with_language(...)

        # éªŒè¯
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Task {task_id} Validation"):
                batch = {k: v.to(device) for k, v in batch.items()}
                loss, _ = policy.forward(batch)
                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        validation_results[task_id] = avg_loss

        print(f"  Task {task_id} Validation Loss: {avg_loss:.4f}")

    # åˆ†æé—å¿˜æƒ…å†µ
    if current_task_id > 1:
        print("\nâš ï¸ Forgetting Analysis:")
        for task_id in range(1, current_task_id):
            loss = validation_results[task_id]
            if loss < 0.7:
                status = "âœ… Well Retained"
            elif loss < 1.0:
                status = "âš ï¸ Slight Degradation"
            else:
                status = "âŒ Significant Forgetting"

            print(f"  Task {task_id}: {status} (loss={loss:.4f})")

    policy.train()
    return validation_results
```

### 4.2 éªŒè¯é…ç½®

```yaml
training:
  # å¤šä»»åŠ¡éªŒè¯é…ç½®ï¼ˆé˜²é—å¿˜çš„å…³é”®ï¼‰
  validate_all_previous_tasks: True
  validation_freq_epoch: 2  # æ¯2ä¸ªepochéªŒè¯æ‰€æœ‰ä¹‹å‰çš„ä»»åŠ¡
  validation_episodes: 20   # æ›´å¤šéªŒè¯episodesï¼Œè¯„ä¼°æ›´å‡†ç¡®
```

## 5. è®­ç»ƒä¼˜åŒ–ç­–ç•¥

### 5.1 ä¼˜åŒ–å™¨é…ç½®

```yaml
# é’ˆå¯¹batch_size=64ä¼˜åŒ–çš„å‚æ•°
optimizer_betas: [0.9, 0.999]  # beta2=0.999å¯¹è¾ƒå¤§batchæ›´ç¨³å®š
optimizer_eps: 1.0e-08
optimizer_weight_decay: 5.0e-7  # é€‚åº¦é™ä½æ­£åˆ™åŒ–ï¼Œé¿å…æ¬ æ‹Ÿåˆ
optimizer_grad_clip_norm: 1.0   # VLM embeddingç©ºé—´å¤§ï¼Œéœ€è¦ä¸¥æ ¼æ¢¯åº¦æ§åˆ¶
```

### 5.2 å­¦ä¹ ç‡è°ƒåº¦

```yaml
# å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
scheduler_warmup_steps: 1500  # VLM+Action Expertå¼‚æ„æ¶æ„éœ€è¦æ›´é•¿warmup
scheduler_decay_steps: 25000  # å……åˆ†çš„cosine decayä¿è¯æ”¶æ•›
scheduler_decay_lr: 1e-6      # æœ€ç»ˆå­¦ä¹ ç‡è¡°å‡åˆ°å¾ˆå°
```

### 5.3 æ•°æ®åŠ è½½ä¼˜åŒ–

```yaml
# æ•°æ®åŠ è½½ï¼ˆé’ˆå¯¹batch_size=64ä¼˜åŒ–ï¼‰
batch_size: 64
num_workers: 16              # é™ä½workeræ•°é‡ï¼Œé¿å…CPUèµ„æºç«äº‰
drop_last: True
prefetch_factor: 2           # å¢åŠ é¢„å–ï¼Œæé«˜GPUåˆ©ç”¨ç‡
persistent_workers: True     # ä¿æŒworkerè¿›ç¨‹ï¼Œå‡å°‘é‡å¯å¼€é”€
```

## 6. è®­ç»ƒè„šæœ¬å®ç°

### 6.1 ä¸»è®­ç»ƒæµç¨‹

```python
@hydra.main(config_path="../configs/policy/", config_name="smolvla_sequential_base")
def main(cfg: DictConfig):
    """ä¸»è®­ç»ƒæµç¨‹"""

    # è®¾ç½®HuggingFaceé•œåƒæº
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

    # åŠ è½½ä»»åŠ¡é…ç½®
    task_cfg = load_task_config(cfg_root, task_id)

    # æ„å»ºPolicyé…ç½®
    policy_cfg = instantiate(cfg.policy, ...)

    # åŠ è½½/åˆ›å»ºæ¨¡å‹
    if task_cfg.task.training.resume_from == 'pretrained':
        # Stage 1: ä»HuggingFaceé¢„è®­ç»ƒåŠ è½½
        policy = SmolVLAPolicyWrapper.from_pretrained(
            task_cfg.task.training.pretrained_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )
    elif task_cfg.task.training.resume_from == 'task':
        # Stage 2+: ä»ä¸Šä¸€ä¸ªä»»åŠ¡ç»§ç»­
        policy = SmolVLAPolicyWrapper.from_pretrained(
            resume_path,
            config=policy_cfg,
            dataset_stats=dataset_stats
        )

    # å‡†å¤‡æ•°æ®ï¼ˆåŒ…æ‹¬replay bufferï¼‰
    replay_manager = ReplayDatasetManager(...)
    dataloader = create_mixed_dataloader(cfg, task_cfg, replay_manager)

    # æ„å»ºä¼˜åŒ–å™¨
    optimizer = policy.config.get_optimizer_preset().build(policy.parameters())
    lr_scheduler = policy.config.get_scheduler_preset().build(optimizer, ...)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(task_cfg.task.training.max_epoch):
        # è®­ç»ƒé˜¶æ®µ
        for batch in dataloader:
            loss, _ = policy.forward(batch)
            loss.backward()
            optimizer.step()
            lr_scheduler.step()

        # å¤šä»»åŠ¡éªŒè¯
        if (epoch + 1) % cfg.training.validation_freq_epoch == 0:
            validation_results = validate_all_tasks(...)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            policy.save_pretrained(best_path)
```

### 6.2 æ•°æ®æ··åˆå®ç°

```python
class MixedDataset(torch.utils.data.Dataset):
    """æ··åˆå¤šä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†ä¿ç•™è‡ªå·±çš„language instruction"""

    def __init__(self, datasets_with_language):
        self.datasets_with_language = datasets_with_language
        self.lengths = [len(ds) for ds, _ in datasets_with_language]
        self.total_length = sum(self.lengths)

        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„é‡‡æ ·æ¦‚ç‡ï¼ˆåŸºäºreplay weightsï¼‰
        stage_key = f"stage{task_id}_replay"
        replay_config = cfg.sequential.get(stage_key, {})

        self.weights = []
        for i, (ds, _) in enumerate(datasets_with_language):
            if i == 0:
                # å½“å‰ä»»åŠ¡çš„weight
                task_key = f"task{task_id}"
                weight = replay_config.get(task_key, 1.0)
            else:
                # Replayä»»åŠ¡çš„weight
                task_key = f"task{i}"
                weight = replay_config.get(task_key, 0.1)
            self.weights.append(weight)

        # å½’ä¸€åŒ–weights
        total_weight = sum(self.weights)
        self.weights = [w / total_weight for w in self.weights]

    def __getitem__(self, idx):
        # æ ¹æ®weightséšæœºé€‰æ‹©ä¸€ä¸ªdataset
        dataset_idx = random.choices(
            range(len(self.datasets_with_language)),
            weights=self.weights, k=1
        )[0]
        dataset, language = self.datasets_with_language[dataset_idx]

        # ä»è¯¥datasetéšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        sample_idx = random.randint(0, len(dataset) - 1)
        sample = dataset[sample_idx]

        # æ·»åŠ language instruction
        sample['task'] = language

        return sample
```

## 7. ä½¿ç”¨ç¤ºä¾‹

### 7.1 è®­ç»ƒä»»åŠ¡1

```bash
python kuavo_train/train_smolvla_sequential.py \
    --config-path=../configs/policy \
    --config-name=smolvla_sequential_base \
    task=tasks/task1_moving_grasp
```

### 7.2 è®­ç»ƒä»»åŠ¡2

```bash
python kuavo_train/train_smolvla_sequential.py \
    --config-path=../configs/policy \
    --config-name=smolvla_sequential_base \
    task=tasks/task2_weighing
```

### 7.3 æ¨ç†ä½¿ç”¨

```python
from kuavo_train.wrapper.policy.smolvla import SmolVLAPolicyWrapper

# åŠ è½½æœ€ç»ˆå¤šä»»åŠ¡æ¨¡å‹
policy = SmolVLAPolicyWrapper.from_pretrained("path/to/task4/checkpoint")

# æ‰§è¡Œä»»åŠ¡1
action1 = policy.select_action({
    'observation.images': images,
    'observation.state': state,
    'task': ['Grasp the object from the conveyor belt using visual guidance...']
})

# æ‰§è¡Œä»»åŠ¡2
action2 = policy.select_action({
    'observation.images': images,
    'observation.state': state,
    'task': ['Pick up the package from the conveyor belt, weigh it on the electronic scale...']
})

# æ‰§è¡Œä»»åŠ¡3
action3 = policy.select_action({
    'observation.images': images,
    'observation.state': state,
    'task': ['Pick up a bottle from the cluttered daily chemical bottles...']
})

# æ‰§è¡Œä»»åŠ¡4
action4 = policy.select_action({
    'observation.images': images,
    'observation.state': state,
    'task': ['Move from the starting point to the rack, pick up the workpiece...']
})
```

## 8. æ€§èƒ½è¯„ä¼°

### 8.1 è®­ç»ƒæŒ‡æ ‡

- **ä»»åŠ¡1**: 100 epochs, æœ€ç»ˆloss < 0.5
- **ä»»åŠ¡2**: 25 epochs, æœ€ç»ˆloss < 0.6
- **ä»»åŠ¡3**: 30 epochs, æœ€ç»ˆloss < 0.7
- **ä»»åŠ¡4**: 35 epochs, æœ€ç»ˆloss < 0.8

### 8.2 é—å¿˜æ£€æµ‹

- **ä»»åŠ¡1**: loss < 0.7 (Well Retained)
- **ä»»åŠ¡2**: loss < 0.8 (Slight Degradation)
- **ä»»åŠ¡3**: loss < 0.9 (Slight Degradation)

### 8.3 æ¨¡å‹æ€§èƒ½

- **å‚æ•°é‡**: çº¦500M (è½»é‡çº§)
- **æ¨ç†é€Ÿåº¦**: å®æ—¶ (10æ­¥Flow Matching)
- **å¤šä»»åŠ¡èƒ½åŠ›**: æ”¯æŒ4ä¸ªä»»åŠ¡é€šè¿‡è¯­è¨€æŒ‡ä»¤åˆ‡æ¢

## 9. æœ€ä½³å®è·µ

### 9.1 è®­ç»ƒå»ºè®®

1. **å­¦ä¹ ç‡è®¾ç½®**: ä»5e-5å¼€å§‹ï¼Œæ¯ä»»åŠ¡é€’å‡30%
2. **Replayæ¯”ä¾‹**: å½“å‰ä»»åŠ¡å 60-80%ï¼Œä¹‹å‰ä»»åŠ¡å 20-40%
3. **éªŒè¯é¢‘ç‡**: æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡
4. **ä¿å­˜ç­–ç•¥**: ä¿å­˜æœ€ä½³æ¨¡å‹å’Œå®šæœŸæ£€æŸ¥ç‚¹

### 9.2 è°ƒè¯•å»ºè®®

1. **ç›‘æ§é—å¿˜**: å®šæœŸæ£€æŸ¥ä¹‹å‰ä»»åŠ¡çš„éªŒè¯loss
2. **è°ƒæ•´æ¯”ä¾‹**: æ ¹æ®é—å¿˜æƒ…å†µè°ƒæ•´replayæ¯”ä¾‹
3. **å­¦ä¹ ç‡è°ƒæ•´**: å¦‚æœé—å¿˜ä¸¥é‡ï¼Œè¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡
4. **æ•°æ®è´¨é‡**: ç¡®ä¿æ¯ä¸ªä»»åŠ¡çš„æ•°æ®è´¨é‡

### 9.3 éƒ¨ç½²å»ºè®®

1. **æ¨¡å‹é€‰æ‹©**: ä½¿ç”¨ä»»åŠ¡4çš„æœ€ç»ˆæ¨¡å‹
2. **è¯­è¨€æŒ‡ä»¤**: ä½¿ç”¨ç²¾ç¡®çš„ä»»åŠ¡æè¿°
3. **æ‰¹å¤„ç†**: æ”¯æŒæ‰¹é‡æ¨ç†æé«˜æ•ˆç‡
4. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶

## 10. æ€»ç»“

SmolVLA å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥é€šè¿‡é¡ºåºå¾®è°ƒå’Œé˜²é—å¿˜æŠ€æœ¯ï¼ŒæˆåŠŸå®ç°äº†ä¸€ä¸ªæ¨¡å‹æ”¯æŒ4ä¸ªä¸åŒæœºå™¨äººæ“ä½œä»»åŠ¡çš„ç›®æ ‡ã€‚å…³é”®æŠ€æœ¯åŒ…æ‹¬ï¼š

- **é¡ºåºè®­ç»ƒ**: ä»é¢„è®­ç»ƒæ¨¡å‹é€æ­¥å­¦ä¹ 4ä¸ªä»»åŠ¡
- **é˜²é—å¿˜æŠ€æœ¯**: Replay Buffer + å†»ç»“ç­–ç•¥ + å­¦ä¹ ç‡è¡°å‡
- **å¤šä»»åŠ¡éªŒè¯**: å®šæœŸæ£€æµ‹é—å¿˜æƒ…å†µ
- **è¯­è¨€æŒ‡ä»¤**: é€šè¿‡è‡ªç„¶è¯­è¨€åˆ‡æ¢ä»»åŠ¡
- **ç»´åº¦é€‚é…**: è‡ªåŠ¨å¤„ç†16ç»´åˆ°32ç»´çš„è½¬æ¢

è¯¥ç­–ç•¥ä¸ºæœºå™¨äººå¤šä»»åŠ¡å­¦ä¹ æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚
