# åˆ†å±‚äººå½¢æœºå™¨äºº Diffusion Policy æ¶æ„è¯¦è§£

> **ä½œè€…**: AI Assistant
> **æ—¥æœŸ**: 2025-10-10
> **ç‰ˆæœ¬**: 1.0
> **é€‚ç”¨äº**: `kuavo_data_challenge` é¡¹ç›®

---

## ğŸ“‹ ç›®å½•

1. [æ¶æ„æ¦‚è§ˆ](#1-æ¶æ„æ¦‚è§ˆ)
2. [è®­ç»ƒä¸»æµç¨‹](#2-è®­ç»ƒä¸»æµç¨‹)
3. [åˆ†å±‚æ¶æ„è¯¦è§£](#3-åˆ†å±‚æ¶æ„è¯¦è§£)
4. [è¯¾ç¨‹å­¦ä¹ æœºåˆ¶](#4-è¯¾ç¨‹å­¦ä¹ æœºåˆ¶)
5. [ä»»åŠ¡ç‰¹å®šè®­ç»ƒ](#5-ä»»åŠ¡ç‰¹å®šè®­ç»ƒ)
6. [æ¨ç†é€»è¾‘](#6-æ¨ç†é€»è¾‘)
7. [é…ç½®ç³»ç»Ÿ](#7-é…ç½®ç³»ç»Ÿ)
8. [å…³é”®è®¾è®¡å†³ç­–](#8-å…³é”®è®¾è®¡å†³ç­–)

---

## 1. æ¶æ„æ¦‚è§ˆ

### 1.1 æ•´ä½“è®¾è®¡ç†å¿µ

åˆ†å±‚äººå½¢æœºå™¨äºº Diffusion Policyï¼ˆHierarchical Humanoid Diffusion Policyï¼‰æ˜¯ä¸€ä¸ª**æ··åˆæ¶æ„**ï¼Œç»“åˆäº†ï¼š

- **Diffusion Policy**: åŸºç¡€çš„æ‰©æ•£æ¨¡å‹ç”¨äºåŠ¨ä½œç”Ÿæˆ
- **åˆ†å±‚æ§åˆ¶**: å››å±‚ä¼˜å…ˆçº§ç»“æ„ï¼Œæ¨¡æ‹Ÿäººç±»è®¤çŸ¥å±‚æ¬¡
- **è¯¾ç¨‹å­¦ä¹ **: æ¸è¿›å¼è®­ç»ƒï¼Œä»ç®€å•åˆ°å¤æ‚
- **ä»»åŠ¡ç‰¹å®šè®­ç»ƒ**: æ”¯æŒå¤šä»»åŠ¡åœºæ™¯ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜

### 1.2 æ ¸å¿ƒç»„ä»¶å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     train_hierarchical_policy.py                â”‚
â”‚                          (è®­ç»ƒä¸»å…¥å£)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HumanoidDiffusionPolicyâ”‚   â”‚ TaskSpecificTrainingManager  â”‚
    â”‚  (ç­–ç•¥ä¸»æ§åˆ¶å™¨)         â”‚   â”‚   (ä»»åŠ¡ç®¡ç†å™¨)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚HierarchicalSchedulerâ”‚  â”‚HierarchicalDiffusionModelâ”‚
â”‚  (å±‚è°ƒåº¦å™¨)     â”‚  â”‚   (Diffusionæ¨¡å‹)      â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ ç®¡ç†å››ä¸ªå±‚
    â”‚
    â”œâ”€â–º SafetyReflexLayer (ä¼˜å…ˆçº§1, ~10ms)
    â”œâ”€â–º GaitControlLayer (ä¼˜å…ˆçº§2, ~20ms)
    â”œâ”€â–º ManipulationLayer (ä¼˜å…ˆçº§3, ~100ms)
    â””â”€â–º GlobalPlanningLayer (ä¼˜å…ˆçº§4, ~500ms)
```

### 1.3 æ•°æ®æµæ¦‚è§ˆ

```
è¾“å…¥æ•°æ® (è§‚æµ‹)
    â”‚
    â”œâ”€â–º observation.state (å…³èŠ‚çŠ¶æ€)
    â”œâ”€â–º observation.images.* (RGBç›¸æœº)
    â””â”€â–º observation.depth.* (æ·±åº¦ç›¸æœº)
    â”‚
    â–¼
å›¾åƒé¢„å¤„ç† (è£å‰ªã€ç¼©æ”¾ã€å½’ä¸€åŒ–)
    â”‚
    â–¼
åˆ†å±‚å¤„ç† (HierarchicalScheduler)
    â”‚
    â”œâ”€â–º Layer 1: SafetyReflexLayer
    â”‚   â””â”€â–º è¾“å‡º: ç´§æ€¥çŠ¶æ€ã€å¹³è¡¡æ§åˆ¶
    â”‚
    â”œâ”€â–º Layer 2: GaitControlLayer
    â”‚   â””â”€â–º è¾“å‡º: æ­¥æ€ç‰¹å¾ã€è´Ÿè½½é€‚åº”
    â”‚
    â”œâ”€â–º Layer 3: ManipulationLayer
    â”‚   â””â”€â–º è¾“å‡º: æ“ä½œç‰¹å¾ã€åŒè‡‚åè°ƒ
    â”‚
    â””â”€â–º Layer 4: GlobalPlanningLayer
        â””â”€â–º è¾“å‡º: è§„åˆ’ç‰¹å¾ã€ä»»åŠ¡åˆ†è§£
    â”‚
    â–¼
Diffusion Model æŸå¤±è®¡ç®—
    â”‚
    â–¼
å±‚æŸå¤±èšåˆ (åŠ æƒæ±‚å’Œ)
    â”‚
    â–¼
æœ€ç»ˆæŸå¤± & åå‘ä¼ æ’­
```

---

## 2. è®­ç»ƒä¸»æµç¨‹

### 2.1 å…¥å£å‡½æ•°: `train_hierarchical_policy.py::main()`

```python
@hydra.main(config_path="../configs/policy/",
            config_name="humanoid_diffusion_config")
def main(cfg: DictConfig):
    """ç»Ÿä¸€åˆ†å±‚æ¶æ„è®­ç»ƒä¸»å‡½æ•°"""
```

#### è®­ç»ƒæµç¨‹å›¾

```
å¼€å§‹
  â”‚
  â”œâ”€â–º 1. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
  â”‚
  â”œâ”€â–º 2. è®¾ç½®éšæœºç§å­
  â”‚
  â”œâ”€â–º 3. æ£€æŸ¥è®­ç»ƒæ¨¡å¼
  â”‚      â”œâ”€â–º åŸºç¡€æ¨¡å¼ (use_hierarchical=True, task_specific=False)
  â”‚      â””â”€â–º ä»»åŠ¡ç‰¹å®šæ¨¡å¼ (task_specific=True)
  â”‚
  â”œâ”€â–º 4. åŠ è½½æ•°æ®é›†
  â”‚      â”œâ”€â–º åŸºç¡€æ¨¡å¼: ç›´æ¥åŠ è½½ LeRobotDataset
  â”‚      â””â”€â–º ä»»åŠ¡ç‰¹å®šæ¨¡å¼: åŠ¨æ€åŠ è½½å¤šä»»åŠ¡æ•°æ®
  â”‚
  â”œâ”€â–º 5. æ„å»º Policy
  â”‚      â””â”€â–º HumanoidDiffusionPolicy (åˆ†å±‚æ¶æ„)
  â”‚
  â”œâ”€â–º 6. æ„å»ºä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦å™¨
  â”‚
  â”œâ”€â–º 7. è¯¾ç¨‹å­¦ä¹  (å¯é€‰)
  â”‚      â””â”€â–º æŒ‰é˜¶æ®µè®­ç»ƒå„å±‚
  â”‚
  â”œâ”€â–º 8. ä¸»è®­ç»ƒå¾ªç¯
  â”‚      â”œâ”€â–º Epochå¾ªç¯
  â”‚      â”œâ”€â–º Batchå¾ªç¯
  â”‚      â”œâ”€â–º å‰å‘ä¼ æ’­
  â”‚      â”œâ”€â–º æŸå¤±è®¡ç®—
  â”‚      â”œâ”€â–º åå‘ä¼ æ’­
  â”‚      â””â”€â–º å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
  â”‚
  â””â”€â–º 9. è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
```

### 2.2 å…³é”®è®­ç»ƒæ­¥éª¤è¯¦è§£

#### 2.2.1 æ•°æ®é›†åŠ è½½

**åŸºç¡€æ¨¡å¼:**
```python
# ä»é…ç½®è¯»å–æ•°æ®è·¯å¾„
dataset_metadata = LeRobotDatasetMetadata(cfg.repoid, root=cfg.root)

# æ„å»ºdelta timestampsï¼ˆè§‚æµ‹å’ŒåŠ¨ä½œçš„æ—¶é—´åç§»ï¼‰
delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)

# åˆ›å»ºæ•°æ®é›†
dataset = LeRobotDataset(
    cfg.repoid,
    delta_timestamps=delta_timestamps,
    root=cfg.root,
    episodes=episodes_to_use,  # å¯é™åˆ¶ä½¿ç”¨çš„episodes
    image_transforms=image_transforms,  # å›¾åƒå¢å¼º
)
```

**ä»»åŠ¡ç‰¹å®šæ¨¡å¼:**
```python
# æ³¨å†Œå¯ç”¨ä»»åŠ¡
task_manager.register_available_task(task_id, episode_count, data_path)

# åŠ è½½æ¯ä¸ªä»»åŠ¡çš„æ•°æ®é›†
for task_id in available_tasks:
    dataset, metadata = load_task_dataset(task_id, cfg, policy_cfg, transforms)
    datasets[task_id] = dataset

# åˆ›å»ºåŠ æƒé‡‡æ ·çš„DataLoader
dataloader = create_task_specific_dataloader(datasets, task_manager, cfg, device)
```

#### 2.2.2 Policy æ„å»º

```python
def build_hierarchical_policy(policy_cfg, dataset_stats):
    """æ„å»ºåˆ†å±‚æ¶æ„çš„policy"""
    return HumanoidDiffusionPolicy(policy_cfg, dataset_stats)
```

**Policyåˆå§‹åŒ–æµç¨‹:**
1. æ£€æŸ¥ `use_hierarchical` é…ç½®
2. å¦‚æœå¯ç”¨ï¼Œåˆ›å»º `HierarchicalScheduler` (ç®¡ç†å››å±‚)
3. åˆ›å»º `HierarchicalDiffusionModel` (æ›¿æ¢æ ‡å‡†Diffusionæ¨¡å‹)
4. åˆå§‹åŒ–ä»»åŠ¡æ¡ä»¶æƒé‡ç³»ç»Ÿ

#### 2.2.3 è¯¾ç¨‹å­¦ä¹ æ‰§è¡Œ

```python
def run_curriculum_learning_stage(
    policy, stage_config, dataset, cfg, device, writer,
    current_step, optimizer, lr_scheduler, scaler,
    output_directory, amp_enabled, task_manager, dataloader
):
    """è¿è¡Œè¯¾ç¨‹å­¦ä¹ çš„å•ä¸ªé˜¶æ®µ"""
```

**é˜¶æ®µæ‰§è¡Œæµç¨‹:**
```
1. è§£æé˜¶æ®µé…ç½®
   â”œâ”€â–º name: é˜¶æ®µåç§°
   â”œâ”€â–º layers: æ¿€æ´»çš„å±‚åˆ—è¡¨ ['safety', 'manipulation', ...]
   â”œâ”€â–º epochs: è®­ç»ƒè½®æ¬¡
   â””â”€â–º target_task: ç›®æ ‡ä»»åŠ¡ID (ä»»åŠ¡ç‰¹å®šæ¨¡å¼)

2. æ¿€æ´»æŒ‡å®šå±‚
   â””â”€â–º policy.set_curriculum_stage(enabled_layers)

3. é…ç½®ä»»åŠ¡å±‚æƒé‡ (ä»»åŠ¡ç‰¹å®šæ¨¡å¼)
   â””â”€â–º policy.set_task_layer_weights(layer_weights)

4. Epochå¾ªç¯
   â”œâ”€â–º Batchè¿­ä»£
   â”œâ”€â–º æ„å»ºcurriculum_info
   â”œâ”€â–º å‰å‘ä¼ æ’­: loss, hierarchical_info = policy.forward(batch, curriculum_info)
   â”œâ”€â–º åå‘ä¼ æ’­
   â””â”€â–º è®°å½•æŸå¤±å’Œå±‚æ€§èƒ½æŒ‡æ ‡

5. ä¿å­˜æœ€ä½³æ¨¡å‹
   â””â”€â–º åŸºäºepochå¹³å‡æŸå¤±
```

#### 2.2.4 ä¸»è®­ç»ƒå¾ªç¯

```python
for epoch in range(start_epoch, cfg.training.max_epoch):
    dataloader = create_dataloader(...)  # åˆ›å»ºæˆ–æ›´æ–°dataloader

    for batch in dataloader:
        # æ•°æ®ç§»åˆ°è®¾å¤‡
        batch = {k: v.to(device) for k, v in batch.items()}

        # å‰å‘ä¼ æ’­
        with autocast(amp_enabled):
            if use_task_specific:
                task_loss_weights = task_manager.get_task_specific_loss_weights(batch)
                loss, info = policy.forward(batch, task_weights=task_loss_weights)
            else:
                loss, info = policy.forward(batch)

        # æ¢¯åº¦ç´¯ç§¯
        scaled_loss = loss / accumulation_steps
        scaler.scale(scaled_loss).backward()

        # ä¼˜åŒ–å™¨æ­¥éª¤
        if steps % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            lr_scheduler.step()

        # è®°å½•æ—¥å¿—
        if steps % log_freq == 0:
            writer.add_scalar("train/loss", scaled_loss.item(), steps)
            # è®°å½•åˆ†å±‚æ¶æ„çš„è¯¦ç»†ä¿¡æ¯
            log_hierarchical_info(writer, info, steps)

        steps += 1

    # ä¿å­˜æ£€æŸ¥ç‚¹
    save_hierarchical_checkpoint(...)
```

---

## 3. åˆ†å±‚æ¶æ„è¯¦è§£

### 3.1 HierarchicalScheduler (å±‚è°ƒåº¦å™¨)

**ä½ç½®**: `kuavo_train/wrapper/policy/humanoid/HierarchicalScheduler.py`

#### 3.1.1 æ ¸å¿ƒèŒè´£

```python
class HierarchicalScheduler(nn.Module):
    """åˆ†å±‚æ¶æ„çš„æ ¸å¿ƒè°ƒåº¦å™¨ï¼Œè´Ÿè´£ç®¡ç†å››ä¸ªå±‚æ¬¡çš„æ¿€æ´»ã€è°ƒåº¦å’Œè¾“å‡ºèšåˆ"""
```

1. **å±‚ç®¡ç†**: åˆ›å»ºå’Œç®¡ç†å››ä¸ªå±‚
2. **ä¼˜å…ˆçº§è°ƒåº¦**: æŒ‰ä¼˜å…ˆçº§é¡ºåºæ‰§è¡Œå±‚
3. **ä¸Šä¸‹æ–‡ä¼ é€’**: åœ¨å±‚ä¹‹é—´ä¼ é€’ä¿¡æ¯
4. **ç´§æ€¥å¤„ç†**: å®‰å…¨å±‚å¯ç«‹å³ä¸­æ–­
5. **æ€§èƒ½ç›‘æ§**: è·Ÿè¸ªå„å±‚æ‰§è¡Œæ—¶é—´å’Œæ¿€æ´»æ¬¡æ•°

#### 3.1.2 åˆå§‹åŒ–æµç¨‹

```python
def __init__(self, hierarchical_config, base_config):
    # 1. ä¿å­˜é…ç½®
    self.config = hierarchical_config
    self.base_config = base_config

    # 2. æ„å»ºå››ä¸ªå±‚
    self.layers = self._build_layers()
    #    â”œâ”€â–º SafetyReflexLayer
    #    â”œâ”€â–º GaitControlLayer
    #    â”œâ”€â–º ManipulationLayer
    #    â””â”€â–º GlobalPlanningLayer

    # 3. è®¾ç½®ä¼˜å…ˆçº§å’Œæƒé‡
    self.layer_priorities = {name: layer.get_priority()
                            for name, layer in self.layers.items()}
    self.layer_weights = hierarchical_config.get('layer_weights', {})

    # 4. åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
    self.total_forward_calls = 0
    self.layer_activation_stats = {name: 0 for name in self.layers.keys()}
```

#### 3.1.3 å‰å‘ä¼ æ’­æµç¨‹

```python
def forward(self, batch, task_info=None):
    """åˆ†å±‚å¤„ç†å‰å‘ä¼ æ’­"""

    # 1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ¨ç†æ¨¡å¼
    if task_info and 'latency_budget_ms' in task_info:
        return self.inference_mode(batch, task_info, latency_budget)

    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context = self._build_context(batch, task_info)
    #    åŒ…å«: batch_size, device, training, task_info

    # 3. æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†å„å±‚
    outputs = {}
    for layer_name in self._get_processing_order():
        layer = self.layers[layer_name]

        # 3.1 æ£€æŸ¥æ˜¯å¦åº”è¯¥æ¿€æ´»
        if not layer.should_activate(batch, context):
            continue

        # 3.2 æ‰§è¡Œå±‚çš„å‰å‘ä¼ æ’­ï¼ˆå¸¦æ—¶é—´ç›‘æ§ï¼‰
        layer_output = layer.forward_with_timing(batch, context)
        outputs[layer_name] = layer_output

        # 3.3 æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆä¾›åç»­å±‚ä½¿ç”¨ï¼‰
        context.update(layer_output)

        # 3.4 å®‰å…¨å±‚ç´§æ€¥æ£€æŸ¥
        if layer_name == 'safety' and layer_output.get('emergency'):
            return {layer_name: layer_output}  # ç«‹å³è¿”å›

    # 4. è¿”å›æ‰€æœ‰å±‚çš„è¾“å‡º
    return outputs
```

#### 3.1.4 æ¨ç†æ¨¡å¼ (Inference Mode)

```python
def inference_mode(self, batch, task_info, latency_budget_ms=50.0):
    """æ ¹æ®å»¶è¿Ÿé¢„ç®—è‡ªé€‚åº”æ¿€æ´»å±‚"""

    remaining_budget = latency_budget_ms
    outputs = {}

    for layer_name in self._get_processing_order():
        layer = self.layers[layer_name]

        # 1. æ£€æŸ¥æ—¶é—´é¢„ç®—
        layer_budget = layer.get_latency_budget()
        if remaining_budget < layer_budget:
            print(f"â° Skipping {layer_name} due to time budget")
            continue

        # 2. æ‰§è¡Œå±‚æ¨ç†
        start_time = time.time()
        layer_output = layer.forward_with_timing(batch, context)
        outputs[layer_name] = layer_output

        # 3. æ›´æ–°å‰©ä½™é¢„ç®—
        layer_time = (time.time() - start_time) * 1000
        remaining_budget -= layer_time

        # 4. ç´§æ€¥æƒ…å†µç«‹å³è¿”å›
        if layer_name == 'safety' and layer_output.get('emergency'):
            break

    outputs['_inference_stats'] = {
        'total_time_ms': total_time,
        'remaining_budget_ms': remaining_budget,
        'within_budget': total_time <= latency_budget_ms
    }

    return outputs
```

### 3.2 Layer 1: SafetyReflexLayer (å®‰å…¨åå°„å±‚)

**ä½ç½®**: `kuavo_train/wrapper/policy/humanoid/layers/SafetyReflexLayer.py`

#### 3.2.1 è®¾è®¡ç†å¿µ

```
ä¼˜å…ˆçº§: 1 (æœ€é«˜)
å“åº”æ—¶é—´: <10ms
æ¶æ„: æç®€GRU
æ ¸å¿ƒåŠŸèƒ½: é˜²è·Œå€’ã€ç´§æ€¥åœæ­¢ã€åŸºç¡€å¹³è¡¡
```

#### 3.2.2 ç½‘ç»œæ¶æ„

```python
class SafetyReflexLayer(BaseLayer):
    def __init__(self, config, base_config):
        super().__init__(config, "safety", priority=1)

        # è¾“å…¥ç»´åº¦: æœºå™¨äººå…³èŠ‚çŠ¶æ€
        # only_arm=True: åŒè‡‚14ç»´ + æ‰‹çˆª2ç»´ = 16ç»´
        self.input_dim = 16
        self.hidden_size = 64
        self.output_dim = 16

        # 1. æç®€GRU (åªç”¨ä¸€å±‚ç¡®ä¿é€Ÿåº¦)
        self.balance_gru = nn.GRU(
            input_size=self.input_dim,
            hidden_size=self.hidden_size,
            num_layers=1,
            batch_first=True
        )

        # 2. ç´§æ€¥æƒ…å†µæ£€æµ‹å™¨
        self.emergency_detector = nn.Sequential(
            nn.Linear(self.hidden_size, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # è¾“å‡º0-1çš„ç´§æ€¥è¯„åˆ†
        )

        # 3. å¹³è¡¡æ§åˆ¶å™¨
        self.balance_controller = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.ReLU(),
            nn.Linear(self.hidden_size, self.output_dim),
            nn.Tanh()  # é™åˆ¶è¾“å‡ºèŒƒå›´
        )

        # 4. å€¾æ–œæ£€æµ‹å™¨
        self.tilt_detector = nn.Linear(self.hidden_size, 2)  # roll, pitch

        # 5. ç´§æ€¥åŠ¨ä½œç”Ÿæˆå™¨
        self.emergency_action_generator = nn.Sequential(
            nn.Linear(self.hidden_size, self.output_dim),
            nn.Tanh()
        )
```

#### 3.2.3 å‰å‘ä¼ æ’­é€»è¾‘

```python
def forward(self, inputs, context=None):
    """å®‰å…¨åå°„å±‚å‰å‘ä¼ æ’­"""

    # 1. æå–å…³èŠ‚çŠ¶æ€
    robot_state = inputs['observation.state']
    # å¤„ç†ç»´åº¦: [batch, state_dim] -> [batch, 1, state_dim]
    if len(robot_state.shape) == 2:
        robot_state = robot_state.unsqueeze(1)

    # 2. GRUå¤„ç†
    gru_output, hidden = self.balance_gru(robot_state)
    last_output = gru_output[:, -1, :]  # [batch, hidden_size]

    # 3. ç´§æ€¥æƒ…å†µæ£€æµ‹
    emergency_score = self.emergency_detector(last_output)  # [batch, 1]
    emergency = (emergency_score > self.emergency_threshold).squeeze(-1)  # [batch]

    # 4. å€¾æ–œæ£€æµ‹
    tilt_angles = self.tilt_detector(last_output)  # [batch, 2]
    tilt_angles_degrees = tilt_angles * 45.0  # ç¼©æ”¾åˆ°Â±45åº¦
    tilt_emergency = torch.any(
        torch.abs(tilt_angles_degrees) > self.tilt_threshold_degrees,
        dim=-1
    )  # [batch]

    # 5. ç»¼åˆç´§æ€¥çŠ¶æ€
    overall_emergency = torch.logical_or(emergency, tilt_emergency)

    # 6. ç”Ÿæˆæ§åˆ¶è¾“å‡º
    emergency_action = self.emergency_action_generator(last_output)
    balance_action_normal = self.balance_controller(last_output)

    # 7. æ ¹æ®ç´§æ€¥çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
    overall_emergency_expanded = overall_emergency.unsqueeze(-1)
    balance_action = torch.where(
        overall_emergency_expanded,
        emergency_action,      # ç´§æ€¥æ—¶ä½¿ç”¨ç´§æ€¥åŠ¨ä½œ
        balance_action_normal  # æ­£å¸¸æ—¶ä½¿ç”¨å¹³è¡¡åŠ¨ä½œ
    )

    # 8. è®¡ç®—å¹³è¡¡ç½®ä¿¡åº¦
    max_tilt = torch.max(torch.abs(tilt_angles_degrees), dim=-1)[0]
    balance_confidence = torch.exp(-max_tilt / 10.0)

    return {
        'emergency': overall_emergency,
        'emergency_score': emergency_score.squeeze(-1),
        'balance_action': balance_action,
        'emergency_action': emergency_action,
        'tilt_angles_degrees': tilt_angles_degrees,
        'balance_confidence': balance_confidence,
        'safety_status': self._compute_safety_status(...),
        'action': balance_action,
        'layer': 'safety'
    }
```

#### 3.2.4 æ¿€æ´»æ¡ä»¶

```python
def should_activate(self, inputs, context=None):
    """å®‰å…¨å±‚å§‹ç»ˆæ¿€æ´»"""
    return True
```

### 3.3 Layer 2: GaitControlLayer (æ­¥æ€æ§åˆ¶å±‚)

**ä½ç½®**: `kuavo_train/wrapper/policy/humanoid/layers/GaitControlLayer.py`

#### 3.3.1 è®¾è®¡ç†å¿µ

```
ä¼˜å…ˆçº§: 2
å“åº”æ—¶é—´: ~20ms
æ¶æ„: æ··åˆGRU + è½»é‡Transformer
æ ¸å¿ƒåŠŸèƒ½: æ­¥æ€è§„åˆ’ã€è´Ÿè½½é€‚åº”ã€åœ°å½¢é€‚åº”
```

#### 3.3.2 ç½‘ç»œæ¶æ„

```python
class GaitControlLayer(BaseLayer):
    def __init__(self, config, base_config):
        super().__init__(config, "gait", priority=2)

        self.input_dim = 16  # åŒè‡‚+æ‰‹çˆªé…ç½®
        self.gru_hidden = 128
        self.gru_layers = 2
        self.tf_layers = 2
        self.tf_heads = 4

        # 1. GRUç”¨äºæ­¥æ€çŠ¶æ€è·Ÿè¸ª
        self.gait_state_gru = nn.GRU(
            input_size=self.input_dim,
            hidden_size=self.gru_hidden,
            num_layers=self.gru_layers,
            batch_first=True,
            dropout=0.1
        )

        # 2. è½»é‡Transformerç”¨äºæ­¥æ€è§„åˆ’
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.gru_hidden,
            nhead=self.tf_heads,
            dim_feedforward=self.gru_hidden * 2,
            dropout=0.1,
            batch_first=True
        )
        self.gait_planner = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.tf_layers
        )

        # 3. è´Ÿè½½é€‚åº”æ¨¡å—
        self.load_adapter = LoadAdaptationModule(self.gru_hidden)

        # 4. è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(self.gru_hidden, self.input_dim)
```

#### 3.3.3 å‰å‘ä¼ æ’­é€»è¾‘

```python
def forward(self, inputs, context=None):
    """æ­¥æ€æ§åˆ¶å‰å‘ä¼ æ’­"""

    # 1. è·å–å…³èŠ‚çŠ¶æ€
    robot_state = inputs.get('observation.state')
    # å¤„ç†ç»´åº¦: [batch, state_dim] -> [batch, seq_len, state_dim]
    if len(robot_state.shape) == 2:
        robot_state = robot_state.unsqueeze(1)
    batch_size, seq_len, state_dim = robot_state.shape

    # 2. GRUå¤„ç†æ­¥æ€çŠ¶æ€
    gru_output, gru_hidden = self.gait_state_gru(robot_state)

    # 3. Transformeræ­¥æ€è§„åˆ’ï¼ˆå¦‚æœåºåˆ—è¶³å¤Ÿé•¿ï¼‰
    if seq_len >= 10:  # è‡³å°‘200mså†å²
        planned_gait = self.gait_planner(gru_output)
    else:
        planned_gait = gru_output

    # 4. è´Ÿè½½é€‚åº”
    adapted_gait = self.load_adapter(planned_gait, context)

    # 5. æœ€ç»ˆè¾“å‡º
    final_output = self.output_projection(adapted_gait[:, -1, :])

    return {
        'gait_features': gru_output,
        'planned_gait': planned_gait,
        'adapted_gait': adapted_gait,
        'action': final_output,
        'layer': 'gait'
    }
```

#### 3.3.4 æ¿€æ´»æ¡ä»¶

```python
def should_activate(self, inputs, context=None):
    """å½“æœºå™¨äººéœ€è¦ç§»åŠ¨æ—¶æ¿€æ´»"""
    if context is None:
        return True
    return context.get('requires_locomotion', True)
```

### 3.4 Layer 3: ManipulationLayer (æ“ä½œæ§åˆ¶å±‚)

**ä½ç½®**: `kuavo_train/wrapper/policy/humanoid/layers/ManipulationLayer.py`

#### 3.4.1 è®¾è®¡ç†å¿µ

```
ä¼˜å…ˆçº§: 3
å“åº”æ—¶é—´: ~100ms
æ¶æ„: Transformerä¸»å¯¼
æ ¸å¿ƒåŠŸèƒ½: æŠ“å–ã€æ‘†æ”¾ã€åŒè‡‚åè°ƒã€çº¦æŸæ»¡è¶³
```

#### 3.4.2 ç½‘ç»œæ¶æ„

```python
class ManipulationLayer(BaseLayer):
    def __init__(self, config, base_config):
        super().__init__(config, "manipulation", priority=3)

        self.hidden_size = 512
        self.num_layers = 3
        self.num_heads = 8
        self.dim_feedforward = 2048

        # ç‰¹å¾ç»´åº¦
        self.visual_dim = 1280  # EfficientNet-B0è¾“å‡º
        self.state_dim = 16
        actual_visual_dim = 12  # 3RGB + 3æ·±åº¦ç›¸æœº

        # 1. è§†è§‰æŠ•å½±å±‚
        self.visual_projection = nn.Linear(actual_visual_dim, self.visual_dim)

        # 2. æ€»è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(
            self.visual_dim + self.state_dim,
            self.hidden_size
        )

        # 3. ä¸»è¦çš„Transformerç½‘ç»œ
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_size,
            nhead=self.num_heads,
            dim_feedforward=self.dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.manipulation_transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers
        )

        # 4. çº¦æŸæ»¡è¶³æ¨¡å—
        self.constraint_solver = ConstraintSatisfactionModule(self.hidden_size)

        # 5. åŒè‡‚åè°ƒæ¨¡å—
        self.bimanual_coordinator = BimanualCoordinationModule(
            self.hidden_size,
            self.state_dim
        )

        # 6. è¾“å‡ºæŠ•å½±
        self.action_head = nn.Linear(self.hidden_size, self.state_dim)
```

#### 3.4.3 å‰å‘ä¼ æ’­é€»è¾‘

```python
def forward(self, inputs, context=None):
    """æ“ä½œæ§åˆ¶å‰å‘ä¼ æ’­"""

    # 1. æå–å’Œèåˆå¤šæ¨¡æ€ç‰¹å¾
    features = self._extract_features(inputs)
    #    â”œâ”€â–º çŠ¶æ€ç‰¹å¾: observation.state
    #    â”œâ”€â–º è§†è§‰ç‰¹å¾: observation.images.*
    #    â””â”€â–º æ·±åº¦ç‰¹å¾: observation.depth.*
    batch_size, seq_len, _ = features.shape

    # 2. Transformerå¤„ç†
    manipulation_features = self.manipulation_transformer(features)

    # 3. çº¦æŸæ»¡è¶³
    constraint_solution = self.constraint_solver(manipulation_features, context)
    #    â””â”€â–º è¾“å‡º: constraint_satisfaction_score, constraints_met

    # 4. åŒè‡‚åè°ƒ
    coordinated_actions = self.bimanual_coordinator(manipulation_features, context)

    # 5. æœ€ç»ˆåŠ¨ä½œ
    final_action = self.action_head(manipulation_features[:, -1, :])

    return {
        'manipulation_features': manipulation_features,
        'constraint_solution': constraint_solution,
        'coordinated_actions': coordinated_actions,
        'action': final_action,
        'layer': 'manipulation'
    }
```

#### 3.4.4 ç‰¹å¾æå–è¯¦è§£

```python
def _extract_features(self, inputs):
    """æå–å¹¶èåˆå¤šæ¨¡æ€ç‰¹å¾"""
    features_list = []

    # 1. çŠ¶æ€ç‰¹å¾
    if 'observation.state' in inputs:
        state_features = inputs['observation.state']
        # ç¡®ä¿æ˜¯3D: [batch, seq_len, state_dim]
        if len(state_features.shape) == 2:
            state_features = state_features.unsqueeze(1)
        features_list.append(state_features)

    # 2. è§†è§‰ç‰¹å¾ï¼ˆå¤„ç†å¤šç›¸æœºè¾“å…¥ï¼‰
    visual_features_list = []
    image_keys = [k for k in inputs.keys()
                  if k.startswith('observation.images.')
                  or k.startswith('observation.depth')]

    for key in image_keys:
        img_feature = inputs[key]
        # å…¨å±€å¹³å‡æ± åŒ–: [batch, C, H, W] -> [batch, C]
        if len(img_feature.shape) == 4:
            img_feature = img_feature.mean(dim=(-2, -1))
        visual_features_list.append(img_feature)

    # 3. æ‹¼æ¥æ‰€æœ‰ç›¸æœºç‰¹å¾
    if visual_features_list:
        combined_visual = torch.cat(visual_features_list, dim=-1)
        # æŠ•å½±åˆ°æ ‡å‡†ç»´åº¦
        combined_visual = self.visual_projection(combined_visual)
        if len(combined_visual.shape) == 2:
            combined_visual = combined_visual.unsqueeze(1)
        features_list.append(combined_visual)
    else:
        # é›¶å¡«å……
        batch_size, seq_len = features_list[0].shape[:2]
        device = features_list[0].device
        zero_visual = torch.zeros(batch_size, seq_len, self.visual_dim, device=device)
        features_list.append(zero_visual)

    # 4. ç‰¹å¾æ‹¼æ¥å’ŒæŠ•å½±
    combined_features = torch.cat(features_list, dim=-1)
    projected_features = self.input_projection(combined_features)

    return projected_features
```

#### 3.4.5 æ¿€æ´»æ¡ä»¶

```python
def should_activate(self, inputs, context=None):
    """å½“éœ€è¦ç²¾ç»†æ“ä½œæ—¶æ¿€æ´»"""
    if context is None:
        return True
    return context.get('requires_manipulation', True)
```

### 3.5 Layer 4: GlobalPlanningLayer (å…¨å±€è§„åˆ’å±‚)

**ä½ç½®**: `kuavo_train/wrapper/policy/humanoid/layers/GlobalPlanningLayer.py`

#### 3.5.1 è®¾è®¡ç†å¿µ

```
ä¼˜å…ˆçº§: 4 (æœ€ä½ï¼Œæœ€å¤æ‚)
å“åº”æ—¶é—´: ~500ms
æ¶æ„: å¤§å‹Transformer
æ ¸å¿ƒåŠŸèƒ½: é•¿æœŸè®°å¿†ã€å¤æ‚ä»»åŠ¡è§„åˆ’ã€ä»»åŠ¡åˆ†è§£
```

#### 3.5.2 ç½‘ç»œæ¶æ„

```python
class GlobalPlanningLayer(BaseLayer):
    def __init__(self, config, base_config):
        super().__init__(config, "planning", priority=4)

        self.hidden_size = 1024
        self.num_layers = 4
        self.num_heads = 16
        self.dim_feedforward = 4096

        visual_dim = 1280
        self.state_dim = 16
        self.input_projection = nn.Linear(visual_dim + self.state_dim, self.hidden_size)

        # 1. å¤§å‹Transformerç”¨äºå¤æ‚æ¨ç†
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_size,
            nhead=self.num_heads,
            dim_feedforward=self.dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.global_transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers
        )

        # 2. é•¿æœŸè®°å¿†æ¨¡å—
        self.memory_bank = LongTermMemoryModule(self.hidden_size)

        # 3. ä»»åŠ¡åˆ†è§£æ¨¡å—
        self.task_decomposer = TaskDecompositionModule(self.hidden_size)

        # 4. è¾“å‡ºæŠ•å½±
        self.action_head = nn.Linear(self.hidden_size, self.state_dim)
        self.plan_head = nn.Linear(self.hidden_size, 64)
```

#### 3.5.3 å‰å‘ä¼ æ’­é€»è¾‘

```python
def forward(self, inputs, context=None):
    """å…¨å±€è§„åˆ’å‰å‘ä¼ æ’­"""

    # 1. ç¼–ç å…¨å±€çŠ¶æ€
    global_state = self._encode_global_state(inputs, context)
    batch_size, seq_len, _ = global_state.shape

    # 2. è®°å¿†æ£€ç´¢
    relevant_memory = self.memory_bank.retrieve(global_state)
    #    â””â”€â–º ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ä»è®°å¿†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯

    # 3. å…¨å±€æ¨ç†
    enhanced_state = torch.cat([global_state, relevant_memory], dim=-1)
    planning_output = self.global_transformer(enhanced_state)

    # 4. ä»»åŠ¡åˆ†è§£
    task_plan = self.task_decomposer(planning_output, context)
    #    â””â”€â–º è¾“å‡º: task_scores, task_priorities, num_subtasks

    # 5. è¾“å‡ºåŠ¨ä½œå’Œè§„åˆ’
    final_action = self.action_head(planning_output[:, -1, :])
    global_plan = self.plan_head(planning_output[:, -1, :])

    return {
        'global_features': planning_output,
        'task_plan': task_plan,
        'relevant_memory': relevant_memory,
        'global_plan': global_plan,
        'action': final_action,
        'layer': 'planning'
    }
```

#### 3.5.4 æ¿€æ´»æ¡ä»¶

```python
def should_activate(self, inputs, context=None):
    """å½“éœ€è¦å¤æ‚è§„åˆ’æ—¶æ¿€æ´»"""
    if context is None:
        return False  # é»˜è®¤ä¸æ¿€æ´»æœ€å¤æ‚çš„å±‚

    task_complexity = context.get('task_complexity', 'medium')
    return task_complexity in ['high', 'very_high']
```

---

## 4. è¯¾ç¨‹å­¦ä¹ æœºåˆ¶

### 4.1 è®¾è®¡ç†å¿µ

è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰æ¨¡æ‹Ÿäººç±»å­¦ä¹ è¿‡ç¨‹ï¼Œä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥æ¿€æ´»æ›´å¤šå±‚ã€‚

### 4.2 è¯¾ç¨‹å­¦ä¹ é…ç½®

```yaml
# åœ¨ humanoid_diffusion_config.yaml
hierarchical:
  curriculum_learning:
    enable: True
    universal_stages:
      stage1:
        name: 'manipulation_first'
        layers: ['manipulation']  # å…ˆå­¦æŠ“å–è¡Œä¸º
        epochs: 8
        loss_threshold: 0.1

      stage2:
        name: 'manipulation_safety'
        layers: ['manipulation', 'safety']  # åŠ å…¥å®‰å…¨çº¦æŸ
        epochs: 7
        loss_threshold: 0.08

      stage3:
        name: 'manipulation_safety_gait'
        layers: ['manipulation', 'safety', 'gait']  # æ·»åŠ æ­¥æ€
        epochs: 5
        loss_threshold: 0.06

      stage4:
        name: 'full_hierarchy'
        layers: ['safety', 'gait', 'manipulation', 'planning']  # å…¨å¼€
        epochs: 10
        loss_threshold: 0.05
```

### 4.3 è¯¾ç¨‹å­¦ä¹ æ‰§è¡Œæµç¨‹

```python
# åœ¨ train_hierarchical_policy.py
curriculum_stages = task_manager.get_current_curriculum_stages()

current_step = steps
for stage_name, stage_config in curriculum_stages.items():
    print(f"ğŸ“ å¼€å§‹è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: {stage_name}")
    print(f"   æ¿€æ´»å±‚: {stage_config['layers']}")
    print(f"   è®­ç»ƒè½®æ¬¡: {stage_config['epochs']}")

    # æ‰§è¡Œè¯¥é˜¶æ®µ
    current_step = run_curriculum_learning_stage(
        policy, stage_config, dataset, cfg, device, writer,
        current_step, optimizer, lr_scheduler, scaler,
        output_directory, amp_enabled, task_manager, dataloader
    )
```

### 4.4 é˜¶æ®µå†…éƒ¨æ‰§è¡Œ

```python
def run_curriculum_learning_stage(...):
    # 1. æ¿€æ´»æŒ‡å®šçš„å±‚
    policy.set_curriculum_stage(enabled_layers)
    #    â””â”€â–º æ›´æ–° policy.enabled_layers

    # 2. Epochå¾ªç¯
    for epoch in range(stage_epochs):
        for batch in dataloader:
            # 3. æ„å»ºè¯¾ç¨‹å­¦ä¹ ä¿¡æ¯
            curriculum_info = {
                'stage': stage_name,
                'enabled_layers': enabled_layers
            }

            # 4. å‰å‘ä¼ æ’­ï¼ˆåªè®¡ç®—æ¿€æ´»å±‚çš„æŸå¤±ï¼‰
            loss, hierarchical_info = policy.forward(batch, curriculum_info)

            # 5. åå‘ä¼ æ’­
            scaled_loss.backward()
            optimizer.step()
            lr_scheduler.step()

        # 6. ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_stage_loss:
            policy.save_pretrained(best_save_path)
```

### 4.5 Policyä¸­çš„è¯¾ç¨‹å­¦ä¹ å¤„ç†

```python
# åœ¨ HumanoidDiffusionPolicy.py
def _hierarchical_forward(self, batch, curriculum_info, task_weights):
    # 1. æ›´æ–°è¯¾ç¨‹å­¦ä¹ çŠ¶æ€
    self._update_curriculum_state(curriculum_info)
    #    â””â”€â–º æ›´æ–° self.enabled_layers

    # 2. åˆ†å±‚å¤„ç†
    layer_outputs = self.scheduler(batch, task_info)

    # 3. DiffusionæŸå¤±è®¡ç®—
    diffusion_loss = self.diffusion.compute_loss(batch, layer_outputs)

    # 4. åˆ†å±‚æŸå¤±èšåˆï¼ˆåªè®¡ç®—æ¿€æ´»å±‚çš„æŸå¤±ï¼‰
    total_loss = diffusion_loss
    for layer_name, layer_output in layer_outputs.items():
        if layer_name in self.enabled_layers:  # åªè®¡ç®—æ¿€æ´»å±‚
            layer_weight = self.task_layer_weights.get(layer_name, 1.0)
            layer_loss = layer_output['loss']
            total_loss = total_loss + layer_weight * layer_loss

    return total_loss, hierarchical_info
```

---

## 5. ä»»åŠ¡ç‰¹å®šè®­ç»ƒ

### 5.1 TaskSpecificTrainingManager

**ä½ç½®**: `kuavo_train/wrapper/policy/humanoid/TaskSpecificTrainingManager.py`

#### 5.1.1 æ ¸å¿ƒèŒè´£

```python
class TaskSpecificTrainingManager:
    """ä»»åŠ¡ç‰¹å®šè®­ç»ƒç®¡ç†å™¨ï¼Œç®¡ç†åˆ†å±‚æ¶æ„åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„è®­ç»ƒ"""
```

1. **ä»»åŠ¡ç®¡ç†**: æ³¨å†Œå’Œç®¡ç†å¯ç”¨ä»»åŠ¡æ•°æ®
2. **è¯¾ç¨‹å­¦ä¹ ç”Ÿæˆ**: ä¸ºä¸åŒä»»åŠ¡ç”Ÿæˆç‰¹å®šçš„è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
3. **æƒé‡è°ƒæ•´**: ä»»åŠ¡ç‰¹å®šçš„å±‚æƒé‡é…ç½®
4. **é˜²é—å¿˜**: æ•°æ®é‡‡æ ·ç­–ç•¥å’Œé‡æ”¾æœºåˆ¶
5. **çŠ¶æ€ä¿å­˜**: ä¿å­˜å’Œæ¢å¤ä»»åŠ¡è®­ç»ƒçŠ¶æ€

#### 5.1.2 ä»»åŠ¡å®šä¹‰

```python
# é¢„å®šä¹‰ä»»åŠ¡é…ç½®
self.task_definitions = {
    1: TaskInfo(
        task_id=1,
        name="dynamic_grasping",  # åŠ¨æ€æŠ“å–
        complexity_level=2,
        required_layers=["safety", "manipulation"],
        primary_capabilities=["object_detection", "trajectory_tracking", "grasp_control"]
    ),
    2: TaskInfo(
        task_id=2,
        name="package_weighing",  # ç§°é‡
        complexity_level=3,
        required_layers=["safety", "gait", "manipulation"],
        primary_capabilities=["dual_arm_coordination", "weight_estimation", "balance_control"]
    ),
    3: TaskInfo(
        task_id=3,
        name="precise_placement",  # æ‘†æ”¾
        complexity_level=3,
        required_layers=["safety", "manipulation", "planning"],
        primary_capabilities=["spatial_reasoning", "orientation_control", "precision_placement"]
    ),
    4: TaskInfo(
        task_id=4,
        name="full_process_sorting",  # åˆ†æ‹£
        complexity_level=4,
        required_layers=["safety", "gait", "manipulation", "planning"],
        primary_capabilities=["whole_body_coordination", "sequence_planning", "multi_modal_control"]
    )
}
```

#### 5.1.3 ä»»åŠ¡ç‰¹å®šå±‚æƒé‡

```python
self.task_layer_weights = {
    1: {  # åŠ¨æ€æŠ“å–ï¼šå¼ºè°ƒæ“ä½œå’Œå®‰å…¨
        "safety": 2.0,
        "gait": 0.5,        # ä½æƒé‡
        "manipulation": 2.0,  # é«˜æƒé‡
        "planning": 0.8
    },
    2: {  # ç§°é‡ï¼šå¼ºè°ƒæ­¥æ€å’Œå¹³è¡¡
        "safety": 2.0,
        "gait": 1.8,        # é«˜æƒé‡
        "manipulation": 1.5,
        "planning": 1.0
    },
    3: {  # æ‘†æ”¾ï¼šå¼ºè°ƒæ“ä½œå’Œè§„åˆ’
        "safety": 2.0,
        "gait": 0.8,
        "manipulation": 1.8,  # é«˜æƒé‡
        "planning": 2.0      # é«˜æƒé‡
    },
    4: {  # åˆ†æ‹£ï¼šå¹³è¡¡æ‰€æœ‰å±‚
        "safety": 2.0,
        "gait": 1.5,
        "manipulation": 1.5,
        "planning": 1.5
    }
}
```

#### 5.1.4 ä»»åŠ¡ç‰¹å®šè¯¾ç¨‹å­¦ä¹ 

```python
self.task_curriculum_stages = {
    1: {  # åŠ¨æ€æŠ“å– - å¿«é€Ÿååº”å¯¼å‘
        "stage1": {
            "name": "safety_reflex",
            "layers": ["safety"],
            "epochs": 30
        },
        "stage2": {
            "name": "basic_manipulation",
            "layers": ["safety", "manipulation"],
            "epochs": 70
        },
        "stage3": {
            "name": "full_grasping",
            "layers": ["safety", "gait", "manipulation"],
            "epochs": 100
        }
    },
    2: {  # ç§°é‡ - å¹³è¡¡åè°ƒå¯¼å‘
        "stage1": {"name": "safety_base", "layers": ["safety"], "epochs": 25},
        "stage2": {"name": "gait_control", "layers": ["safety", "gait"], "epochs": 50},
        "stage3": {"name": "dual_arm_coord", "layers": ["safety", "gait", "manipulation"], "epochs": 75},
        "stage4": {"name": "full_weighing", "layers": ["safety", "gait", "manipulation", "planning"], "epochs": 100}
    },
    # ... ä»»åŠ¡3ã€4çš„é…ç½®
}
```

### 5.2 æ¸è¿›å¼å¤šä»»åŠ¡è®­ç»ƒ

```python
def _build_progressive_curriculum(self):
    """æ„å»ºæ¸è¿›å¼å¤šä»»åŠ¡è¯¾ç¨‹å­¦ä¹ """
    stages = {}
    stage_counter = 1

    # æŒ‰ä»»åŠ¡å¤æ‚åº¦æ’åº
    sorted_tasks = sorted(self.available_tasks,
                         key=lambda x: self.task_definitions[x].complexity_level)

    # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸“é—¨çš„é€‚åº”é˜¶æ®µ
    for task_id in sorted_tasks:
        task_info = self.task_definitions[task_id]
        task_stages = self.task_curriculum_stages[task_id]

        for stage_name, stage_config in task_stages.items():
            adapted_stage_name = f"stage{stage_counter}_{task_info.name}"
            stages[adapted_stage_name] = {
                **stage_config,
                "name": adapted_stage_name,
                "target_task": task_id,
                "task_weight": 1.0 / len(self.available_tasks),
                "epochs": max(10, stage_config["epochs"] // len(self.available_tasks))
            }
            stage_counter += 1

    # æ·»åŠ æœ€ç»ˆèåˆé˜¶æ®µ
    stages[f"stage{stage_counter}_integration"] = {
        "name": "multi_task_integration",
        "layers": ["safety", "gait", "manipulation", "planning"],
        "epochs": 50,
        "target_task": "all",
        "task_weight": "balanced"
    }

    return stages
```

### 5.3 å¤šä»»åŠ¡æ•°æ®é‡‡æ ·

```python
def get_task_data_sampling_strategy(self):
    """è·å–ä»»åŠ¡æ•°æ®é‡‡æ ·ç­–ç•¥"""
    sampling_weights = {}
    total_episodes = sum(self.task_definitions[tid].episode_count
                        for tid in self.available_tasks)

    for task_id in self.available_tasks:
        task_info = self.task_definitions[task_id]

        # åŸºäºå¤æ‚åº¦å’Œæ•°æ®é‡çš„æƒé‡è®¡ç®—
        complexity_factor = task_info.complexity_level / 4.0
        data_factor = task_info.episode_count / total_episodes

        # å¹³è¡¡å¤æ‚åº¦å’Œæ•°æ®å¯ç”¨æ€§
        sampling_weights[task_id] = 0.6 * complexity_factor + 0.4 * data_factor

    # å½’ä¸€åŒ–æƒé‡
    total_weight = sum(sampling_weights.values())
    sampling_weights = {k: v/total_weight for k, v in sampling_weights.items()}

    return {
        "strategy": "weighted_sampling",
        "task_weights": sampling_weights,
        "anti_forgetting": True,
        "rehearsal_ratio": 0.2  # 20%çš„æ•°æ®ç”¨äºé˜²é—å¿˜
    }
```

### 5.4 åˆ›å»ºä»»åŠ¡ç‰¹å®šDataLoader

```python
def create_task_specific_dataloader(datasets, task_manager, cfg, device):
    """åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„æ•°æ®åŠ è½½å™¨ï¼ˆå¤šä»»åŠ¡åŠ æƒé‡‡æ ·ï¼‰"""

    # å¤šä»»åŠ¡æƒ…å†µ - ä½¿ç”¨åŠ æƒé‡‡æ ·
    sampling_strategy = task_manager.get_task_data_sampling_strategy()
    task_weights = sampling_strategy.get("task_weights", {})

    combined_datasets = []
    sample_weights = []

    for task_id, dataset in datasets.items():
        task_weight = task_weights.get(task_id, 1.0 / len(datasets))
        dataset_size = len(dataset)

        combined_datasets.append(dataset)
        sample_weights.extend([task_weight] * dataset_size)

    combined_dataset = ConcatDataset(combined_datasets)

    # åŠ æƒéšæœºé‡‡æ ·å™¨
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(combined_dataset),
        replacement=True
    )

    return DataLoader(
        combined_dataset,
        batch_size=cfg.training.batch_size,
        sampler=sampler,
        num_workers=cfg.training.num_workers,
        pin_memory=(device.type != "cpu"),
        drop_last=cfg.training.drop_last
    )
```

---

## 6. æ¨ç†é€»è¾‘

### 6.1 åŠ¨ä½œé€‰æ‹©æµç¨‹ (Inference)

```python
# åœ¨ HumanoidDiffusionPolicy.py
def select_action(self, batch):
    """é€‰æ‹©åŠ¨ä½œï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰"""
    if self.use_hierarchical:
        return self._hierarchical_select_action(batch)
    else:
        return super().select_action(batch)
```

### 6.2 åˆ†å±‚æ¨ç†æµç¨‹

```python
def _hierarchical_select_action(self, batch):
    """åˆ†å±‚æ¶æ„çš„åŠ¨ä½œé€‰æ‹©"""

    # 1. é¢„å¤„ç†
    batch = self._preprocess_batch(batch)
    batch = self.normalize_inputs(batch)

    # 2. ä»»åŠ¡è¯†åˆ«
    task_info = self._identify_task(batch)
    #    â””â”€â–º æ ¹æ®è§‚æµ‹æ¨æ–­ä»»åŠ¡ç±»å‹

    # 3. åˆ†å±‚æ¨ç†
    with torch.no_grad():
        layer_outputs = self.scheduler(batch, task_info)

    # 4. ä¿å­˜layer_outputsä¾›æ—¥å¿—è®°å½•
    self._last_layer_outputs = layer_outputs

    # 5. ä»åˆ†å±‚è¾“å‡ºä¸­æå–æœ€ç»ˆåŠ¨ä½œ
    return self._extract_action_from_layers(layer_outputs, batch)
```

### 6.3 åŠ¨ä½œæå–é€»è¾‘

```python
def _extract_action_from_layers(self, layer_outputs, batch):
    """ä»åˆ†å±‚è¾“å‡ºä¸­æå–æœ€ç»ˆåŠ¨ä½œ"""

    # 1. ä¼˜å…ˆçº§å¤„ç†ï¼šå®‰å…¨å±‚å¯ä»¥è¦†ç›–å…¶ä»–å±‚çš„è¾“å‡º
    if 'safety' in layer_outputs:
        is_emergency = layer_outputs['safety'].get('emergency', False)
        if is_emergency:
            # è¿”å›ç´§æ€¥åŠ¨ä½œ
            return layer_outputs['safety'].get('emergency_action', torch.zeros_like(...))

    # 2. æ­£å¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨æœ€é«˜çº§åˆ«å¯ç”¨å±‚çš„è¾“å‡º
    for layer_name in ['planning', 'manipulation', 'gait', 'safety']:
        if layer_name in layer_outputs and 'action' in layer_outputs[layer_name]:
            return layer_outputs[layer_name]['action']

    # 3. å›é€€ï¼šä½¿ç”¨ä¼ ç»Ÿdiffusionè¾“å‡º
    return super().select_action(batch)
```

### 6.4 æ¨ç†æ¨¡å¼ï¼ˆInference Modeï¼‰

```python
# åœ¨ HierarchicalScheduler.py
def inference_mode(self, batch, task_info, latency_budget_ms=50.0):
    """æ ¹æ®å»¶è¿Ÿé¢„ç®—è‡ªé€‚åº”æ¿€æ´»å±‚"""

    remaining_budget = latency_budget_ms
    outputs = {}

    # æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†ï¼Œåœ¨é¢„ç®—å†…å°½å¯èƒ½å¤šåœ°æ¿€æ´»å±‚
    for layer_name in self._get_processing_order():
        layer = self.layers[layer_name]

        # æ£€æŸ¥æ—¶é—´é¢„ç®—
        layer_budget = layer.get_latency_budget()
        if remaining_budget < layer_budget:
            print(f"â° Skipping {layer_name} due to time budget")
            continue

        # æ‰§è¡Œå±‚æ¨ç†
        start_time = time.time()
        layer_output = layer.forward_with_timing(batch, context)
        outputs[layer_name] = layer_output

        # æ›´æ–°å‰©ä½™é¢„ç®—
        layer_time = (time.time() - start_time) * 1000
        remaining_budget -= layer_time

        # å®‰å…¨å±‚ç´§æ€¥æƒ…å†µç«‹å³è¿”å›
        if layer_name == 'safety' and layer_output.get('emergency'):
            break

    return outputs
```

### 6.5 å®æ—¶éƒ¨ç½²ç¤ºä¾‹

```python
# åœ¨ kuavo_deploy/examples/eval/eval_kuavo.py
policy = HumanoidDiffusionPolicy.from_pretrained(
    checkpoint_path,
    use_hierarchical=True,
    hierarchical=hierarchical_config
)

# æ¨ç†å¾ªç¯
for step in range(max_steps):
    # 1. è·å–è§‚æµ‹
    obs = env.get_observation()

    # 2. æ·»åŠ å»¶è¿Ÿé¢„ç®—åˆ°task_infoï¼ˆå¦‚æœéœ€è¦ï¼‰
    task_info = {
        'latency_budget_ms': 50.0,  # 50msé¢„ç®—
        'requires_locomotion': True,
        'requires_manipulation': True
    }

    # 3. é€‰æ‹©åŠ¨ä½œ
    action = policy.select_action(obs)

    # 4. æ‰§è¡ŒåŠ¨ä½œ
    env.step(action)

    # 5. è®°å½•å±‚è¾“å‡ºï¼ˆå¯é€‰ï¼‰
    layer_outputs = policy.get_last_layer_outputs()
    if layer_outputs:
        log_layer_performance(layer_outputs)
```

---

## 7. é…ç½®ç³»ç»Ÿ

### 7.1 ä¸»é…ç½®æ–‡ä»¶

**ä½ç½®**: `configs/policy/humanoid_diffusion_config.yaml`

### 7.2 å…³é”®é…ç½®é¡¹

#### 7.2.1 åŸºç¡€è®­ç»ƒé…ç½®

```yaml
training:
  output_directory: 'outputs/train/${task}/${method}'
  seed: 42
  max_epoch: 500
  batch_size: 64
  num_workers: 25
  accumulation_steps: 1

  # æµ‹è¯•æ¨¡å¼
  test_training_mode: False
  test_training_epochs: 10
```

#### 7.2.2 åˆ†å±‚æ¶æ„é…ç½®

```yaml
policy:
  use_hierarchical: True

  hierarchical:
    # å››å±‚é…ç½®
    layers:
      safety:
        type: 'GRU'
        hidden_size: 64
        num_layers: 1
        input_dim: 16
        output_dim: 16
        response_time_ms: 10
        priority: 1

      gait:
        type: 'Hybrid'
        gru_hidden: 128
        tf_layers: 2
        priority: 2

      manipulation:
        type: 'Transformer'
        layers: 3
        hidden_size: 512
        priority: 3

      planning:
        type: 'Transformer'
        layers: 4
        hidden_size: 1024
        priority: 4

    # å±‚æƒé‡
    layer_weights:
      safety: 1.0
      gait: 1.0
      manipulation: 2.0  # æŠ“å–ä»»åŠ¡é«˜æƒé‡
      planning: 0.5

    # è¯¾ç¨‹å­¦ä¹ 
    curriculum_learning:
      enable: True
      universal_stages:
        stage1:
          name: 'manipulation_first'
          layers: ['manipulation']
          epochs: 8
        stage2:
          name: 'manipulation_safety'
          layers: ['manipulation', 'safety']
          epochs: 7
        # ...
```

#### 7.2.3 ä»»åŠ¡ç‰¹å®šè®­ç»ƒé…ç½®

```yaml
task_specific_training:
  enable: True

  data_config:
    base_path: '/robot/data'
    task_directories:
      1: 'task-1/1-2000/lerobot'
      2: 'task-2/lerobot'
      3: 'task-3/lerobot'
      4: 'task-4/lerobot'

  current_phase: 1

  training_strategy:
    progressive_learning: True
    min_epochs_per_phase: 10
    phase_transition_loss_threshold: 0.08

  memory_management:
    max_concurrent_tasks: 2
    max_episodes_per_task: 300
```

### 7.3 é…ç½®åŠ è½½æµç¨‹

```python
# åœ¨ train_hierarchical_policy.py
@hydra.main(config_path="../configs/policy/",
            config_name="humanoid_diffusion_config")
def main(cfg: DictConfig):
    # 1. Hydraè‡ªåŠ¨åŠ è½½é…ç½®

    # 2. æ£€æŸ¥è®­ç»ƒæ¨¡å¼
    use_task_specific = cfg.get('task_specific_training', {}).get('enable', False)

    # 3. æ„å»ºpolicyé…ç½®
    policy_cfg = build_policy_config(cfg, input_features, output_features)

    # 4. å®ä¾‹åŒ–policy
    policy = HumanoidDiffusionPolicy(policy_cfg, dataset_stats)
    #    â””â”€â–º å†…éƒ¨è¯»å– hierarchical é…ç½®
```

---

## 8. å…³é”®è®¾è®¡å†³ç­–

### 8.1 ä¸ºä»€ä¹ˆä¸è¿›è¡Œç‰¹å¾èåˆï¼Ÿ

**è®¾è®¡å†³ç­–**: åˆ†å±‚æ¶æ„çš„ä»·å€¼åœ¨äº**è¯¾ç¨‹å­¦ä¹ **å’Œ**å±‚é—´åè°ƒ**ï¼Œè€Œä¸æ˜¯ç‰¹å¾èåˆã€‚

**åŸå› :**
1. **Diffusionæ¨¡å‹çš„ç‹¬ç«‹æ€§**: Diffusionæ¨¡å‹æœ¬èº«å·²ç»å¾ˆå¼ºå¤§ï¼Œç›´æ¥èåˆå±‚ç‰¹å¾å¯èƒ½ç ´åå…¶å†…éƒ¨ç»“æ„
2. **æŸå¤±èšåˆæ›´å®‰å…¨**: é€šè¿‡åŠ æƒæŸå¤±èšåˆï¼Œå„å±‚å¯ä»¥ç‹¬ç«‹å­¦ä¹ ï¼Œé¿å…æ¢¯åº¦å†²çª
3. **è¯¾ç¨‹å­¦ä¹ çš„æ ¸å¿ƒ**: æ¸è¿›å¼æ¿€æ´»å±‚æ‰æ˜¯å…³é”®ï¼Œè€Œä¸æ˜¯ç‰¹å¾æ‹¼æ¥

**ä»£ç ä½“ç°:**
```python
# åœ¨ HierarchicalDiffusionModel.py
def compute_loss(self, batch, layer_outputs=None):
    """ç›´æ¥ä½¿ç”¨åŸå§‹æ‰¹æ¬¡è®¡ç®—æŸå¤±ï¼Œä¸èåˆlayer_outputs"""
    return super().compute_loss(batch)
```

### 8.2 ä¸ºä»€ä¹ˆå®‰å…¨å±‚å§‹ç»ˆæ¿€æ´»ï¼Ÿ

**è®¾è®¡å†³ç­–**: SafetyReflexLayer åœ¨æ‰€æœ‰æ¨¡å¼ä¸‹å§‹ç»ˆæ¿€æ´»ã€‚

**åŸå› :**
1. **å®‰å…¨ç¬¬ä¸€**: é˜²è·Œå€’å’Œç´§æ€¥åœæ­¢æ˜¯æœºå™¨äººçš„åŸºæœ¬è¦æ±‚
2. **æœ€é«˜ä¼˜å…ˆçº§**: å¯ä»¥è¦†ç›–å…¶ä»–å±‚çš„è¾“å‡º
3. **ä½å»¶è¿Ÿ**: <10mså“åº”æ—¶é—´ï¼Œä¸å½±å“æ•´ä½“æ€§èƒ½

**ä»£ç ä½“ç°:**
```python
def should_activate(self, inputs, context=None):
    """å®‰å…¨å±‚å§‹ç»ˆæ¿€æ´»"""
    return True
```

### 8.3 ä¸ºä»€ä¹ˆä½¿ç”¨æ··åˆæ¶æ„ï¼Ÿ

**è®¾è®¡å†³ç­–**: ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„ç½‘ç»œæ¶æ„ï¼ˆGRUã€Transformerã€æ··åˆï¼‰ã€‚

**åŸå› :**
1. **å“åº”æ—¶é—´éœ€æ±‚ä¸åŒ**: å®‰å…¨å±‚éœ€è¦æå¿«ï¼ˆGRUï¼‰ï¼Œè§„åˆ’å±‚å¯ä»¥æ…¢ï¼ˆå¤§Transformerï¼‰
2. **ä»»åŠ¡æ€§è´¨ä¸åŒ**: æ­¥æ€éœ€è¦åºåˆ—å»ºæ¨¡ï¼ˆGRU+Transformerï¼‰ï¼Œæ“ä½œéœ€è¦å…¨å±€æ³¨æ„åŠ›ï¼ˆTransformerï¼‰
3. **å‚æ•°æ•ˆç‡**: æ ¹æ®å±‚çš„é‡è¦æ€§åˆ†é…ä¸åŒçš„æ¨¡å‹å®¹é‡

| å±‚ | æ¶æ„ | å‚æ•°é‡ | å“åº”æ—¶é—´ |
|---|---|---|---|
| Safety | 1å±‚GRU | ~10K | <10ms |
| Gait | 2å±‚GRU + 2å±‚Transformer | ~200K | ~20ms |
| Manipulation | 3å±‚Transformer | ~2M | ~100ms |
| Planning | 4å±‚Transformer | ~10M | ~500ms |

### 8.4 ä¸ºä»€ä¹ˆéœ€è¦ä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼Ÿ

**è®¾è®¡å†³ç­–**: æ”¯æŒå¤šä»»åŠ¡åœºæ™¯ï¼Œæ¯ä¸ªä»»åŠ¡æœ‰ç‰¹å®šçš„å±‚æƒé‡å’Œè¯¾ç¨‹å­¦ä¹ ã€‚

**åŸå› :**
1. **ä»»åŠ¡å¤šæ ·æ€§**: åŠ¨æ€æŠ“å–ã€ç§°é‡ã€æ‘†æ”¾ã€åˆ†æ‹£éœ€è¦ä¸åŒçš„èƒ½åŠ›ç»„åˆ
2. **é˜²æ­¢ç¾éš¾æ€§é—å¿˜**: é€šè¿‡åŠ æƒé‡‡æ ·å’Œé‡æ”¾æœºåˆ¶
3. **æ¸è¿›å¼å­¦ä¹ **: ä»ç®€å•ä»»åŠ¡åˆ°å¤æ‚ä»»åŠ¡ï¼Œé€æ­¥æ‰©å±•èƒ½åŠ›

**æ•ˆæœ:**
- ä»»åŠ¡1ï¼ˆæŠ“å–ï¼‰: å¼ºåŒ– manipulation å±‚
- ä»»åŠ¡2ï¼ˆç§°é‡ï¼‰: å¼ºåŒ– gait å’Œ manipulation å±‚
- ä»»åŠ¡3ï¼ˆæ‘†æ”¾ï¼‰: å¼ºåŒ– manipulation å’Œ planning å±‚
- ä»»åŠ¡4ï¼ˆåˆ†æ‹£ï¼‰: å¹³è¡¡æ‰€æœ‰å±‚

### 8.5 ä¸ºä»€ä¹ˆä½¿ç”¨åŠ æƒæŸå¤±èšåˆï¼Ÿ

**è®¾è®¡å†³ç­–**: æ€»æŸå¤± = DiffusionæŸå¤± + Î£(å±‚æƒé‡ Ã— å±‚æŸå¤±)

**åŸå› :**
1. **çµæ´»æ€§**: å¯ä»¥åŠ¨æ€è°ƒæ•´å„å±‚çš„è®­ç»ƒå¼ºåº¦
2. **è¯¾ç¨‹å­¦ä¹ **: åªè®¡ç®—æ¿€æ´»å±‚çš„æŸå¤±
3. **ä»»åŠ¡ç‰¹å®š**: ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒçš„å±‚æƒé‡

**ä»£ç ä½“ç°:**
```python
def _aggregate_hierarchical_loss(self, diffusion_loss, layer_outputs, use_task_weights=False):
    total_loss = diffusion_loss

    for layer_name, layer_output in layer_outputs.items():
        if layer_name in self.enabled_layers:  # åªè®¡ç®—æ¿€æ´»å±‚
            layer_weight = self.task_layer_weights.get(layer_name, 1.0)
            layer_loss = layer_output['loss']
            total_loss = total_loss + layer_weight * layer_loss

    return total_loss
```

---

## é™„å½•

### A. æ–‡ä»¶ç»“æ„

```
kuavo_data_challenge/
â”œâ”€â”€ kuavo_train/
â”‚   â”œâ”€â”€ train_hierarchical_policy.py  # è®­ç»ƒä¸»å…¥å£
â”‚   â””â”€â”€ wrapper/
â”‚       â””â”€â”€ policy/
â”‚           â”œâ”€â”€ diffusion/
â”‚           â”‚   â”œâ”€â”€ DiffusionPolicyWrapper.py
â”‚           â”‚   â”œâ”€â”€ DiffusionConfigWrapper.py
â”‚           â”‚   â””â”€â”€ DiffusionModelWrapper.py
â”‚           â””â”€â”€ humanoid/
â”‚               â”œâ”€â”€ HumanoidDiffusionPolicy.py  # ä¸»ç­–ç•¥
â”‚               â”œâ”€â”€ HierarchicalScheduler.py    # å±‚è°ƒåº¦å™¨
â”‚               â”œâ”€â”€ HierarchicalDiffusionModel.py
â”‚               â”œâ”€â”€ TaskSpecificTrainingManager.py
â”‚               â””â”€â”€ layers/
â”‚                   â”œâ”€â”€ BaseLayer.py
â”‚                   â”œâ”€â”€ SafetyReflexLayer.py
â”‚                   â”œâ”€â”€ GaitControlLayer.py
â”‚                   â”œâ”€â”€ ManipulationLayer.py
â”‚                   â””â”€â”€ GlobalPlanningLayer.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ policy/
â”‚       â””â”€â”€ humanoid_diffusion_config.yaml
â””â”€â”€ kuavo_deploy/
    â””â”€â”€ examples/
        â””â”€â”€ eval/
            â””â”€â”€ eval_kuavo.py  # æ¨ç†ç¤ºä¾‹
```

### B. è®­ç»ƒå‘½ä»¤

```bash
# åŸºç¡€è®­ç»ƒï¼ˆå•ä»»åŠ¡ï¼Œè¯¾ç¨‹å­¦ä¹ ï¼‰
python kuavo_train/train_hierarchical_policy.py \
  --config-name=humanoid_diffusion_config

# ä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼ˆéœ€è¦åœ¨é…ç½®ä¸­è®¾ç½® task_specific_training.enable=Trueï¼‰
python kuavo_train/train_hierarchical_policy.py \
  --config-name=humanoid_diffusion_config

# æµ‹è¯•æ¨¡å¼ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
python kuavo_train/train_hierarchical_policy.py \
  --config-name=humanoid_diffusion_config \
  training.test_training_mode=True \
  training.test_training_epochs=2
```

### C. æ¨ç†å‘½ä»¤

```bash
# ä½¿ç”¨åˆ†å±‚æ¶æ„æ¨ç†
python kuavo_deploy/examples/eval/eval_kuavo.py \
  --checkpoint path/to/checkpoint \
  --use-hierarchical \
  --latency-budget 50  # 50mså»¶è¿Ÿé¢„ç®—
```

### D. æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|---|---|---|
| æ€»å‚æ•°é‡ | ~12M | åŒ…å«æ‰€æœ‰å±‚ |
| è®­ç»ƒbatch size | 64 | å¯æ ¹æ®GPUè°ƒæ•´ |
| è®­ç»ƒepochs | 500 | è¯¾ç¨‹å­¦ä¹ åç»§ç»­è®­ç»ƒ |
| è¯¾ç¨‹å­¦ä¹ é˜¶æ®µæ•° | 4 | å¯é…ç½® |
| æ¨ç†å»¶è¿Ÿï¼ˆå…¨å±‚ï¼‰ | ~600ms | æ‰€æœ‰å±‚æ¿€æ´» |
| æ¨ç†å»¶è¿Ÿï¼ˆæ ¸å¿ƒå±‚ï¼‰ | ~130ms | safety+gait+manipulation |
| æ¨ç†å»¶è¿Ÿï¼ˆæœ€å°ï¼‰ | <10ms | ä»…safetyå±‚ |

---

## è”ç³»ä¸æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥é˜…ï¼š
- é¡¹ç›®README: `/Users/HarowrdLiu/learn/robot/kuavo_data_challenge/README.md`
- ä»£ç æ³¨é‡Š: å„æ¨¡å—å‡æœ‰è¯¦ç»†æ³¨é‡Š
- è®­ç»ƒæ—¥å¿—: `hierarchical_training.log`

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-10-10
**ä½œè€…**: AI Assistant

