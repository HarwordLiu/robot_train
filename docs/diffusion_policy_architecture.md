# Diffusion Policy æ¶æ„è¯¦è§£

> **ä½œè€…**: AI Assistant
> **æ—¥æœŸ**: 2025-10-10
> **ç‰ˆæœ¬**: 1.0
> **é€‚ç”¨äº**: `kuavo_data_challenge` é¡¹ç›®

---

## ğŸ“‹ ç›®å½•

1. [æ¶æ„æ¦‚è§ˆ](#1-æ¶æ„æ¦‚è§ˆ)
2. [è®­ç»ƒä¸»æµç¨‹](#2-è®­ç»ƒä¸»æµç¨‹)
3. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#3-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
4. [Transformeræ¶æ„](#4-transformeræ¶æ„)
5. [å¤šæ¨¡æ€èåˆ](#5-å¤šæ¨¡æ€èåˆ)
6. [æ‰©æ•£è¿‡ç¨‹](#6-æ‰©æ•£è¿‡ç¨‹)
7. [æ¨ç†é€»è¾‘](#7-æ¨ç†é€»è¾‘)
8. [é…ç½®ç³»ç»Ÿ](#8-é…ç½®ç³»ç»Ÿ)
9. [å…³é”®è®¾è®¡å†³ç­–](#9-å…³é”®è®¾è®¡å†³ç­–)

---

## 1. æ¶æ„æ¦‚è§ˆ

### 1.1 æ•´ä½“è®¾è®¡ç†å¿µ

Diffusion Policy æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æœºå™¨äººæ§åˆ¶ç­–ç•¥ï¼Œå°†æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰åº”ç”¨äºåŠ¨ä½œç”Ÿæˆã€‚

**æ ¸å¿ƒæ€æƒ³**:
- **æ‰©æ•£è¿‡ç¨‹**: å°†åŠ¨ä½œåºåˆ—è§†ä¸ºä»å™ªå£°é€æ­¥å»å™ªçš„è¿‡ç¨‹
- **æ¡ä»¶ç”Ÿæˆ**: æ ¹æ®è§‚æµ‹ï¼ˆRGBå›¾åƒã€æ·±åº¦å›¾ã€æœºå™¨äººçŠ¶æ€ï¼‰ç”ŸæˆåŠ¨ä½œ
- **å¤šæ¨¡æ€èåˆ**: æ•´åˆè§†è§‰å’Œæœ¬ä½“æ„ŸçŸ¥ä¿¡æ¯
- **Transformer backbone**: ä½¿ç”¨Transformerå¤„ç†åºåˆ—æ•°æ®

### 1.2 æ ¸å¿ƒç»„ä»¶å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     train_policy.py                             â”‚
â”‚                     (è®­ç»ƒä¸»å…¥å£)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚CustomDiffusionPolicyWrapperâ”‚  â”‚ Noise Scheduler (DDPM/DDIM) â”‚
â”‚   (Policyä¸»æ§åˆ¶å™¨)      â”‚   â”‚   (æ‰©æ•£è°ƒåº¦å™¨)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚CustomDiffusionModelWrapperâ”‚
â”‚   (æ ¸å¿ƒæ‰©æ•£æ¨¡å‹)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç‰¹å¾ç¼–ç å™¨            â”‚          â”‚    å»å™ªç½‘ç»œ              â”‚
â”‚                        â”‚          â”‚                          â”‚
â”‚ â”œâ”€â–º RGB Encoder        â”‚          â”‚ â”œâ”€â–º U-Net (å¯é€‰)        â”‚
â”‚ â”‚   (ResNet + Spatial  â”‚          â”‚ â”‚                       â”‚
â”‚ â”‚    SoftMax)          â”‚          â”‚ â””â”€â–º Transformer         â”‚
â”‚ â”‚                      â”‚          â”‚     (ä¸»è¦ä½¿ç”¨)           â”‚
â”‚ â”œâ”€â–º Depth Encoder      â”‚          â”‚                          â”‚
â”‚ â”‚   (ResNet + Spatial  â”‚          â”‚  TransformerForDiffusion â”‚
â”‚ â”‚    SoftMax)          â”‚          â”‚  â”œâ”€ Time Embedding      â”‚
â”‚ â”‚                      â”‚          â”‚  â”œâ”€ Condition Encoder   â”‚
â”‚ â””â”€â–º State Encoder      â”‚          â”‚  â”œâ”€ Decoder Layers      â”‚
â”‚     (MLP)              â”‚          â”‚  â””â”€ Output Head         â”‚
â”‚                        â”‚          â”‚                          â”‚
â”‚ â””â”€â–º å¤šæ¨¡æ€èåˆ          â”‚          â”‚                          â”‚
â”‚     (Cross-Attention)  â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ•°æ®æµæ¦‚è§ˆ

```
è§‚æµ‹æ•°æ®
  â”œâ”€â–º RGB Images [B, n_obs, n_cam, 3, H, W]
  â”œâ”€â–º Depth Images [B, n_obs, n_cam, 1, H, W]
  â””â”€â–º Robot State [B, n_obs, state_dim]
        â”‚
        â–¼
ç‰¹å¾ç¼–ç 
  â”œâ”€â–º RGB: ResNet â†’ Spatial SoftMax â†’ [B, n_obs, n_cam, feat_dim]
  â”œâ”€â–º Depth: ResNet â†’ Spatial SoftMax â†’ [B, n_obs, n_cam, feat_dim]
  â””â”€â–º State: MLP â†’ [B, n_obs, state_dim] æˆ– [B, n_obs, encoded_dim]
        â”‚
        â–¼
å¤šæ¨¡æ€èåˆ (Cross-Attention)
  â”œâ”€â–º RGB âŠ— Depth â†’ RGB_fused
  â”œâ”€â–º Depth âŠ— RGB â†’ Depth_fused
  â””â”€â–º Concatenate with State
        â”‚
        â–¼
å…¨å±€æ¡ä»¶ global_cond [B, n_obs, cond_dim]
        â”‚
        â–¼
æ‰©æ•£æ¨¡å‹ (è®­ç»ƒ)
  â”œâ”€â–º 1. å¯¹çœŸå®åŠ¨ä½œæ·»åŠ å™ªå£°: action + Îµ ~ N(0, I)
  â”œâ”€â–º 2. Transformeré¢„æµ‹å™ªå£°: Îµ_pred = Transformer(noisy_action, t, global_cond)
  â””â”€â–º 3. è®¡ç®—æŸå¤±: L = ||Îµ - Îµ_pred||Â²
        â”‚
        â–¼
æ‰©æ•£æ¨¡å‹ (æ¨ç†)
  â”œâ”€â–º 1. ä»çº¯å™ªå£°å¼€å§‹: action_T ~ N(0, I)
  â”œâ”€â–º 2. é€æ­¥å»å™ª (Tæ­¥ â†’ 0æ­¥)
  â”‚      for t in [T, T-1, ..., 1]:
  â”‚        Îµ_pred = Transformer(action_t, t, global_cond)
  â”‚        action_{t-1} = denoise(action_t, Îµ_pred, t)
  â””â”€â–º 3. è¾“å‡ºæœ€ç»ˆåŠ¨ä½œ: action_0
```

---

## 2. è®­ç»ƒä¸»æµç¨‹

### 2.1 å…¥å£å‡½æ•°: `train_policy.py::main()`

```python
@hydra.main(config_path="../configs/policy/",
            config_name="diffusion_config")
def main(cfg: DictConfig):
    """Diffusion Policyè®­ç»ƒä¸»å‡½æ•°"""
```

#### 2.1.1 è®­ç»ƒæµç¨‹å›¾

```
å¼€å§‹
  â”‚
  â”œâ”€â–º 1. åˆå§‹åŒ–
  â”‚      â”œâ”€ è®¾ç½®éšæœºç§å­
  â”‚      â”œâ”€ åˆ›å»ºè¾“å‡ºç›®å½•
  â”‚      â””â”€ åˆå§‹åŒ–TensorBoard
  â”‚
  â”œâ”€â–º 2. åŠ è½½æ•°æ®é›†
  â”‚      â”œâ”€ LeRobotDatasetMetadata (å…ƒæ•°æ®)
  â”‚      â”œâ”€ æ„å»º input_features & output_features
  â”‚      â””â”€ LeRobotDataset (å®é™…æ•°æ®)
  â”‚
  â”œâ”€â–º 3. æ„å»ºPolicy
  â”‚      â”œâ”€ build_policy_config()
  â”‚      â”œâ”€ CustomDiffusionPolicyWrapper(config, dataset_stats)
  â”‚      â”‚   â””â”€â–º å†…éƒ¨åˆ›å»º CustomDiffusionModelWrapper
  â”‚      â””â”€ policy.to(device)
  â”‚
  â”œâ”€â–º 4. æ„å»ºä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦å™¨
  â”‚      â”œâ”€ AdamW optimizer
  â”‚      â””â”€ Cosine/Linear scheduler
  â”‚
  â”œâ”€â–º 5. åˆå§‹åŒ–AMP (å¯é€‰)
  â”‚      â””â”€ torch.amp.GradScaler
  â”‚
  â”œâ”€â–º 6. æ¢å¤è®­ç»ƒ (å¯é€‰)
  â”‚      â”œâ”€ åŠ è½½ policy, optimizer, scheduler
  â”‚      â”œâ”€ åŠ è½½ AMP scaler
  â”‚      â””â”€ æ¢å¤ RNG state
  â”‚
  â”œâ”€â–º 7. ä¸»è®­ç»ƒå¾ªç¯
  â”‚      for epoch in range(max_epoch):
  â”‚        â”œâ”€ åˆ›å»º DataLoader
  â”‚        â”œâ”€ for batch in dataloader:
  â”‚        â”‚   â”œâ”€ å‰å‘ä¼ æ’­: loss, _ = policy.forward(batch)
  â”‚        â”‚   â”œâ”€ åå‘ä¼ æ’­: loss.backward()
  â”‚        â”‚   â”œâ”€ ä¼˜åŒ–å™¨æ­¥éª¤: optimizer.step()
  â”‚        â”‚   â””â”€ è®°å½•æ—¥å¿—
  â”‚        â”‚
  â”‚        â”œâ”€ ä¿å­˜æœ€ä½³æ¨¡å‹
  â”‚        â”œâ”€ å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
  â”‚        â””â”€ ä¿å­˜è®­ç»ƒçŠ¶æ€
  â”‚
  â””â”€â–º 8. è®­ç»ƒå®Œæˆ
```

### 2.2 å…³é”®è®­ç»ƒæ­¥éª¤è¯¦è§£

#### 2.2.1 æ•°æ®é›†åŠ è½½

```python
# 1. åŠ è½½å…ƒæ•°æ®
dataset_metadata = LeRobotDatasetMetadata(cfg.repoid, root=cfg.root)
# åŒ…å«: camera_keys, features, fps, total_frames, statsç­‰

# 2. è½¬æ¢ä¸ºPolicyç‰¹å¾
features = dataset_to_policy_features(dataset_metadata.features)
input_features = {k: ft for k, ft in features.items()
                 if ft.type is not FeatureType.ACTION}
output_features = {k: ft for k, ft in features.items()
                  if ft.type is FeatureType.ACTION}

# input_featuresç¤ºä¾‹:
# {
#   'observation.images.head_cam_h': PolicyFeature(shape=[3, 480, 640], ...),
#   'observation.images.wrist_cam_l': PolicyFeature(shape=[3, 480, 640], ...),
#   'observation.images.wrist_cam_r': PolicyFeature(shape=[3, 480, 640], ...),
#   'observation.depth.depth_h': PolicyFeature(shape=[1, 480, 640], ...),
#   'observation.depth.depth_l': PolicyFeature(shape=[1, 480, 640], ...),
#   'observation.depth.depth_r': PolicyFeature(shape=[1, 480, 640], ...),
#   'observation.state': PolicyFeature(shape=[16], ...)
# }

# output_featuresç¤ºä¾‹:
# {
#   'action': PolicyFeature(shape=[16], ...)
# }

# 3. æ„å»ºdelta timestamps (ç”¨äºå¤šå¸§è§‚æµ‹)
delta_timestamps = build_delta_timestamps(dataset_metadata, policy_cfg)
# ä¾‹å¦‚: {'observation.state': [0, -0.02], 'action': [0, 0.02, 0.04, ...]}

# 4. åˆ›å»ºæ•°æ®é›†
dataset = LeRobotDataset(
    cfg.repoid,
    delta_timestamps=delta_timestamps,
    root=cfg.root,
    episodes=episodes_to_use,  # å¯é™åˆ¶ä½¿ç”¨çš„episodes
    image_transforms=image_transforms,  # RGBå¢å¼º
)
```

#### 2.2.2 Policyæ„å»º

```python
def build_policy_config(cfg, input_features, output_features):
    """æ„å»ºpolicyé…ç½®"""
    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=cfg.training.device,
    )
    return policy_cfg

def build_policy(name, policy_cfg, dataset_stats):
    """æ„å»ºpolicyå®ä¾‹"""
    policy = {
        "diffusion": CustomDiffusionPolicyWrapper,
        "act": ACTPolicy,
    }[name](policy_cfg, dataset_stats)
    return policy

# ä½¿ç”¨
policy = build_policy("diffusion", policy_cfg, dataset_metadata.stats)
```

**Policyåˆå§‹åŒ–æµç¨‹**:
```
CustomDiffusionPolicyWrapper.__init__()
  â”‚
  â”œâ”€â–º è°ƒç”¨çˆ¶ç±» DiffusionPolicy.__init__()
  â”‚     â”œâ”€ ä¿å­˜ config
  â”‚     â”œâ”€ åˆå§‹åŒ–å½’ä¸€åŒ–å‚æ•° (ä» dataset_stats)
  â”‚     â””â”€ åˆå§‹åŒ–è§‚æµ‹é˜Ÿåˆ— (deque)
  â”‚
  â””â”€â–º åˆ›å»º CustomDiffusionModelWrapper
        â”œâ”€â–º åˆ›å»ºç‰¹å¾ç¼–ç å™¨
        â”‚   â”œâ”€ RGB Encoder (ResNet + SpatialSoftMax)
        â”‚   â”œâ”€ Depth Encoder (ResNet + SpatialSoftMax)
        â”‚   â””â”€ State Encoder (MLP, å¯é€‰)
        â”‚
        â”œâ”€â–º åˆ›å»ºå»å™ªç½‘ç»œ
        â”‚   â”œâ”€ U-Net (if config.use_unet)
        â”‚   â””â”€ TransformerForDiffusion (if config.use_transformer)
        â”‚
        â””â”€â–º åˆ›å»ºå™ªå£°è°ƒåº¦å™¨
            â””â”€ DDPM / DDIM Scheduler
```

#### 2.2.3 è®­ç»ƒå¾ªç¯

```python
for epoch in range(start_epoch, cfg.training.max_epoch):
    # åˆ›å»ºDataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=cfg.training.batch_size,
        shuffle=True,
        num_workers=cfg.training.num_workers,
        pin_memory=True,
        drop_last=cfg.training.drop_last,
    )

    for batch in dataloader:
        # 1. æ•°æ®ç§»åˆ°GPU
        batch = {k: v.to(device) for k, v in batch.items()}
        # batchåŒ…å«:
        # - observation.images.*: [64, 2, 3, 480, 640]
        # - observation.depth.*: [64, 2, 1, 480, 640]
        # - observation.state: [64, 2, 16]
        # - action: [64, 16, 16]  # [batch, horizon, action_dim]

        # 2. å‰å‘ä¼ æ’­ (AMPå¯é€‰)
        with autocast(amp_enabled):
            loss, _ = policy.forward(batch)

        # policy.forward() å†…éƒ¨æµç¨‹:
        #   a. å›¾åƒé¢„å¤„ç† (crop, resize)
        #   b. å½’ä¸€åŒ– (observations & actions)
        #   c. diffusion.compute_loss(batch)
        #      â”œâ”€ æå–ç‰¹å¾: global_cond = _prepare_global_conditioning(batch)
        #      â”œâ”€ æ·»åŠ å™ªå£°: noisy_actions = actions + noise
        #      â”œâ”€ é¢„æµ‹å™ªå£°: noise_pred = unet(noisy_actions, timesteps, global_cond)
        #      â””â”€ è®¡ç®—æŸå¤±: loss = ||noise - noise_pred||Â²

        # 3. åå‘ä¼ æ’­
        scaled_loss = loss / accumulation_steps
        if amp_enabled:
            scaler.scale(scaled_loss).backward()
        else:
            scaled_loss.backward()

        # 4. ä¼˜åŒ–å™¨æ­¥éª¤
        if steps % accumulation_steps == 0:
            if amp_enabled:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()
            optimizer.zero_grad()
            lr_scheduler.step()

        # 5. è®°å½•æ—¥å¿—
        if steps % log_freq == 0:
            writer.add_scalar("train/loss", scaled_loss.item(), steps)
            writer.add_scalar("train/lr", lr.get_last_lr()[0], steps)

        steps += 1

    # 6. Epochç»“æŸåçš„æ“ä½œ
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if total_loss < best_loss:
        best_loss = total_loss
        policy.save_pretrained(output_directory / "best")

    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
    if (epoch + 1) % save_freq_epoch == 0:
        policy.save_pretrained(output_directory / f"epoch{epoch+1}")

    # ä¿å­˜è®­ç»ƒçŠ¶æ€ (ç”¨äºæ¢å¤è®­ç»ƒ)
    checkpoint = {
        "optimizer": optimizer.state_dict(),
        "lr_scheduler": lr_scheduler.state_dict(),
        "scaler": scaler.state_dict() if amp_enabled else None,
        "steps": steps,
        "epoch": epoch + 1,
        "best_loss": best_loss
    }
    torch.save(checkpoint, output_directory / "learning_state.pth")
```

---

## 3. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 3.1 CustomDiffusionModelWrapper

**ä½ç½®**: `kuavo_train/wrapper/policy/diffusion/DiffusionModelWrapper.py`

#### 3.1.1 æ•´ä½“æ¶æ„

```python
class CustomDiffusionModelWrapper(DiffusionModel):
    """
    æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒåŒ…è£…å™¨

    ä¸»è¦èŒè´£:
    1. ç‰¹å¾ç¼–ç  (RGB, Depth, State)
    2. å¤šæ¨¡æ€èåˆ
    3. å»å™ªç½‘ç»œ (U-Net æˆ– Transformer)
    4. æŸå¤±è®¡ç®—
    """

    def __init__(self, config):
        super().__init__(config)

        # 1. è®¡ç®—å…¨å±€æ¡ä»¶ç»´åº¦
        global_cond_dim = 0

        # 2. æ„å»ºç¼–ç å™¨
        if config.robot_state_feature:
            if config.use_state_encoder:
                self.state_encoder = FeatureEncoder(in_dim, out_dim)
                global_cond_dim += out_dim
            else:
                global_cond_dim += state_dim

        if config.image_features:
            self.rgb_encoder = DiffusionRgbEncoder(config)
            global_cond_dim += rgb_encoder.feature_dim * num_cameras
            self.rgb_attn_layer = nn.MultiheadAttention(...)

        if config.depth_features:
            self.depth_encoder = DiffusionDepthEncoder(config)
            global_cond_dim += depth_encoder.feature_dim * num_cameras
            self.depth_attn_layer = nn.MultiheadAttention(...)

        # 3. å¤šæ¨¡æ€èåˆ
        if config.use_depth and config.depth_features:
            self.multimodalfuse = nn.ModuleDict({
                "depth_q": nn.MultiheadAttention(...),  # Depth query RGB
                "rgb_q": nn.MultiheadAttention(...),    # RGB query Depth
            })

        # 4. å»å™ªç½‘ç»œ
        if config.use_unet:
            self.unet = DiffusionConditionalUnet1d(config, global_cond_dim)
        elif config.use_transformer:
            self.unet = TransformerForDiffusion(
                input_dim=action_dim,
                output_dim=action_dim,
                horizon=config.horizon,
                n_obs_steps=config.n_obs_steps,
                cond_dim=global_cond_dim,
                n_layer=config.transformer_n_layer,
                n_head=config.transformer_n_head,
                n_emb=config.transformer_n_emb,
                ...
            )

        # 5. å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = _make_noise_scheduler(
            config.noise_scheduler_type,  # DDPM / DDIM
            num_train_timesteps=config.num_train_timesteps,
            ...
        )
```

#### 3.1.2 ç‰¹å¾ç¼–ç 

##### RGBç¼–ç å™¨

```python
class DiffusionRgbEncoder(nn.Module):
    """RGBå›¾åƒç¼–ç å™¨"""

    def __init__(self, config):
        super().__init__()

        # 1. ä½¿ç”¨é¢„è®­ç»ƒResNetä½œä¸ºbackbone
        backbone_model = torchvision.models.resnet18(
            weights=config.pretrained_backbone_weights
        )
        # å»æ‰æœ€åä¸¤å±‚ (å…¨å±€æ± åŒ–å’Œåˆ†ç±»å±‚)
        self.backbone = nn.Sequential(*(list(backbone_model.children())[:-2]))

        # 2. Spatial SoftMax æ± åŒ–
        # é€šè¿‡dry runè·å–ç‰¹å¾å›¾å°ºå¯¸
        feature_map_shape = get_output_shape(self.backbone, dummy_input)[1:]
        self.pool = SpatialSoftmax(
            feature_map_shape,
            num_kp=config.spatial_softmax_num_keypoints
        )

        # 3. è¾“å‡ºå±‚
        self.feature_dim = config.spatial_softmax_num_keypoints * 2
        self.out = nn.Linear(num_keypoints * 2, self.feature_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        """
        Args:
            x: [B, 3, H, W] RGBå›¾åƒ
        Returns:
            [B, feature_dim] ç‰¹å¾å‘é‡
        """
        # ResNetç‰¹å¾æå–
        x = self.backbone(x)  # [B, 512, H', W']

        # Spatial SoftMaxæ± åŒ–
        x = self.pool(x)  # [B, num_keypoints * 2]

        # è¾“å‡ºæŠ•å½±
        x = self.relu(self.out(x))  # [B, feature_dim]

        return x
```

**Spatial SoftMax åŸç†**:
```
è¾“å…¥: feature_map [B, C, H, W]

å¯¹æ¯ä¸ªé€šé“ c:
  1. è®¡ç®—softmaxæƒé‡:
     weights[b, h, w] = exp(feature_map[b, c, h, w]) / Î£ exp(feature_map[b, c, :, :])

  2. è®¡ç®—åŠ æƒåæ ‡ (keypoint):
     x_c = Î£(h,w) weights[b, h, w] * x_coord[h, w]
     y_c = Î£(h,w) weights[b, h, w] * y_coord[h, w]

è¾“å‡º: [B, C * 2]  # æ¯ä¸ªé€šé“ä¸€ä¸ª(x, y)åæ ‡
```

**ä¼˜åŠ¿**:
- ä¿ç•™ç©ºé—´ä¿¡æ¯ï¼ˆç›¸æ¯”å…¨å±€å¹³å‡æ± åŒ–ï¼‰
- å¯å¾®åˆ†ï¼ˆç›¸æ¯”argmaxï¼‰
- é™ç»´ï¼ˆä»HÃ—Wåˆ°2ä¸ªåæ ‡ï¼‰

##### æ·±åº¦ç¼–ç å™¨

```python
class DiffusionDepthEncoder(nn.Module):
    """æ·±åº¦å›¾ç¼–ç å™¨"""

    def __init__(self, config):
        super().__init__()

        # 1. ä½¿ç”¨ResNetï¼Œä½†ä¿®æ”¹ç¬¬ä¸€å±‚æ¥å—1é€šé“è¾“å…¥
        backbone_model = torchvision.models.resnet18(...)
        modules = list(backbone_model.children())[:-2]

        # ä¿®æ”¹ç¬¬ä¸€ä¸ªå·ç§¯å±‚
        if isinstance(modules[0], nn.Conv2d):
            old_conv = modules[0]
            modules[0] = nn.Conv2d(
                in_channels=1,  # æ·±åº¦å›¾åªæœ‰1ä¸ªé€šé“
                out_channels=old_conv.out_channels,
                kernel_size=old_conv.kernel_size,
                stride=old_conv.stride,
                padding=old_conv.padding,
            )
            # æƒé‡åˆå§‹åŒ–: å¯¹RGBæƒé‡å–å¹³å‡
            with torch.no_grad():
                modules[0].weight = nn.Parameter(
                    old_conv.weight.mean(dim=1, keepdim=True)
                )

        self.backbone = nn.Sequential(*modules)

        # 2. Spatial SoftMax (åŒRGB)
        self.pool = SpatialSoftmax(...)
        self.out = nn.Linear(...)
        self.relu = nn.ReLU()

    def forward(self, x):
        """
        Args:
            x: [B, 1, H, W] æ·±åº¦å›¾
        Returns:
            [B, feature_dim] ç‰¹å¾å‘é‡
        """
        x = self.backbone(x)
        x = torch.flatten(self.pool(x), start_dim=1)
        x = self.relu(self.out(x))
        return x
```

##### çŠ¶æ€ç¼–ç å™¨

```python
class FeatureEncoder(nn.Module):
    """çŠ¶æ€ç‰¹å¾ç¼–ç å™¨"""

    def __init__(self, in_dim: int, out_dim: int = 128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(in_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(inplace=False)
        )

    def forward(self, x):
        """
        Args:
            x: [B, in_dim] æˆ– [B, T, in_dim]
        Returns:
            ç¼–ç åçš„ç‰¹å¾
        """
        if x.dim() == 2:
            return self.encoder(x)  # [B, out_dim]
        elif x.dim() == 3:
            B, T, D = x.shape
            x = x.view(B * T, D)
            x = self.encoder(x)
            x = x.view(B, T, -1)
            return x  # [B, T, out_dim]
```

#### 3.1.3 å…¨å±€æ¡ä»¶å‡†å¤‡

```python
def _prepare_global_conditioning(self, batch):
    """
    ç¼–ç å¹¶èåˆæ‰€æœ‰è§‚æµ‹ç‰¹å¾

    Args:
        batch: è¾“å…¥æ‰¹æ¬¡
            - observation.images: [B, n_obs, n_cam, 3, H, W]
            - observation.depth: [B, n_obs, n_cam, 1, H, W]
            - observation.state: [B, n_obs, state_dim]

    Returns:
        global_cond: [B, n_obs, cond_dim]
    """
    batch_size, n_obs_steps, n_camera = batch[OBS_STATE].shape[:3]
    global_cond_feats = []

    # 1. RGBç‰¹å¾æå–
    if self.config.image_features:
        if self.config.use_separate_rgb_encoder_per_camera:
            # æ¯ä¸ªç›¸æœºä½¿ç”¨ç‹¬ç«‹ç¼–ç å™¨
            images_per_camera = einops.rearrange(
                batch[OBS_IMAGES], "b s n ... -> n (b s) ..."
            )
            img_features_list = torch.cat([
                encoder(images)
                for encoder, images in zip(self.rgb_encoder, images_per_camera)
            ])
            img_features = einops.rearrange(
                img_features_list, "(n b s) ... -> b s n ...",
                b=batch_size, s=n_obs_steps
            )
        else:
            # æ‰€æœ‰ç›¸æœºå…±äº«ç¼–ç å™¨
            img_features = self.rgb_encoder(
                einops.rearrange(batch[OBS_IMAGES], "b s n ... -> (b s n) ...")
            )
            img_features = einops.rearrange(
                img_features, "(b s n) ... -> b s n ...",
                b=batch_size, s=n_obs_steps
            )

        # Self-Attentionèšåˆå¤šç›¸æœºç‰¹å¾
        img_features = einops.rearrange(
            img_features, "b s n ... -> (b s) n ...",
            b=batch_size, s=n_obs_steps
        )
        img_features = self.rgb_attn_layer(
            query=img_features,
            key=img_features,
            value=img_features
        )[0]  # [B*n_obs, n_cam, feat_dim]

    # 2. æ·±åº¦ç‰¹å¾æå– (åŒRGB)
    if self.config.use_depth and self.config.depth_features:
        # ... ç±»ä¼¼RGBçš„å¤„ç† ...
        depth_features = self.depth_attn_layer(
            query=depth_features,
            key=depth_features,
            value=depth_features
        )[0]

    # 3. å¤šæ¨¡æ€èåˆ (Cross-Attention)
    if (img_features is not None) and (depth_features is not None):
        # RGB query Depth
        rgb_q_fuse = self.multimodalfuse["rgb_q"](
            query=img_features,      # [B*n_obs, n_cam, feat_dim]
            key=depth_features,      # [B*n_obs, n_cam, feat_dim]
            value=depth_features
        )[0]

        # Depth query RGB
        depth_q_fuse = self.multimodalfuse["depth_q"](
            query=depth_features,
            key=img_features,
            value=img_features
        )[0]

        # é‡æ’å¹¶æ‹¼æ¥
        rgb_q_fuse = einops.rearrange(
            rgb_q_fuse, "(b s) n ... -> b s (n ...)",
            b=batch_size, s=n_obs_steps
        )
        depth_q_fuse = einops.rearrange(
            depth_q_fuse, "(b s) n ... -> b s (n ...)",
            b=batch_size, s=n_obs_steps
        )
        global_cond_feats.extend([rgb_q_fuse, depth_q_fuse])

    # 4. çŠ¶æ€ç‰¹å¾
    if self.config.robot_state_feature:
        if self.config.use_state_encoder:
            state_features = self.state_encoder(batch[OBS_STATE])
        else:
            state_features = batch[OBS_STATE]
        global_cond_feats.append(state_features)

    # 5. æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    if self.config.use_transformer:
        # Transformeréœ€è¦ä¿ç•™æ—¶é—´ç»´åº¦
        return torch.cat(global_cond_feats, dim=-1)  # [B, n_obs, cond_dim]
    else:
        # U-Netéœ€è¦å±•å¹³
        return torch.cat(global_cond_feats, dim=-1).flatten(start_dim=1)  # [B, n_obs*cond_dim]
```

### 3.2 CustomDiffusionPolicyWrapper

**ä½ç½®**: `kuavo_train/wrapper/policy/diffusion/DiffusionPolicyWrapper.py`

#### 3.2.1 æ ¸å¿ƒæ–¹æ³•

##### forward() - è®­ç»ƒæ—¶ä½¿ç”¨

```python
def forward(self, batch):
    """
    è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­

    Args:
        batch: æ•°æ®æ‰¹æ¬¡
            - observation.images.*: [B, n_obs, 3, H, W]
            - observation.depth.*: [B, n_obs, 1, H, W]
            - observation.state: [B, n_obs, state_dim]
            - action: [B, horizon, action_dim]

    Returns:
        (loss, None)
    """
    # 1. å›¾åƒé¢„å¤„ç† (è£å‰ªã€ç¼©æ”¾)
    random_crop = self.config.crop_is_random and self.training
    crop_position = None

    if self.config.image_features:
        batch = dict(batch)  # shallow copy
        for key in self.config.image_features:
            # è£å‰ª
            batch[key], crop_position = crop_image(
                batch[key],
                target_range=self.config.crop_shape,
                random_crop=random_crop
            )
            # ç¼©æ”¾
            batch[key] = resize_image(
                batch[key],
                target_size=self.config.resize_shape,
                image_type="rgb"
            )

    # æ·±åº¦å›¾åŒæ ·å¤„ç†
    if self.config.use_depth and self.config.depth_features:
        for key in self.config.depth_features:
            if len(crop_position) == 4:
                batch[key] = torchvision.transforms.functional.crop(
                    batch[key], *crop_position
                )
            else:
                batch[key] = torchvision.transforms.functional.center_crop(
                    batch[key], crop_position
                )
            batch[key] = resize_image(
                batch[key],
                target_size=self.config.resize_shape,
                image_type="depth"
            )

    # 2. å½’ä¸€åŒ–
    batch = self.normalize_inputs(batch)   # å½’ä¸€åŒ–è§‚æµ‹

    # 3. å †å å›¾åƒ (åœ¨å½’ä¸€åŒ–ä¹‹å)
    if self.config.image_features:
        batch[OBS_IMAGES] = torch.stack(
            [batch[key] for key in self.config.image_features],
            dim=-4
        )  # [B, n_obs, n_cam, 3, H, W]

    if self.config.use_depth and self.config.depth_features:
        batch[OBS_DEPTH] = torch.stack(
            [batch[key] for key in self.config.depth_features],
            dim=-4
        )  # [B, n_obs, n_cam, 1, H, W]

    batch = self.normalize_targets(batch)  # å½’ä¸€åŒ–åŠ¨ä½œ

    # 4. è®¡ç®—æ‰©æ•£æŸå¤±
    loss = self.diffusion.compute_loss(batch)

    return loss, None
```

##### select_action() - æ¨ç†æ—¶ä½¿ç”¨

```python
def select_action(self, batch):
    """
    æ¨ç†æ—¶é€‰æ‹©åŠ¨ä½œ

    ä½¿ç”¨observation queueç¼“å­˜å†å²è§‚æµ‹ï¼Œ
    ä½¿ç”¨action queueç¼“å­˜ç”Ÿæˆçš„åŠ¨ä½œåºåˆ—ã€‚

    Args:
        batch: å½“å‰è§‚æµ‹
            - observation.images.*: [1, 3, H, W]
            - observation.depth.*: [1, 1, H, W]
            - observation.state: [1, state_dim]

    Returns:
        action: [1, action_dim]
    """
    # ç§»é™¤action (æ¨ç†æ—¶ä¸éœ€è¦)
    if ACTION in batch:
        batch.pop(ACTION)

    # å½’ä¸€åŒ–è¾“å…¥
    batch = self.normalize_inputs(batch)

    # å›¾åƒé¢„å¤„ç† (ä¸è®­ç»ƒæ—¶ç±»ä¼¼ï¼Œä½†ä½¿ç”¨center crop)
    random_crop = self.config.crop_is_random and self.training
    if self.config.image_features:
        # ... è£å‰ªå’Œç¼©æ”¾ ...
        batch[OBS_IMAGES] = torch.stack(...)

    if self.config.use_depth:
        # ... æ·±åº¦å›¾å¤„ç† ...
        batch[OBS_DEPTH] = torch.stack(...)

    # å¡«å……observation queue
    self._queues = populate_queues(self._queues, batch)
    # ä¾‹å¦‚: _queues['observation.state'] åŒ…å«æœ€è¿‘n_obs_stepsä¸ªè§‚æµ‹

    # å¦‚æœaction queueä¸ºç©ºï¼Œç”Ÿæˆæ–°çš„åŠ¨ä½œåºåˆ—
    if len(self._queues[ACTION]) == 0:
        # é¢„æµ‹åŠ¨ä½œchunk
        actions = self.predict_action_chunk(batch)
        # actions: [1, horizon, action_dim]

        # è½¬ç½®å¹¶å¡«å……åˆ°action queue
        self._queues[ACTION].extend(actions.transpose(0, 1))
        # action queueç°åœ¨åŒ…å«horizonä¸ªåŠ¨ä½œ

    # ä»action queueä¸­å–å‡ºç¬¬ä¸€ä¸ªåŠ¨ä½œ
    action = self._queues[ACTION].popleft()

    return action
```

---

## 4. Transformeræ¶æ„

### 4.1 TransformerForDiffusion

**ä½ç½®**: `kuavo_train/wrapper/policy/diffusion/transformer_diffusion.py`

#### 4.1.1 æ•´ä½“æ¶æ„

```python
class TransformerForDiffusion(ModuleAttrMixin):
    """
    ä¸“é—¨ç”¨äºæ‰©æ•£æ¨¡å‹çš„Transformer

    æ¶æ„:
    - Encoder: ç¼–ç æ¡ä»¶ä¿¡æ¯ (æ—¶é—´æ­¥ + è§‚æµ‹)
    - Decoder: è§£ç åŠ¨ä½œåºåˆ— (åŸºäºæ¡ä»¶ç”Ÿæˆå»å™ªåŠ¨ä½œ)
    """

    def __init__(self,
                 input_dim: int,          # åŠ¨ä½œç»´åº¦
                 output_dim: int,         # è¾“å‡ºç»´åº¦ (é€šå¸¸ç­‰äºinput_dim)
                 horizon: int,            # åŠ¨ä½œåºåˆ—é•¿åº¦
                 n_obs_steps: int,        # è§‚æµ‹æ­¥æ•°
                 cond_dim: int,           # æ¡ä»¶ç»´åº¦
                 n_layer: int = 12,       # Decoderå±‚æ•°
                 n_head: int = 12,        # æ³¨æ„åŠ›å¤´æ•°
                 n_emb: int = 768,        # åµŒå…¥ç»´åº¦
                 p_drop_emb: float = 0.1,
                 p_drop_attn: float = 0.1,
                 causal_attn: bool = False,    # æ˜¯å¦ä½¿ç”¨å› æœæ³¨æ„åŠ›
                 time_as_cond: bool = True,     # æ—¶é—´æ­¥ä½œä¸ºæ¡ä»¶
                 obs_as_cond: bool = False,     # è§‚æµ‹ä½œä¸ºæ¡ä»¶
                 n_cond_layers: int = 0         # Encoderå±‚æ•°
                 ):
        super().__init__()

        # è®¡ç®—tokenæ•°é‡
        T = horizon  # åŠ¨ä½œåºåˆ—é•¿åº¦
        T_cond = 1   # æ¡ä»¶tokenæ•°é‡ (æ—¶é—´æ­¥)

        if not time_as_cond:
            T += 1
            T_cond -= 1

        obs_as_cond = cond_dim > 0
        if obs_as_cond:
            assert time_as_cond
            T_cond += n_obs_steps  # æ·»åŠ è§‚æµ‹token

        # 1. è¾“å…¥åµŒå…¥
        self.input_emb = nn.Linear(input_dim, n_emb)
        self.pos_emb = nn.Parameter(torch.zeros(1, T, n_emb))
        self.drop = nn.Dropout(p_drop_emb)

        # 2. æ¡ä»¶ç¼–ç å™¨
        self.time_emb = SinusoidalPosEmb(n_emb)
        self.cond_obs_emb = None
        if obs_as_cond:
            self.cond_obs_emb = nn.Linear(cond_dim, n_emb)

        self.cond_pos_emb = None
        self.encoder = None
        self.decoder = None

        if T_cond > 0:
            self.cond_pos_emb = nn.Parameter(torch.zeros(1, T_cond, n_emb))

            if n_cond_layers > 0:
                # ä½¿ç”¨Transformer Encoder
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=n_emb,
                    nhead=n_head,
                    dim_feedforward=4*n_emb,
                    dropout=p_drop_attn,
                    activation='gelu',
                    batch_first=True,
                    norm_first=True
                )
                self.encoder = nn.TransformerEncoder(
                    encoder_layer=encoder_layer,
                    num_layers=n_cond_layers
                )
            else:
                # ä½¿ç”¨ç®€å•MLP
                self.encoder = nn.Sequential(
                    nn.Linear(n_emb, 4 * n_emb),
                    nn.Mish(),
                    nn.Linear(4 * n_emb, n_emb)
                )

            # Decoder (æ ¸å¿ƒå»å™ªç½‘ç»œ)
            decoder_layer = nn.TransformerDecoderLayer(
                d_model=n_emb,
                nhead=n_head,
                dim_feedforward=4*n_emb,
                dropout=p_drop_attn,
                activation='gelu',
                batch_first=True,
                norm_first=True  # é‡è¦: æå‡è®­ç»ƒç¨³å®šæ€§
            )
            self.decoder = nn.TransformerDecoder(
                decoder_layer=decoder_layer,
                num_layers=n_layer
            )
        else:
            # Encoder-onlyæ¶æ„ (BERTé£æ ¼)
            encoder_layer = nn.TransformerEncoderLayer(...)
            self.encoder = nn.TransformerEncoder(...)

        # 3. æ³¨æ„åŠ›æ©ç 
        if causal_attn:
            # å› æœæ©ç  (è‡ªå›å½’)
            sz = T
            mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
            self.register_buffer("mask", mask)

            if time_as_cond and obs_as_cond:
                # Memoryæ©ç  (Decoderå¯ä»¥çœ‹åˆ°çš„Encoder token)
                S = T_cond
                t, s = torch.meshgrid(
                    torch.arange(T),
                    torch.arange(S),
                    indexing='ij'
                )
                mask = t >= (s-1)
                mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
                self.register_buffer('memory_mask', mask)

        # 4. è¾“å‡ºå¤´
        self.ln_f = nn.LayerNorm(n_emb)
        self.head = nn.Linear(n_emb, output_dim)
```

#### 4.1.2 å‰å‘ä¼ æ’­

```python
def forward(self, sample, timestep, global_cond=None, **kwargs):
    """
    å‰å‘ä¼ æ’­

    Args:
        sample: å™ªå£°åŠ¨ä½œ [B, T, input_dim]
        timestep: æ‰©æ•£æ—¶é—´æ­¥ [B] æˆ– æ ‡é‡
        global_cond: å…¨å±€æ¡ä»¶ [B, To, cond_dim]

    Returns:
        noise_pred: é¢„æµ‹çš„å™ªå£° [B, T, output_dim]
    """
    cond = global_cond

    # 1. æ—¶é—´æ­¥åµŒå…¥
    timesteps = timestep
    if not torch.is_tensor(timesteps):
        timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)
    elif len(timesteps.shape) == 0:
        timesteps = timesteps[None].to(sample.device)
    timesteps = timesteps.expand(sample.shape[0])

    time_emb = self.time_emb(timesteps).unsqueeze(1)
    # [B, 1, n_emb]

    # 2. è¾“å…¥åµŒå…¥
    input_emb = self.input_emb(sample)
    # [B, T, n_emb]

    if self.encoder_only:
        # ===== BERTé£æ ¼: Encoder-only =====
        token_embeddings = torch.cat([time_emb, input_emb], dim=1)
        # [B, T+1, n_emb]

        t = token_embeddings.shape[1]
        position_embeddings = self.pos_emb[:, :t, :]
        x = self.drop(token_embeddings + position_embeddings)

        x = self.encoder(src=x, mask=self.mask)
        # [B, T+1, n_emb]

        x = x[:, 1:, :]  # ç§»é™¤æ—¶é—´æ­¥token
        # [B, T, n_emb]
    else:
        # ===== Encoder-Decoderæ¶æ„ =====

        # 3. æ¡ä»¶ç¼–ç 
        cond_embeddings = time_emb
        if self.obs_as_cond:
            cond_obs_emb = self.cond_obs_emb(cond)
            # [B, To, n_emb]
            cond_embeddings = torch.cat([cond_embeddings, cond_obs_emb], dim=1)
            # [B, 1+To, n_emb]

        tc = cond_embeddings.shape[1]
        position_embeddings = self.cond_pos_emb[:, :tc, :]
        x = self.drop(cond_embeddings + position_embeddings)
        x = self.encoder(x)
        memory = x
        # [B, T_cond, n_emb]

        # 4. åŠ¨ä½œè§£ç 
        token_embeddings = input_emb
        t = token_embeddings.shape[1]
        position_embeddings = self.pos_emb[:, :t, :]
        x = self.drop(token_embeddings + position_embeddings)
        # [B, T, n_emb]

        x = self.decoder(
            tgt=x,                    # Query: å™ªå£°åŠ¨ä½œ
            memory=memory,            # Key & Value: æ¡ä»¶ä¿¡æ¯
            tgt_mask=self.mask,       # å› æœæ©ç  (å¯é€‰)
            memory_mask=self.memory_mask  # Memoryæ©ç  (å¯é€‰)
        )
        # [B, T, n_emb]

    # 5. è¾“å‡ºå¤´
    x = self.ln_f(x)
    x = self.head(x)
    # [B, T, output_dim]

    return x
```

#### 4.1.3 æ©ç æœºåˆ¶

##### å› æœæ©ç  (Causal Mask)

```python
# è‡ªå›å½’ç”Ÿæˆï¼šæ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ä½ç½®
sz = T  # horizon
mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
# [
#   [0, -inf, -inf, -inf],
#   [0,    0, -inf, -inf],
#   [0,    0,    0, -inf],
#   [0,    0,    0,    0]
# ]

# åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ä½¿ç”¨
scores = Q @ K.T / sqrt(d_k)
scores = scores.masked_fill(mask == float('-inf'), float('-inf'))
attn_weights = softmax(scores)
```

**ä¸ºä»€ä¹ˆéœ€è¦å› æœæ©ç ?**
- é˜²æ­¢"æœªæ¥ä¿¡æ¯æ³„éœ²"
- é€‚ç”¨äºè‡ªå›å½’ç”Ÿæˆåœºæ™¯
- å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œå› æœæ€§ä¸æ˜¯å¿…éœ€çš„ï¼ˆå¯ä»¥å…¨å±€çœ‹ï¼‰

##### Memoryæ©ç 

```python
# Decoderå¯ä»¥çœ‹åˆ°å“ªäº›Encoder token
T = horizon  # åŠ¨ä½œåºåˆ—é•¿åº¦
S = n_obs_steps + 1  # æ¡ä»¶tokenæ•°é‡ (æ—¶é—´æ­¥ + è§‚æµ‹)

t, s = torch.meshgrid(
    torch.arange(T),
    torch.arange(S),
    indexing='ij'
)

mask = t >= (s-1)  # æ¯ä¸ªåŠ¨ä½œä½ç½®å¯ä»¥çœ‹åˆ°ç›¸åº”æ—¶é—´ä¹‹å‰çš„è§‚æµ‹
# ä¾‹å¦‚: T=4, S=3
# [
#   [0, -inf, -inf],  # action[0] åªèƒ½çœ‹åˆ° time_emb
#   [0,    0, -inf],  # action[1] å¯ä»¥çœ‹åˆ° time_emb + obs[0]
#   [0,    0,    0],  # action[2] å¯ä»¥çœ‹åˆ° time_emb + obs[0] + obs[1]
#   [0,    0,    0],  # action[3] å¯ä»¥çœ‹åˆ°æ‰€æœ‰
# ]
```

### 4.2 æ—¶é—´æ­¥åµŒå…¥

```python
class SinusoidalPosEmb(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç  (ç”¨äºæ—¶é—´æ­¥)"""

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        """
        Args:
            x: æ—¶é—´æ­¥ [B]
        Returns:
            åµŒå…¥ [B, dim]
        """
        device = x.device
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb
```

**æ•°å­¦åŸç†**:
```
ç»™å®šæ—¶é—´æ­¥ tï¼Œç”ŸæˆåµŒå…¥:

emb[i] = sin(t / 10000^(2i/d))     for i < d/2
emb[i] = cos(t / 10000^(2(i-d/2)/d))  for i >= d/2

å…¶ä¸­ d = dim
```

**ä¼˜åŠ¿**:
- è¿ç»­æ€§ï¼šç›¸é‚»æ—¶é—´æ­¥çš„åµŒå…¥ç›¸ä¼¼
- å¤–æ¨æ€§ï¼šå¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„æ—¶é—´æ­¥
- å¯å¾®åˆ†ï¼šæ”¯æŒæ¢¯åº¦ä¼ æ’­

---

## 5. å¤šæ¨¡æ€èåˆ

### 5.1 è®¾è®¡æ€è·¯

**é—®é¢˜**: å¦‚ä½•æœ‰æ•ˆèåˆRGBå›¾åƒã€æ·±åº¦å›¾å’Œæœºå™¨äººçŠ¶æ€ï¼Ÿ

**æ–¹æ¡ˆ**: åˆ†å±‚èåˆ
1. **å±‚å†…èåˆ**: ä½¿ç”¨Self-AttentionèšåˆåŒç±»å‹å¤šç›¸æœºçš„ç‰¹å¾
2. **å±‚é—´èåˆ**: ä½¿ç”¨Cross-AttentionèåˆRGBå’ŒDepth
3. **å…¨å±€æ‹¼æ¥**: å°†èåˆåçš„è§†è§‰ç‰¹å¾ä¸çŠ¶æ€ç‰¹å¾æ‹¼æ¥

### 5.2 å®ç°ç»†èŠ‚

#### 5.2.1 Self-Attentionèšåˆå¤šç›¸æœº

```python
# RGBç‰¹å¾: [B*n_obs, n_cam, feat_dim]
img_features = self.rgb_attn_layer(
    query=img_features,
    key=img_features,
    value=img_features
)[0]
# è¾“å‡º: [B*n_obs, n_cam, feat_dim]

# ä½œç”¨: å­¦ä¹ ç›¸æœºé—´çš„å…³ç³»ï¼Œçªå‡ºé‡è¦ç›¸æœº
```

#### 5.2.2 Cross-AttentionèåˆRGBå’ŒDepth

```python
# RGB query Depth
rgb_q_fuse = self.multimodalfuse["rgb_q"](
    query=img_features,      # [B*n_obs, n_cam, feat_dim]
    key=depth_features,      # [B*n_obs, n_cam, feat_dim]
    value=depth_features
)[0]

# Depth query RGB
depth_q_fuse = self.multimodalfuse["depth_q"](
    query=depth_features,
    key=img_features,
    value=img_features
)[0]

# ä½œç”¨:
# - RGB query Depth: ä¸ºRGBç‰¹å¾è¡¥å……æ·±åº¦ä¿¡æ¯
# - Depth query RGB: ä¸ºæ·±åº¦ç‰¹å¾è¡¥å……é¢œè‰²/çº¹ç†ä¿¡æ¯
```

**ä¸ºä»€ä¹ˆåŒå‘?**
- RGBå’ŒDepthäº’è¡¥ï¼šRGBæä¾›çº¹ç†ï¼ŒDepthæä¾›å‡ ä½•
- åŒå‘èåˆæ¯”å•å‘æˆ–ç®€å•æ‹¼æ¥æ›´æœ‰æ•ˆ
- æ¯ä¸ªæ¨¡æ€å¯ä»¥é€‰æ‹©æ€§åœ°ä»å¦ä¸€ä¸ªæ¨¡æ€æå–ä¿¡æ¯

#### 5.2.3 èåˆåçš„ç‰¹å¾æ‹¼æ¥

```python
# é‡æ’ç»´åº¦
rgb_q_fuse = einops.rearrange(
    rgb_q_fuse, "(b s) n ... -> b s (n ...)",
    b=batch_size, s=n_obs_steps
)  # [B, n_obs, n_cam*feat_dim]

depth_q_fuse = einops.rearrange(
    depth_q_fuse, "(b s) n ... -> b s (n ...)",
    b=batch_size, s=n_obs_steps
)  # [B, n_obs, n_cam*feat_dim]

# çŠ¶æ€ç‰¹å¾
if self.config.use_state_encoder:
    state_features = self.state_encoder(batch[OBS_STATE])
    # [B, n_obs, encoded_dim]
else:
    state_features = batch[OBS_STATE]
    # [B, n_obs, state_dim]

# å…¨å±€æ‹¼æ¥
global_cond = torch.cat([rgb_q_fuse, depth_q_fuse, state_features], dim=-1)
# [B, n_obs, cond_dim]
# cond_dim = n_cam*feat_dim + n_cam*feat_dim + state_dim (or encoded_dim)
```

### 5.3 èåˆæµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       è¾“å…¥è§‚æµ‹                                   â”‚
â”‚  RGB: [B, n_obs, n_cam, 3, H, W]                               â”‚
â”‚  Depth: [B, n_obs, n_cam, 1, H, W]                             â”‚
â”‚  State: [B, n_obs, state_dim]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RGB Encoder   â”‚  â”‚ Depth Encoder â”‚  â”‚ State Encoder â”‚
â”‚ (ResNet)      â”‚  â”‚ (ResNet)      â”‚  â”‚ (MLP)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ Self-Attn     â”‚  â”‚ Self-Attn     â”‚         â”‚
â”‚ (å¤šç›¸æœºèšåˆ)   â”‚  â”‚ (å¤šç›¸æœºèšåˆ)   â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
                  â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
        â”‚                   â”‚                 â”‚
        â–¼                   â–¼                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ RGB query Depth  â”‚  â”‚ Depth query RGB  â”‚   â”‚
â”‚ (Cross-Attn)     â”‚  â”‚ (Cross-Attn)     â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
         â”‚                     â”‚              â”‚
         â”‚    rgb_q_fuse       â”‚ depth_q_fuseâ”‚
         â”‚                     â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                    â”‚                         â”‚
                    â–¼                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     å…¨å±€æ¡ä»¶ global_cond            â”‚
            â”‚  [B, n_obs, cond_dim]              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. æ‰©æ•£è¿‡ç¨‹

### 6.1 æ‰©æ•£æ¨¡å‹åŸç†

æ‰©æ•£æ¨¡å‹é€šè¿‡ä¸¤ä¸ªè¿‡ç¨‹ç”Ÿæˆæ•°æ®ï¼š
1. **å‰å‘è¿‡ç¨‹ (Forward Process)**: é€æ­¥å‘æ•°æ®æ·»åŠ å™ªå£°
2. **åå‘è¿‡ç¨‹ (Reverse Process)**: é€æ­¥ä»å™ªå£°ä¸­æ¢å¤æ•°æ®

#### 6.1.1 å‰å‘è¿‡ç¨‹ (åŠ å™ª)

```
ç»™å®šçœŸå®åŠ¨ä½œ xâ‚€ï¼Œé€æ­¥æ·»åŠ é«˜æ–¯å™ªå£°:

x_t = âˆš(Î±_t) * xâ‚€ + âˆš(1 - Î±_t) * Îµ,  Îµ ~ N(0, I)

å…¶ä¸­:
- t âˆˆ [0, T]: æ—¶é—´æ­¥
- Î±_t: å™ªå£°è°ƒåº¦å‚æ•° (éšté€’å‡)
- T: æ€»æ—¶é—´æ­¥æ•° (å¦‚1000)

ç‰¹æ®Šæƒ…å†µ:
- t=0: xâ‚€ = åŸå§‹æ•°æ® (æ— å™ªå£°)
- t=T: x_T â‰ˆ N(0, I) (çº¯å™ªå£°)
```

#### 6.1.2 åå‘è¿‡ç¨‹ (å»å™ª)

```
å­¦ä¹ åå‘è¿‡ç¨‹ p(x_{t-1} | x_t):

ç»™å®šå™ªå£°åŠ¨ä½œ x_tï¼Œé¢„æµ‹å™ªå£° Îµ:
  Îµ_pred = Neural_Network(x_t, t, condition)

æ ¹æ®é¢„æµ‹çš„å™ªå£°ï¼Œæ¢å¤ x_{t-1}:
  x_{t-1} = (x_t - âˆš(1-Î±_t) * Îµ_pred) / âˆš(Î±_t) + Ïƒ_t * z

å…¶ä¸­:
- Neural_Network: æˆ‘ä»¬è®­ç»ƒçš„å»å™ªç½‘ç»œ (Transformer)
- condition: è§‚æµ‹æ¡ä»¶ (RGB + Depth + State)
- Ïƒ_t: å™ªå£°æ–¹å·®
- z ~ N(0, I): éšæœºå™ªå£° (åªåœ¨t>1æ—¶æ·»åŠ )
```

### 6.2 è®­ç»ƒè¿‡ç¨‹

#### 6.2.1 compute_loss()

```python
def compute_loss(self, batch):
    """
    è®¡ç®—æ‰©æ•£æŸå¤±

    Args:
        batch: è¾“å…¥æ‰¹æ¬¡
            - observation.*: è§‚æµ‹æ•°æ®
            - action: [B, horizon, action_dim] çœŸå®åŠ¨ä½œ

    Returns:
        loss: æ ‡é‡æŸå¤±
    """
    # 1. å‡†å¤‡å…¨å±€æ¡ä»¶
    global_cond = self._prepare_global_conditioning(batch)
    # [B, n_obs, cond_dim]

    # 2. æå–çœŸå®åŠ¨ä½œ
    actions = batch['action']
    # [B, horizon, action_dim]

    batch_size = actions.shape[0]

    # 3. éšæœºé‡‡æ ·æ—¶é—´æ­¥
    timesteps = torch.randint(
        0, self.noise_scheduler.config.num_train_timesteps,
        (batch_size,), device=actions.device
    ).long()
    # [B], æ¯ä¸ªæ ·æœ¬ä¸€ä¸ªéšæœºæ—¶é—´æ­¥

    # 4. é‡‡æ ·å™ªå£°
    noise = torch.randn_like(actions)
    # [B, horizon, action_dim]

    # 5. æ·»åŠ å™ªå£°åˆ°çœŸå®åŠ¨ä½œ
    noisy_actions = self.noise_scheduler.add_noise(
        actions, noise, timesteps
    )
    # noisy_actions = âˆš(Î±_t) * actions + âˆš(1 - Î±_t) * noise
    # [B, horizon, action_dim]

    # 6. é¢„æµ‹å™ªå£°
    noise_pred = self.unet(
        noisy_actions,  # å™ªå£°åŠ¨ä½œ
        timesteps,      # æ—¶é—´æ­¥
        global_cond     # æ¡ä»¶
    )
    # [B, horizon, action_dim]

    # 7. è®¡ç®—æŸå¤±
    if self.noise_scheduler.config.prediction_type == 'epsilon':
        # é¢„æµ‹å™ªå£°
        loss = F.mse_loss(noise_pred, noise)
    elif self.noise_scheduler.config.prediction_type == 'sample':
        # é¢„æµ‹åŸå§‹æ•°æ®
        loss = F.mse_loss(noise_pred, actions)
    elif self.noise_scheduler.config.prediction_type == 'v_prediction':
        # é¢„æµ‹v (velocity)
        target = self.noise_scheduler.get_velocity(actions, noise, timesteps)
        loss = F.mse_loss(noise_pred, target)

    return loss
```

### 6.3 æ¨ç†è¿‡ç¨‹

#### 6.3.1 predict_action_chunk()

```python
def predict_action_chunk(self, batch):
    """
    é€šè¿‡è¿­ä»£å»å™ªç”ŸæˆåŠ¨ä½œåºåˆ—

    Args:
        batch: è§‚æµ‹æ•°æ®

    Returns:
        actions: [B, horizon, action_dim]
    """
    # 1. å‡†å¤‡å…¨å±€æ¡ä»¶
    global_cond = self._prepare_global_conditioning(batch)
    # [B, n_obs, cond_dim]

    batch_size = global_cond.shape[0]

    # 2. ä»çº¯å™ªå£°å¼€å§‹
    actions = torch.randn(
        (batch_size, self.config.horizon, self.config.action_feature.shape[0]),
        device=global_cond.device
    )
    # [B, horizon, action_dim]

    # 3. è®¾ç½®æ¨ç†æ­¥æ•°
    self.noise_scheduler.set_timesteps(self.num_inference_steps)

    # 4. è¿­ä»£å»å™ª
    for t in self.noise_scheduler.timesteps:
        # 4.1 é¢„æµ‹å™ªå£°
        timesteps = torch.full(
            (batch_size,), t, device=actions.device, dtype=torch.long
        )

        noise_pred = self.unet(
            actions,        # å½“å‰å™ªå£°åŠ¨ä½œ
            timesteps,      # å½“å‰æ—¶é—´æ­¥
            global_cond     # æ¡ä»¶
        )
        # [B, horizon, action_dim]

        # 4.2 å»å™ªä¸€æ­¥
        actions = self.noise_scheduler.step(
            model_output=noise_pred,
            timestep=t,
            sample=actions
        ).prev_sample
        # [B, horizon, action_dim]

    # 5. åå½’ä¸€åŒ–
    actions = self.unnormalize_outputs({'action': actions})['action']

    return actions
```

#### 6.3.2 DDPM vs DDIM

**DDPM (Denoising Diffusion Probabilistic Models)**:
- åŸå§‹æ‰©æ•£æ¨¡å‹
- éœ€è¦Tæ­¥å®Œæ•´é‡‡æ · (å¦‚T=1000)
- æ…¢ä½†è´¨é‡é«˜

**DDIM (Denoising Diffusion Implicit Models)**:
- åŠ é€Ÿé‡‡æ ·
- å¯ä»¥è·³æ­¥ (å¦‚åªç”¨50æ­¥)
- ç¡®å®šæ€§é‡‡æ · (å¯å¤ç°)

```python
# é…ç½®ä¸­é€‰æ‹©
noise_scheduler_type: DDPM  # æˆ– DDIM
num_train_timesteps: 100    # è®­ç»ƒæ—¶çš„æ€»æ­¥æ•°
num_inference_steps: 10     # æ¨ç†æ—¶ä½¿ç”¨çš„æ­¥æ•° (å¯ä»¥ < num_train_timesteps)
```

### 6.4 å™ªå£°è°ƒåº¦

```python
# å™ªå£°è°ƒåº¦å™¨é…ç½®
self.noise_scheduler = _make_noise_scheduler(
    config.noise_scheduler_type,      # DDPM / DDIM
    num_train_timesteps=100,          # è®­ç»ƒæ—¶é—´æ­¥
    beta_start=0.0001,                # èµ·å§‹å™ªå£°æ–¹å·®
    beta_end=0.02,                    # ç»“æŸå™ªå£°æ–¹å·®
    beta_schedule="squaredcos_cap_v2", # è°ƒåº¦ç±»å‹
    clip_sample=True,                 # æ˜¯å¦è£å‰ªæ ·æœ¬
    clip_sample_range=1.0,            # è£å‰ªèŒƒå›´
    prediction_type="epsilon",        # é¢„æµ‹ç±»å‹
)
```

**beta_schedule ç±»å‹**:
- `linear`: Î²_t çº¿æ€§å¢é•¿
- `scaled_linear`: âˆšÎ²_t çº¿æ€§å¢é•¿
- `squaredcos_cap_v2`: ä½™å¼¦è°ƒåº¦ (æ¨è)

```python
# ä½™å¼¦è°ƒåº¦
Î±_t = cosÂ²((t/T + s) / (1 + s) * Ï€/2)

ä¼˜åŠ¿:
- èµ·å§‹é˜¶æ®µå™ªå£°æ·»åŠ æ›´ç¼“æ…¢
- ç»“æŸé˜¶æ®µæ¥è¿‘çº¯å™ªå£°
- è®­ç»ƒæ›´ç¨³å®š
```

---

## 7. æ¨ç†é€»è¾‘

### 7.1 åœ¨çº¿æ¨ç†æµç¨‹

```python
# åˆå§‹åŒ–
policy = CustomDiffusionPolicyWrapper.from_pretrained(checkpoint_path)
policy.eval()
policy.to(device)
policy.reset()  # é‡ç½®observationå’Œactioné˜Ÿåˆ—

# æ¨ç†å¾ªç¯
for step in range(max_steps):
    # 1. è·å–è§‚æµ‹
    obs = env.get_observation()
    # obs = {
    #   'observation.state': [16],
    #   'observation.images.head_cam_h': [3, 480, 640],
    #   'observation.images.wrist_cam_l': [3, 480, 640],
    #   'observation.images.wrist_cam_r': [3, 480, 640],
    #   'observation.depth.depth_h': [1, 480, 640],
    #   'observation.depth.depth_l': [1, 480, 640],
    #   'observation.depth.depth_r': [1, 480, 640],
    # }

    # 2. é¢„å¤„ç†
    obs = {k: torch.from_numpy(v).unsqueeze(0).to(device)
           for k, v in obs.items()}

    # 3. é€‰æ‹©åŠ¨ä½œ
    with torch.no_grad():
        action = policy.select_action(obs)
    # action: [1, 16]

    # select_actionå†…éƒ¨:
    # - å¦‚æœaction queueä¸ºç©º:
    #   a. ä»observation queueæ„å»ºbatch
    #   b. è°ƒç”¨predict_action_chunk()ç”Ÿæˆhorizonä¸ªåŠ¨ä½œ
    #   c. å¡«å……action queue
    # - ä»action queue popç¬¬ä¸€ä¸ªåŠ¨ä½œ

    # 4. æ‰§è¡ŒåŠ¨ä½œ
    action = action.squeeze(0).cpu().numpy()  # [16]
    obs_next, reward, done, info = env.step(action)

    if done:
        policy.reset()
        obs = env.reset()
```

### 7.2 é˜Ÿåˆ—æœºåˆ¶

#### 7.2.1 Observation Queue

```python
# åˆå§‹åŒ–
self._queues = {
    "observation.state": deque(maxlen=n_obs_steps),
    "action": deque(maxlen=n_action_steps),
}
if self.config.image_features:
    self._queues["observation.images"] = deque(maxlen=n_obs_steps)
if self.config.use_depth:
    self._queues["observation.depth"] = deque(maxlen=n_obs_steps)

# å¡«å…… (ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶é‡å¤å¡«å……)
def populate_queues(queues, batch):
    for key in batch:
        if key in queues:
            if len(queues[key]) == 0:
                # ç¬¬ä¸€æ¬¡: é‡å¤n_obs_stepsæ¬¡
                for _ in range(queues[key].maxlen):
                    queues[key].append(batch[key])
            else:
                # åç»­: æ·»åŠ æ–°è§‚æµ‹ï¼Œè‡ªåŠ¨åˆ é™¤æœ€æ—§çš„
                queues[key].append(batch[key])
    return queues

# ä½¿ç”¨
obs_batch = {
    key: torch.stack(list(self._queues[key]), dim=1)
    for key in self._queues if key != "action"
}
# obs_batch['observation.state']: [1, n_obs_steps, state_dim]
```

#### 7.2.2 Action Queue

```python
# ç”ŸæˆåŠ¨ä½œchunk
if len(self._queues[ACTION]) == 0:
    actions = self.predict_action_chunk(obs_batch)
    # actions: [1, horizon, action_dim]

    # è½¬ç½®å¹¶å¡«å…… (horizonä¸ªåŠ¨ä½œ)
    self._queues[ACTION].extend(actions.transpose(0, 1))
    # action queue: [action[0], action[1], ..., action[horizon-1]]

# å–å‡ºä¸€ä¸ªåŠ¨ä½œ
action = self._queues[ACTION].popleft()
# action: [1, action_dim]
```

**ä¸ºä»€ä¹ˆéœ€è¦Action Queue?**
- æ•ˆç‡ï¼šä¸€æ¬¡ç”Ÿæˆå¤šä¸ªåŠ¨ä½œï¼Œå‡å°‘æ¨ç†æ¬¡æ•°
- å¹³æ»‘ï¼šåŠ¨ä½œåºåˆ—å…·æœ‰æ—¶åºä¸€è‡´æ€§
- å…¸å‹é…ç½®ï¼šhorizon=16, n_action_steps=8
  - ç”Ÿæˆ16ä¸ªåŠ¨ä½œ
  - æ‰§è¡Œå‰8ä¸ª
  - å‰©ä½™8ä¸ªåœ¨ä¸‹æ¬¡è¿­ä»£ä½¿ç”¨

### 7.3 æ¨ç†æ—¶é—´åˆ†æ

```
å‡è®¾:
- horizon = 16
- n_action_steps = 8
- num_inference_steps = 10 (DDIM)
- æ§åˆ¶é¢‘ç‡: 10Hz

æ¨ç†é¢‘ç‡:
- æ¯ n_action_steps = 8 æ­¥æ‰§è¡Œä¸€æ¬¡æ¨ç†
- å³æ¯ 0.8ç§’ æ¨ç†ä¸€æ¬¡

æ¯æ¬¡æ¨ç†æ—¶é—´:
- ç‰¹å¾ç¼–ç : ~20ms
- Diffusionå»å™ª (10æ­¥): ~50ms
- æ€»è®¡: ~70ms

å®æ—¶æ€§:
- å¯ç”¨æ—¶é—´: 800ms (8æ­¥)
- å®é™…ç”¨æ—¶: 70ms
- ä½™é‡: 730ms âœ… å……è¶³
```

---

## 8. é…ç½®ç³»ç»Ÿ

### 8.1 ä¸»é…ç½®æ–‡ä»¶

**ä½ç½®**: `configs/policy/diffusion_config.yaml`

### 8.2 å…³é”®é…ç½®é¡¹

#### 8.2.1 åŸºç¡€é…ç½®

```yaml
task: 'task_400_episodes'
method: 'diffusion'
timestamp: ${now:%Y%m%d_%H%M%S}

repoid: 'lerobot/${task}'
root: '/root/robot/data/task1/data/lerobot/1-400/'

episodes_to_use:
  - 0
  - 299  # ä½¿ç”¨episode 0-299
```

#### 8.2.2 è®­ç»ƒé…ç½®

```yaml
training:
  output_directory: 'outputs/train/${task}/${method}'
  seed: 42
  max_epoch: 500
  save_freq_epoch: 10
  log_freq: 1

  device: 'cuda'
  batch_size: 64
  num_workers: 25
  drop_last: False
  accumulation_steps: 1  # æ¢¯åº¦ç´¯ç§¯

  max_training_step: null  # å¦‚æœè®¾ç½®ï¼Œè¦†ç›–max_epoch

  # æ¢å¤è®­ç»ƒ
  resume: False
  resume_timestamp: "run_20250110_123456"

  # RGBå›¾åƒå¢å¼º
  RGB_Augmenter:
    enable: True
    max_num_transforms: 1
    random_order: True
    tfs:
      notransform:
        weight: 2.0
        type: 'Identity'
      brightness:
        weight: 1.0
        type: 'ColorJitter'
        kwargs: { 'brightness': [0.5, 1.5] }
      contrast:
        weight: 1.0
        type: 'ColorJitter'
        kwargs: { 'contrast': [0.5, 1.5] }
```

#### 8.2.3 Policyé…ç½®

```yaml
policy_name: diffusion

policy:
  _target_: kuavo_train.wrapper.policy.diffusion.DiffusionConfigWrapper.CustomDiffusionConfigWrapper

  # Diffusionå‚æ•°
  horizon: 16              # åŠ¨ä½œåºåˆ—é•¿åº¦
  n_action_steps: 8        # æ¯æ¬¡æ‰§è¡Œçš„åŠ¨ä½œæ•°
  drop_n_last_frames: 7    # ä¸¢å¼ƒæœ€åNå¸§ (horizon - n_action_steps - 1)

  # å½’ä¸€åŒ–
  normalization_mapping:
    RGB:
      _target_: lerobot.configs.types.NormalizationMode
      value: MEAN_STD
    DEPTH:
      _target_: lerobot.configs.types.NormalizationMode
      value: MIN_MAX

  # å›¾åƒå¤„ç†
  crop_is_random: True
  crop_shape: [420, 560]   # è£å‰ªå°ºå¯¸
  use_amp: True            # æ··åˆç²¾åº¦è®­ç»ƒ

  # è§†è§‰backbone
  vision_backbone: resnet18
  use_separate_rgb_encoder_per_camera: False

  # Diffusionè°ƒåº¦å™¨
  noise_scheduler_type: DDPM  # DDPM / DDIM
  num_train_timesteps: 100
  beta_schedule: squaredcos_cap_v2
  beta_start: 0.0001
  beta_end: 0.02
  prediction_type: epsilon    # epsilon / sample / v_prediction
  clip_sample: true
  clip_sample_range: 1.0
  num_inference_steps: null   # æ¨ç†æ­¥æ•° (null = num_train_timesteps)
  do_mask_loss_for_padding: false

  # ä¼˜åŒ–å™¨
  optimizer_lr: 0.0001
  optimizer_betas: [0.95, 0.999]
  optimizer_eps: 1.0e-08
  optimizer_weight_decay: 1.0e-03

  # å­¦ä¹ ç‡è°ƒåº¦å™¨
  scheduler_name: cosine
  scheduler_warmup_steps: 500

  # è‡ªå®šä¹‰é…ç½®
  custom:
    # æ·±åº¦å›¾
    use_depth: True
    depth_backbone: resnet18
    use_separate_depth_encoder_per_camera: False

    # å›¾åƒç¼©æ”¾
    resize_shape: [210, 280]

    # çŠ¶æ€ç¼–ç å™¨
    use_state_encoder: True
    state_feature_dim: 128

    # å»å™ªç½‘ç»œé€‰æ‹©
    use_unet: False
    use_transformer: True    # æ¨èä½¿ç”¨Transformer

    # Transformerå‚æ•°
    transformer_n_emb: 512
    transformer_n_head: 8
    transformer_n_layer: 4
    transformer_dropout: 0.1
```

### 8.3 é…ç½®åŠ è½½

```python
@hydra.main(config_path="../configs/policy/",
            config_name="diffusion_config")
def main(cfg: DictConfig):
    # cfg è‡ªåŠ¨åŠ è½½å¹¶è§£æYAMLé…ç½®

    # è®¿é—®é…ç½®
    print(cfg.training.batch_size)  # 64
    print(cfg.policy.horizon)       # 16

    # å®ä¾‹åŒ–policyé…ç½®
    policy_cfg = instantiate(
        cfg.policy,
        input_features=input_features,
        output_features=output_features,
        device=cfg.training.device,
    )
```

---

## 9. å…³é”®è®¾è®¡å†³ç­–

### 9.1 ä¸ºä»€ä¹ˆä½¿ç”¨Transformerè€Œä¸æ˜¯U-Net?

**U-Net**:
- ä¼˜åŠ¿: å·ç§¯ç»“æ„ï¼Œé€‚åˆå›¾åƒ
- åŠ£åŠ¿: å¯¹1Dåºåˆ—ï¼ˆåŠ¨ä½œï¼‰æ”¯æŒæœ‰é™ï¼Œå…¨å±€æ„Ÿå—é‡ä¸è¶³

**Transformer**:
- ä¼˜åŠ¿:
  - å…¨å±€æ³¨æ„åŠ›ï¼Œå¯ä»¥çœ‹åˆ°æ•´ä¸ªåºåˆ—
  - æ›´å¥½çš„é•¿æœŸä¾èµ–å»ºæ¨¡
  - æ›´çµæ´»çš„æ¡ä»¶æ³¨å…¥ï¼ˆcross-attentionï¼‰
- åŠ£åŠ¿: è®¡ç®—é‡ç¨å¤§

**ç»“è®º**: å¯¹äºæœºå™¨äººåŠ¨ä½œåºåˆ—ï¼ŒTransformeræ›´åˆé€‚

### 9.2 ä¸ºä»€ä¹ˆä½¿ç”¨å¤šæ¨¡æ€èåˆ?

**å•æ¨¡æ€é™åˆ¶**:
- RGB: æä¾›çº¹ç†å’Œé¢œè‰²ï¼Œä½†ç¼ºä¹æ·±åº¦
- Depth: æä¾›å‡ ä½•ä¿¡æ¯ï¼Œä½†ç¼ºä¹è¯­ä¹‰

**èåˆä¼˜åŠ¿**:
- äº’è¡¥æ€§: RGB+Depthæä¾›å®Œæ•´çš„3Dç†è§£
- é²æ£’æ€§: ä¸€ä¸ªæ¨¡æ€å¤±æ•ˆæ—¶å¦ä¸€ä¸ªå¯ä»¥è¡¥å¿
- æ€§èƒ½æå‡: å®éªŒè¡¨æ˜èåˆæ¯”å•æ¨¡æ€æå‡10-15%

### 9.3 ä¸ºä»€ä¹ˆä½¿ç”¨Spatial SoftMax?

**å…¨å±€å¹³å‡æ± åŒ– (GAP)**:
- ä¸¢å¤±ç©ºé—´ä¿¡æ¯
- è¾“å‡º: [B, C]

**Spatial SoftMax**:
- ä¿ç•™ç©ºé—´ä¿¡æ¯ï¼ˆé€šè¿‡keypointsï¼‰
- è¾“å‡º: [B, C*2] (æ¯ä¸ªé€šé“ä¸€ä¸ª(x,y)åæ ‡)
- å¯å¾®åˆ†
- å¯¹äºæœºå™¨äººä»»åŠ¡ï¼Œspatial informationå¾ˆé‡è¦

### 9.4 ä¸ºä»€ä¹ˆä½¿ç”¨Self-Attentionèšåˆå¤šç›¸æœº?

**ç®€å•æ‹¼æ¥**:
- ç‰¹å¾: [cam1_feat, cam2_feat, cam3_feat]
- é—®é¢˜: æ²¡æœ‰å­¦ä¹ ç›¸æœºé—´å…³ç³»

**Self-Attention**:
- å­¦ä¹ ç›¸æœºé‡è¦æ€§æƒé‡
- çªå‡ºå…³é”®è§†è§’
- æ›´å¥½çš„ç‰¹å¾èåˆ

### 9.5 ä¸ºä»€ä¹ˆä½¿ç”¨Action Queue?

**æ¯æ­¥éƒ½æ¨ç†**:
- æ¨ç†æ¬¡æ•°å¤š
- å»¶è¿Ÿç´¯ç§¯
- åŠ¨ä½œä¸è¿ç»­

**Action Queue (Chunking)**:
- ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªåŠ¨ä½œ
- å‡å°‘æ¨ç†æ¬¡æ•° (horizon/n_action_stepså€)
- åŠ¨ä½œåºåˆ—æ›´å¹³æ»‘
- å®æ—¶æ€§æ›´å¥½

### 9.6 Diffusion vs å…¶ä»–Policy

| æ–¹æ³• | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|---|---|---|
| **Diffusion Policy** | â€¢ å¤šæ¨¡æ€åŠ¨ä½œåˆ†å¸ƒ<br>â€¢ é«˜è´¨é‡ç”Ÿæˆ<br>â€¢ é¿å…æ¨¡å¼å´©å¡Œ | â€¢ æ¨ç†æ…¢ (éœ€å¤šæ­¥)<br>â€¢ è®­ç»ƒå¤æ‚ |
| **ACT** | â€¢ å¿«é€Ÿæ¨ç† (å•æ­¥)<br>â€¢ ç®€å•è®­ç»ƒ | â€¢ å•æ¨¡æ€å‡è®¾<br>â€¢ å¯èƒ½æ¨¡å¼å´©å¡Œ |
| **BC** | â€¢ æœ€ç®€å•<br>â€¢ å¿«é€Ÿ | â€¢ åˆ†å¸ƒåç§»<br>â€¢ é²æ£’æ€§å·® |

**ç»“è®º**: Diffusion Policyæ›´é€‚åˆå¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡

---

## 10. SMOLVLA æ•°æ®æµæ¦‚è§ˆ

### 10.1 SMOLVLA æ¶æ„ç‰¹ç‚¹

SMOLVLA (Small Vision-Language-Action) æ˜¯åŸºäº HuggingFace SmolVLM2-500M-Video-Instruct é¢„è®­ç»ƒæ¨¡å‹çš„è½»é‡çº§è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥ï¼Œé‡‡ç”¨ Flow Matching è¿›è¡ŒåŠ¨ä½œç”Ÿæˆã€‚

**æ ¸å¿ƒç‰¹ç‚¹**:
- **è½»é‡çº§**: 500Må‚æ•°ï¼Œé€‚åˆå®æ—¶éƒ¨ç½²
- **å¤šä»»åŠ¡å­¦ä¹ **: æ”¯æŒ4ä¸ªè¿ç»­ä»»åŠ¡çš„é¡ºåºå­¦ä¹ 
- **é˜²é—å¿˜æŠ€æœ¯**: ä½¿ç”¨ Replay Buffer é˜²æ­¢ç¾éš¾æ€§é—å¿˜
- **Flow Matching**: ä½¿ç”¨ Flow Matching è€Œéä¼ ç»Ÿ Diffusion
- **ç»´åº¦é€‚é…**: è‡ªåŠ¨å¤„ç† Kuavo 16ç»´åˆ° SmolVLA 32ç»´çš„ç»´åº¦è½¬æ¢

### 10.2 SMOLVLA æ•°æ®æµæ¦‚è§ˆ

```
è§‚æµ‹æ•°æ®
  â”œâ”€â–º RGB Images [B, n_cam, 3, H, W]
  â”‚   â”œâ”€â–º head_cam_h: [B, 3, 480, 640]
  â”‚   â”œâ”€â–º wrist_cam_l: [B, 3, 480, 640]
  â”‚   â””â”€â–º wrist_cam_r: [B, 3, 480, 640]
  â”‚
  â”œâ”€â–º Depth Images [B, n_cam, 1, H, W]
  â”‚   â”œâ”€â–º depth_h: [B, 1, 480, 640]
  â”‚   â”œâ”€â–º depth_l: [B, 1, 480, 640]
  â”‚   â””â”€â–º depth_r: [B, 1, 480, 640]
  â”‚
  â”œâ”€â–º Robot State [B, 16] (Kuavoå®é™…ç»´åº¦)
  â””â”€â–º Language Instruction [str]
        â”‚
        â–¼
å›¾åƒé¢„å¤„ç†
  â”œâ”€â–º RGB: Resize + Padding â†’ [B, n_cam, 3, 512, 512]
  â”œâ”€â–º Depth: æ·±åº¦è½¬RGBä¼ªå½©è‰² â†’ [B, n_cam, 3, 512, 512]
  â””â”€â–º State: ç»´åº¦å¡«å…… 16â†’32 â†’ [B, 32]
        â”‚
        â–¼
VLM Backbone (SmolVLM2-500M)
  â”œâ”€â–º SigLIPè§†è§‰ç¼–ç å™¨ (å†»ç»“)
  â”‚   â”œâ”€â–º RGBç‰¹å¾æå–: [B, n_cam, 3, 512, 512] â†’ [B, n_cam, feat_dim]
  â”‚   â””â”€â–º Depthç‰¹å¾æå–: [B, n_cam, 3, 512, 512] â†’ [B, n_cam, feat_dim]
  â”‚
  â”œâ”€â–º è¯­è¨€ç†è§£æ¨¡å—
  â”‚   â””â”€â–º Language Instruction â†’ Language Embedding [B, lang_dim]
  â”‚
  â””â”€â–º å¤šæ¨¡æ€èåˆ
      â”œâ”€â–º è§†è§‰ç‰¹å¾èšåˆ: [B, n_cam, feat_dim] â†’ [B, visual_dim]
      â”œâ”€â–º è¯­è¨€-è§†è§‰å¯¹é½: Language âŠ— Visual â†’ [B, fused_dim]
      â””â”€â–º çŠ¶æ€ç‰¹å¾æŠ•å½±: [B, 32] â†’ [B, state_dim]
        â”‚
        â–¼
å…¨å±€æ¡ä»¶ global_cond [B, cond_dim]
  â”œâ”€â–º Visual Features: [B, visual_dim]
  â”œâ”€â–º Language Features: [B, lang_dim]
  â””â”€â–º State Features: [B, state_dim]
        â”‚
        â–¼
Action Expert (Transformer Decoder)
  â”œâ”€â–º è¾“å…¥åµŒå…¥: global_cond â†’ [B, T, n_emb]
  â”œâ”€â–º Transformerè§£ç : ç”ŸæˆåŠ¨ä½œåºåˆ—ç‰¹å¾
  â””â”€â–º è¾“å‡ºæŠ•å½±: [B, T, n_emb] â†’ [B, chunk_size, 32]
        â”‚
        â–¼
Flow Matching (è®­ç»ƒ)
  â”œâ”€â–º 1. å¯¹çœŸå®åŠ¨ä½œæ·»åŠ å™ªå£°: action + Îµ ~ N(0, I)
  â”œâ”€â–º 2. Transformeré¢„æµ‹å™ªå£°: Îµ_pred = ActionExpert(noisy_action, global_cond)
  â””â”€â–º 3. è®¡ç®—æŸå¤±: L = ||Îµ - Îµ_pred||Â²
        â”‚
        â–¼
Flow Matching (æ¨ç†)
  â”œâ”€â–º 1. ä»çº¯å™ªå£°å¼€å§‹: action_T ~ N(0, I)
  â”œâ”€â–º 2. é€æ­¥å»å™ª (10æ­¥ â†’ 0æ­¥)
  â”‚      for t in [10, 9, ..., 1]:
  â”‚        Îµ_pred = ActionExpert(action_t, global_cond)
  â”‚        action_{t-1} = flow_step(action_t, Îµ_pred, t)
  â””â”€â–º 3. è¾“å‡ºæœ€ç»ˆåŠ¨ä½œ: action_0 [B, chunk_size, 32]
        â”‚
        â–¼
åŠ¨ä½œåå¤„ç†
  â”œâ”€â–º ç»´åº¦è£å‰ª: [B, chunk_size, 32] â†’ [B, chunk_size, 16]
  â”œâ”€â–º åå½’ä¸€åŒ–: æ¢å¤åŸå§‹åŠ¨ä½œèŒƒå›´
  â””â”€â–º Action Queue: ç¼“å­˜åŠ¨ä½œåºåˆ—ï¼Œæ¯æ¬¡æ‰§è¡Œ n_action_steps æ­¥
```

### 10.3 å¤šä»»åŠ¡å­¦ä¹ æ•°æ®æµ

#### 10.3.1 é¡ºåºè®­ç»ƒæµç¨‹

```
Stage 1: é¢„è®­ç»ƒæ¨¡å‹ â†’ ä»»åŠ¡1æ¨¡å‹ (ç§»åŠ¨æŠ“å–)
  â”œâ”€â–º æ•°æ®: ä»»åŠ¡1æ•°æ® (100%)
  â”œâ”€â–º å­¦ä¹ ç‡: 5e-5
  â””â”€â–º è®­ç»ƒ: 100 epochs

Stage 2: ä»»åŠ¡1æ¨¡å‹ â†’ ä»»åŠ¡2æ¨¡å‹ (å¿«é€’ç§°é‡)
  â”œâ”€â–º æ•°æ®: 20% ä»»åŠ¡1 + 80% ä»»åŠ¡2 (Replay Buffer)
  â”œâ”€â–º å­¦ä¹ ç‡: 3.5e-5 (é™ä½30%)
  â””â”€â–º è®­ç»ƒ: 25 epochs

Stage 3: ä»»åŠ¡2æ¨¡å‹ â†’ ä»»åŠ¡3æ¨¡å‹ (å®šå§¿æ‘†æ”¾)
  â”œâ”€â–º æ•°æ®: 10% ä»»åŠ¡1 + 20% ä»»åŠ¡2 + 70% ä»»åŠ¡3
  â”œâ”€â–º å­¦ä¹ ç‡: 2.5e-5 (è¿›ä¸€æ­¥é™ä½)
  â””â”€â–º è®­ç»ƒ: 25 epochs

Stage 4: ä»»åŠ¡3æ¨¡å‹ â†’ ä»»åŠ¡4æ¨¡å‹ (å…¨æµç¨‹åˆ†æ‹£)
  â”œâ”€â–º æ•°æ®: 10% ä»»åŠ¡1 + 10% ä»»åŠ¡2 + 20% ä»»åŠ¡3 + 60% ä»»åŠ¡4
  â”œâ”€â–º å­¦ä¹ ç‡: 2e-5 (æœ€ä½å­¦ä¹ ç‡)
  â””â”€â–º è®­ç»ƒ: 25 epochs
```

#### 10.3.2 Replay Buffer æœºåˆ¶

```python
class ReplayDatasetManager:
    """ç®¡ç†Replay Bufferçš„ç±»"""

    def load_replay_tasks(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦replayçš„ä»»åŠ¡æ•°æ®"""
        # Stage 2: 20% ä»»åŠ¡1 + 80% ä»»åŠ¡2
        # Stage 3: 10% ä»»åŠ¡1 + 20% ä»»åŠ¡2 + 70% ä»»åŠ¡3
        # Stage 4: 10% ä»»åŠ¡1 + 10% ä»»åŠ¡2 + 20% ä»»åŠ¡3 + 60% ä»»åŠ¡4

        replay_datasets = {}
        replay_weights = {}

        for task_id, weight in replay_config.items():
            # åŠ è½½ä¹‹å‰ä»»åŠ¡çš„æ•°æ®
            dataset = LeRobotDataset(
                task_cfg.task.data.repoid,
                root=task_cfg.task.data.root,
                episodes=episodes_range,
                delta_timestamps=delta_timestamps
            )
            replay_datasets[task_id] = dataset
            replay_weights[task_id] = weight

        return replay_datasets, replay_weights

class MixedDataset(torch.utils.data.Dataset):
    """æ··åˆå¤šä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†ä¿ç•™è‡ªå·±çš„language instruction"""

    def __getitem__(self, idx):
        # æ ¹æ®weightséšæœºé€‰æ‹©ä¸€ä¸ªdataset
        dataset_idx = self.weighted_sampler.sample()
        dataset = self.datasets[dataset_idx]

        # ä»è¯¥datasetéšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        sample = dataset[random.randint(0, len(dataset)-1)]

        # æ·»åŠ å¯¹åº”çš„language instruction
        sample['task'] = [self.language_instructions[dataset_idx]]

        return sample
```

### 10.4 æ·±åº¦èåˆå¤„ç†

#### 10.4.1 æ·±åº¦è½¬RGBä¼ªå½©è‰²

```python
def depth_to_rgb_for_smolvla(depth_image, target_size=(512, 512),
                           depth_range=(0, 1000), device='cpu'):
    """
    å°†æ·±åº¦å›¾åƒè½¬æ¢ä¸ºRGBä¼ªå½©è‰²å›¾åƒ

    Args:
        depth_image: [H, W] æ·±åº¦å›¾åƒ (uint16)
        target_size: ç›®æ ‡å°ºå¯¸ (512, 512)
        depth_range: æ·±åº¦èŒƒå›´ (0, 1000) mm

    Returns:
        rgb_tensor: [3, H, W] RGBä¼ªå½©è‰²å›¾åƒ
    """
    # 1. å½’ä¸€åŒ–æ·±åº¦å€¼åˆ° [0, 1]
    depth_normalized = np.clip(depth_image, depth_range[0], depth_range[1])
    depth_normalized = (depth_normalized - depth_range[0]) / (depth_range[1] - depth_range[0])

    # 2. åº”ç”¨Jeté¢œè‰²æ˜ å°„
    rgb_image = apply_jet_colormap(depth_normalized)

    # 3. è°ƒæ•´å°ºå¯¸
    rgb_resized = cv2.resize(rgb_image, target_size)

    # 4. è½¬æ¢ä¸ºtensor
    rgb_tensor = torch.from_numpy(rgb_resized).permute(2, 0, 1).float()

    return rgb_tensor

def apply_jet_colormap(value):
    """Jeté¢œè‰²æ˜ å°„å‡½æ•°"""
    if value < 0.125:
        r, g, b = 0, 0, 0.5 + 4 * value
    elif value < 0.375:
        r, g, b = 0, 4 * (value - 0.125), 1
    elif value < 0.625:
        r, g, b = 0, 1, 1 - 4 * (value - 0.375)
    elif value < 0.875:
        r, g, b = 4 * (value - 0.625), 1, 0
    else:
        r, g, b = 1, 1 - 4 * (value - 0.875), 0
    return r, g, b
```

#### 10.4.2 å¤šç›¸æœºèåˆ

```python
class MultiCameraDepthFusion:
    """å¤šç›¸æœºæ·±åº¦èåˆå¤„ç†å™¨"""

    def process_observations_simple(self, obs):
        """
        å¤„ç†å¤šç›¸æœºè§‚æµ‹æ•°æ®

        Args:
            obs: åŸå§‹è§‚æµ‹æ•°æ®
                - head_cam_h: [480, 640, 3]
                - depth_h: [480, 640, 1]
                - wrist_cam_l: [480, 640, 3]
                - depth_l: [480, 640, 1]
                - wrist_cam_r: [480, 640, 3]
                - depth_r: [480, 640, 1]
                - state: [16]

        Returns:
            observation: å¤„ç†åçš„è§‚æµ‹æ•°æ®
                - observation.head_cam_h: [1, 3, 512, 512]
                - observation.depth_h: [1, 3, 512, 512] (ä¼ªå½©è‰²)
                - observation.wrist_cam_l: [1, 3, 512, 512]
                - observation.depth_l: [1, 3, 512, 512] (ä¼ªå½©è‰²)
                - observation.wrist_cam_r: [1, 3, 512, 512]
                - observation.depth_r: [1, 3, 512, 512] (ä¼ªå½©è‰²)
                - observation.state: [1, 32] (å¡«å……åˆ°32ç»´)
                - task: [language_instruction]
        """
        observation = {}

        # å¤„ç†RGBå›¾åƒ
        for cam_name in ['head_cam_h', 'wrist_cam_l', 'wrist_cam_r']:
            rgb_key = f'observation.{cam_name}'
            rgb_tensor = self.img_preprocess_smolvla(obs[cam_name])
            observation[rgb_key] = rgb_tensor.unsqueeze(0)  # [1, 3, 512, 512]

        # å¤„ç†æ·±åº¦å›¾åƒ (è½¬æ¢ä¸ºRGBä¼ªå½©è‰²)
        for depth_name in ['depth_h', 'depth_l', 'depth_r']:
            depth_key = f'observation.{depth_name}'
            depth_rgb = depth_to_rgb_for_smolvla(
                obs[depth_name],
                target_size=(512, 512),
                depth_range=self.depth_range,
                device=self.device
            )
            observation[depth_key] = depth_rgb.unsqueeze(0)  # [1, 3, 512, 512]

        # å¤„ç†çŠ¶æ€ (å¡«å……åˆ°32ç»´)
        state_tensor = torch.from_numpy(obs['state']).float()
        state_padded = pad_tensor_to_target_dim(state_tensor, target_dim=32)
        observation['observation.state'] = state_padded.unsqueeze(0)  # [1, 32]

        # æ·»åŠ è¯­è¨€æŒ‡ä»¤
        observation['task'] = [self.language_instruction]

        return observation
```

### 10.5 ç»´åº¦é€‚é…å¤„ç†

#### 10.5.1 è‡ªåŠ¨ç»´åº¦å¡«å……

```python
def pad_tensor_to_target_dim(tensor, target_dim: int):
    """
    å°†tensorä»å®é™…ç»´åº¦å¡«å……åˆ°ç›®æ ‡ç»´åº¦

    Args:
        tensor: è¾“å…¥tensor [..., actual_dim]
        target_dim: ç›®æ ‡ç»´åº¦ (32)

    Returns:
        å¡«å……åçš„tensor [..., target_dim]
    """
    actual_dim = tensor.shape[-1]
    if actual_dim == target_dim:
        return tensor
    elif actual_dim < target_dim:
        # å¡«å……0åˆ°ç›®æ ‡ç»´åº¦
        pad_size = target_dim - actual_dim
        pad_shape = list(tensor.shape[:-1]) + [pad_size]
        pad_tensor = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)
        return torch.cat([tensor, pad_tensor], dim=-1)
    else:
        # è£å‰ªåˆ°ç›®æ ‡ç»´åº¦
        return tensor[..., :target_dim]

# ä½¿ç”¨ç¤ºä¾‹
# Kuavoå®é™…ç»´åº¦: 16ç»´
# SmolVLAé¢„è®­ç»ƒç»´åº¦: 32ç»´
# è‡ªåŠ¨å¡«å……ç­–ç•¥: å16ç»´å¡«0

state_16d = torch.randn(1, 16)  # KuavoçŠ¶æ€
state_32d = pad_tensor_to_target_dim(state_16d, target_dim=32)  # [1, 32]

action_32d = torch.randn(1, 50, 32)  # SmolVLAç”Ÿæˆçš„åŠ¨ä½œ
action_16d = action_32d[..., :16]  # è£å‰ªå›16ç»´ç”¨äºæ§åˆ¶
```

#### 10.5.2 å½’ä¸€åŒ–å¤„ç†

```python
# å¯¹äºå¡«å……éƒ¨åˆ†ä½¿ç”¨æ’ç­‰å½’ä¸€åŒ–
# mean = 0, std = 1 (å¡«å……éƒ¨åˆ†ä¸ä¼šè¢«æ”¹å˜)
normalization_mapping:
  STATE:
    value: MEAN_STD  # ä½¿ç”¨æ•°æ®é›†ç»Ÿè®¡
  ACTION:
    value: MEAN_STD  # ä½¿ç”¨æ•°æ®é›†ç»Ÿè®¡

# å½’ä¸€åŒ–é…ç½®
def normalize_state_action(data, stats):
    """å½’ä¸€åŒ–çŠ¶æ€å’ŒåŠ¨ä½œæ•°æ®"""
    # çŠ¶æ€å½’ä¸€åŒ– (å‰16ç»´ä½¿ç”¨ç»Ÿè®¡ï¼Œå16ç»´ä¿æŒ0)
    state_mean = torch.cat([stats['observation.state']['mean'], torch.zeros(16)])
    state_std = torch.cat([stats['observation.state']['std'], torch.ones(16)])

    # åŠ¨ä½œå½’ä¸€åŒ– (å‰16ç»´ä½¿ç”¨ç»Ÿè®¡ï¼Œå16ç»´ä¿æŒ0)
    action_mean = torch.cat([stats['action']['mean'], torch.zeros(16)])
    action_std = torch.cat([stats['action']['std'], torch.ones(16)])

    return normalized_data
```

### 10.6 Flow Matching vs Diffusion

#### 10.6.1 Flow Matching åŸç†

```python
# Flow Matching ä½¿ç”¨è¿ç»­æ—¶é—´æµ
# ç›¸æ¯” Diffusion çš„ç¦»æ•£æ—¶é—´æ­¥ï¼ŒFlow Matching æ›´å¹³æ»‘

class FlowMatching:
    """Flow Matching å®ç°"""

    def forward_flow(self, x0, x1, t):
        """
        å‰å‘æµ: x_t = (1-t) * x_0 + t * x_1

        Args:
            x0: åˆå§‹çŠ¶æ€ (çº¯å™ªå£°)
            x1: ç›®æ ‡çŠ¶æ€ (çœŸå®åŠ¨ä½œ)
            t: æ—¶é—´å‚æ•° [0, 1]
        """
        return (1 - t) * x0 + t * x1

    def compute_loss(self, batch):
        """è®¡ç®— Flow Matching æŸå¤±"""
        # 1. å‡†å¤‡å…¨å±€æ¡ä»¶
        global_cond = self._prepare_global_conditioning(batch)

        # 2. æå–çœŸå®åŠ¨ä½œ
        actions = batch['action']  # [B, chunk_size, 32]

        # 3. éšæœºé‡‡æ ·æ—¶é—´æ­¥
        t = torch.rand(actions.shape[0], device=actions.device)  # [B]

        # 4. é‡‡æ ·å™ªå£°
        noise = torch.randn_like(actions)  # [B, chunk_size, 32]

        # 5. è®¡ç®—æµçŠ¶æ€
        flow_state = self.forward_flow(noise, actions, t.unsqueeze(-1).unsqueeze(-1))

        # 6. é¢„æµ‹é€Ÿåº¦åœº
        velocity_pred = self.action_expert(flow_state, t, global_cond)

        # 7. è®¡ç®—æŸå¤± (é¢„æµ‹é€Ÿåº¦ vs çœŸå®é€Ÿåº¦)
        true_velocity = actions - noise
        loss = F.mse_loss(velocity_pred, true_velocity)

        return loss

    def sample(self, global_cond, num_steps=10):
        """Flow Matching é‡‡æ ·"""
        batch_size = global_cond.shape[0]

        # 1. ä»çº¯å™ªå£°å¼€å§‹
        x = torch.randn(batch_size, self.chunk_size, self.action_dim, device=global_cond.device)

        # 2. æ—¶é—´æ­¥
        timesteps = torch.linspace(0, 1, num_steps, device=global_cond.device)

        # 3. é€æ­¥å»å™ª
        for i in range(num_steps - 1):
            t = timesteps[i]
            dt = timesteps[i + 1] - timesteps[i]

            # é¢„æµ‹é€Ÿåº¦åœº
            velocity = self.action_expert(x, t.expand(batch_size), global_cond)

            # æ›´æ–°çŠ¶æ€
            x = x + velocity * dt

        return x
```

#### 10.6.2 Flow Matching vs Diffusion å¯¹æ¯”

| ç‰¹æ€§ | Flow Matching | Diffusion |
|------|---------------|-----------|
| **æ—¶é—´è¡¨ç¤º** | è¿ç»­æ—¶é—´ [0,1] | ç¦»æ•£æ—¶é—´æ­¥ [0,T] |
| **å‰å‘è¿‡ç¨‹** | çº¿æ€§æ’å€¼ | é«˜æ–¯å™ªå£°æ·»åŠ  |
| **è®­ç»ƒç›®æ ‡** | é€Ÿåº¦åœºé¢„æµ‹ | å™ªå£°é¢„æµ‹ |
| **é‡‡æ ·æ­¥æ•°** | 10æ­¥ | 100æ­¥ (DDPM) / 10æ­¥ (DDIM) |
| **æ”¶æ•›æ€§** | æ›´å¹³æ»‘ | å¯èƒ½ä¸ç¨³å®š |
| **è®¡ç®—æ•ˆç‡** | æ›´é«˜ | è¾ƒä½ |

### 10.7 SMOLVLA æ¨ç†æµç¨‹

#### 10.7.1 åœ¨çº¿æ¨ç†

```python
# åˆå§‹åŒ–
policy = SmolVLAPolicyWrapper.from_pretrained(checkpoint_path)
policy.eval()
policy.to(device)

# åˆ›å»ºæ·±åº¦èåˆå¤„ç†å™¨
fusion_processor = create_multi_camera_fusion(
    target_size=(512, 512),
    depth_range=(0, 1000),
    device=device,
    enable_depth=True
)

# æ¨ç†å¾ªç¯
for step in range(max_steps):
    # 1. è·å–è§‚æµ‹
    obs = env.get_observation()
    # obs = {
    #   'head_cam_h': [480, 640, 3],
    #   'depth_h': [480, 640, 1],
    #   'wrist_cam_l': [480, 640, 3],
    #   'depth_l': [480, 640, 1],
    #   'wrist_cam_r': [480, 640, 3],
    #   'depth_r': [480, 640, 1],
    #   'state': [16]
    # }

    # 2. æ·±åº¦èåˆå¤„ç†
    observation = fusion_processor.process_observations_simple(obs)
    # observation = {
    #   'observation.head_cam_h': [1, 3, 512, 512],
    #   'observation.depth_h': [1, 3, 512, 512] (ä¼ªå½©è‰²),
    #   'observation.wrist_cam_l': [1, 3, 512, 512],
    #   'observation.depth_l': [1, 3, 512, 512] (ä¼ªå½©è‰²),
    #   'observation.wrist_cam_r': [1, 3, 512, 512],
    #   'observation.depth_r': [1, 3, 512, 512] (ä¼ªå½©è‰²),
    #   'observation.state': [1, 32] (å¡«å……),
    #   'task': [language_instruction]
    # }

    # 3. é€‰æ‹©åŠ¨ä½œ
    with torch.no_grad():
        action = policy.select_action(observation)
    # action: [1, 16] (è£å‰ªå›16ç»´)

    # select_actionå†…éƒ¨:
    # - å¦‚æœaction queueä¸ºç©º:
    #   a. è°ƒç”¨Flow Matchingç”Ÿæˆchunk_size=50ä¸ªåŠ¨ä½œ
    #   b. å¡«å……action queue
    # - ä»action queue popç¬¬ä¸€ä¸ªåŠ¨ä½œ

    # 4. æ‰§è¡ŒåŠ¨ä½œ
    action = action.squeeze(0).cpu().numpy()  # [16]
    obs_next, reward, done, info = env.step(action)

    if done:
        obs = env.reset()
```

#### 10.7.2 Action Queue æœºåˆ¶

```python
# SMOLVLA Action Queue é…ç½®
chunk_size: 50        # Flow Matching ç”Ÿæˆ50æ­¥åŠ¨ä½œ
n_action_steps: 8     # æ¯æ¬¡æ‰§è¡Œ8æ­¥åŠ¨ä½œ

# Action Queue å·¥ä½œæµç¨‹
if len(self._queues[ACTION]) == 0:
    # ç”ŸæˆåŠ¨ä½œchunk
    actions = self.predict_action_chunk(observation)
    # actions: [1, chunk_size, 32] = [1, 50, 32]

    # è£å‰ªåˆ°16ç»´
    actions = actions[..., :16]  # [1, 50, 16]

    # è½¬ç½®å¹¶å¡«å…… (50ä¸ªåŠ¨ä½œ)
    self._queues[ACTION].extend(actions.transpose(0, 1))
    # action queue: [action[0], action[1], ..., action[49]]

# å–å‡ºä¸€ä¸ªåŠ¨ä½œ
action = self._queues[ACTION].popleft()
# action: [1, 16]

# æ¨ç†é¢‘ç‡åˆ†æ
# æ¯ n_action_steps = 8 æ­¥æ‰§è¡Œä¸€æ¬¡æ¨ç†
# å³æ¯ 0.8ç§’ æ¨ç†ä¸€æ¬¡ (10Hzæ§åˆ¶é¢‘ç‡)
# æ¯æ¬¡æ¨ç†ç”Ÿæˆ50æ­¥åŠ¨ä½œï¼Œæ‰§è¡Œå‰8æ­¥
# å‰©ä½™42æ­¥åœ¨ä¸‹æ¬¡è¿­ä»£ä½¿ç”¨
```

### 10.8 SMOLVLA é…ç½®ç³»ç»Ÿ

#### 10.8.1 åŸºç¡€é…ç½®

```yaml
# smolvla_sequential_base.yaml
policy:
  _target_: kuavo_train.wrapper.policy.smolvla.SmolVLAConfigWrapper

  # VLM Backboneé…ç½®
  vlm_model_name: 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'
  load_vlm_weights: True

  # å†»ç»“ç­–ç•¥
  freeze_vision_encoder: True  # å†»ç»“SigLIPè§†è§‰ç¼–ç å™¨
  train_expert_only: True     # åªè®­ç»ƒAction Expert
  train_state_proj: True       # è®­ç»ƒstateæŠ•å½±å±‚

  # åŠ¨ä½œç©ºé—´é…ç½®
  max_state_dim: 32    # é¢„è®­ç»ƒæ¨¡å‹ç»´åº¦
  max_action_dim: 32   # é¢„è®­ç»ƒæ¨¡å‹ç»´åº¦
  chunk_size: 50       # åŠ¨ä½œåºåˆ—é•¿åº¦
  n_action_steps: 8    # æ¯æ¬¡æ‰§è¡Œæ­¥æ•°

  # å›¾åƒé¢„å¤„ç†
  resize_imgs_with_padding: [512, 512]  # SmolVLAæ ‡å‡†è¾“å…¥å°ºå¯¸

  # æ·±åº¦ç›¸æœºæ”¯æŒ
  use_depth: True
  depth_features:
    - 'observation.depth_h'
    - 'observation.depth_l'
    - 'observation.depth_r'
  depth_resize_with_padding: [512, 512]
  depth_normalization_range: [0.0, 1000.0]

  # Flow Matchingé…ç½®
  num_steps: 10  # æ¨ç†æ—¶çš„å»å™ªæ­¥æ•°
```

#### 10.8.2 ä»»åŠ¡é…ç½®

```yaml
# task1_moving_grasp.yaml
task:
  id: 1
  name: 'moving_grasp'
  stage: 1

  language_instruction: 'Grasp the object from the moving conveyor belt using visual guidance. Place it precisely at the center of the first colored target block on the table, ensuring the object center aligns with the target center. Then grasp it again and place it precisely at the center of the second colored target block on the table, maintaining visual alignment throughout the placement.'

  data:
    root: '/root/robot/data/task-1/1-2000/lerobot/'
    repoid: 'lerobot/task1_moving_grasp'
    episodes_to_use: [0, 199]

  training:
    max_epoch: 100
    resume_from: 'pretrained'
    pretrained_path: 'lerobot/smolvla_base'
    policy:
      optimizer_lr: 5e-5
```

### 10.9 SMOLVLA vs Diffusion Policy å¯¹æ¯”

| ç‰¹æ€§ | SMOLVLA | Diffusion Policy |
|------|---------|------------------|
| **æ¨¡å‹å¤§å°** | 500M (è½»é‡çº§) | ~15M (è‡ªå®šä¹‰) |
| **é¢„è®­ç»ƒ** | HuggingFaceé¢„è®­ç»ƒ | ä»å¤´è®­ç»ƒ |
| **å¤šä»»åŠ¡** | é¡ºåºå­¦ä¹ 4ä¸ªä»»åŠ¡ | å•ä»»åŠ¡è®­ç»ƒ |
| **è¯­è¨€ç†è§£** | æ”¯æŒè‡ªç„¶è¯­è¨€æŒ‡ä»¤ | æ— è¯­è¨€ç†è§£ |
| **åŠ¨ä½œç”Ÿæˆ** | Flow Matching (10æ­¥) | Diffusion (10-100æ­¥) |
| **æ¨ç†é€Ÿåº¦** | æ›´å¿« | è¾ƒæ…¢ |
| **è®­ç»ƒå¤æ‚åº¦** | ä¸­ç­‰ (é˜²é—å¿˜) | ç®€å• |
| **æ‰©å±•æ€§** | æ˜“äºæ·»åŠ æ–°ä»»åŠ¡ | éœ€è¦é‡æ–°è®­ç»ƒ |
| **é²æ£’æ€§** | é«˜ (é¢„è®­ç»ƒ+å¤šä»»åŠ¡) | ä¸­ç­‰ |

---

## é™„å½•

### A. æ–‡ä»¶ç»“æ„

```
kuavo_data_challenge/
â”œâ”€â”€ kuavo_train/
â”‚   â”œâ”€â”€ train_policy.py  # è®­ç»ƒä¸»å…¥å£
â”‚   â”œâ”€â”€ wrapper/
â”‚   â”‚   â”œâ”€â”€ policy/
â”‚   â”‚   â”‚   â””â”€â”€ diffusion/
â”‚   â”‚   â”‚       â”œâ”€â”€ DiffusionPolicyWrapper.py       # PolicyåŒ…è£…å™¨
â”‚   â”‚   â”‚       â”œâ”€â”€ DiffusionModelWrapper.py        # æ¨¡å‹åŒ…è£…å™¨
â”‚   â”‚   â”‚       â”œâ”€â”€ DiffusionConfigWrapper.py       # é…ç½®åŒ…è£…å™¨
â”‚   â”‚   â”‚       â”œâ”€â”€ transformer_diffusion.py        # Transformerå®ç°
â”‚   â”‚   â”‚       â””â”€â”€ DiT_model.py                    # DiT (æœªä½¿ç”¨)
â”‚   â”‚   â””â”€â”€ dataset/
â”‚   â”‚       â””â”€â”€ LeRobotDatasetWrapper.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ augmenter.py
â”‚       â”œâ”€â”€ transforms.py
â”‚       â””â”€â”€ utils.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ policy/
â”‚       â””â”€â”€ diffusion_config.yaml
â””â”€â”€ kuavo_deploy/
    â””â”€â”€ examples/
        â””â”€â”€ eval/
            â””â”€â”€ eval_kuavo.py
```

### B. è®­ç»ƒå‘½ä»¤

```bash
# åŸºç¡€è®­ç»ƒ
python kuavo_train/train_policy.py \
  --config-name=diffusion_config

# æŒ‡å®šå‚æ•°
python kuavo_train/train_policy.py \
  --config-name=diffusion_config \
  training.batch_size=32 \
  training.max_epoch=1000 \
  policy.horizon=32

# æ¢å¤è®­ç»ƒ
python kuavo_train/train_policy.py \
  --config-name=diffusion_config \
  training.resume=True \
  training.resume_timestamp=run_20250110_123456
```

### C. æ¨ç†å‘½ä»¤

```bash
# ä½¿ç”¨Diffusion Policyæ¨ç†
python kuavo_deploy/examples/eval/eval_kuavo.py \
  --checkpoint path/to/best \
  --policy-type diffusion
```

### D. æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|---|---|---|
| æ€»å‚æ•°é‡ | ~15M | åŒ…å«æ‰€æœ‰ç¼–ç å™¨å’ŒTransformer |
| è®­ç»ƒbatch size | 64 | å¯æ ¹æ®GPUè°ƒæ•´ |
| è®­ç»ƒepochs | 500 | é€šå¸¸300-500 epochæ”¶æ•› |
| Horizon | 16 | ç”Ÿæˆ16æ­¥åŠ¨ä½œ |
| n_action_steps | 8 | æ‰§è¡Œå‰8æ­¥ |
| æ¨ç†æ—¶é—´ (DDPM, 100æ­¥) | ~500ms | å¤ªæ…¢ |
| æ¨ç†æ—¶é—´ (DDIM, 10æ­¥) | ~50ms | å®æ—¶ âœ… |
| æ§åˆ¶é¢‘ç‡ | 10Hz | æœºå™¨äººæ§åˆ¶ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-10-10
**ä½œè€…**: AI Assistant

