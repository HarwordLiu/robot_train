# åˆ†å±‚æ¶æ„ - æ•°æ®æµä¸å®æˆ˜æŒ‡å—

> è¯¦ç»†è¯´æ˜æ•°æ®å¦‚ä½•åœ¨ç³»ç»Ÿä¸­æµåŠ¨ï¼Œä»¥åŠå¦‚ä½•å®é™…ä½¿ç”¨è¯¥æ¶æ„

---

## ç›®å½•

1. [å®Œæ•´æ•°æ®æµè¯¦è§£](#1-å®Œæ•´æ•°æ®æµè¯¦è§£)
2. [è®­ç»ƒæ•°æ®æµ](#2-è®­ç»ƒæ•°æ®æµ)
3. [æ¨ç†æ•°æ®æµ](#3-æ¨ç†æ•°æ®æµ)
4. [å®æˆ˜ç¤ºä¾‹](#4-å®æˆ˜ç¤ºä¾‹)
5. [è°ƒè¯•æŠ€å·§](#5-è°ƒè¯•æŠ€å·§)
6. [æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
7. [å¸¸è§é—®é¢˜](#7-å¸¸è§é—®é¢˜)

---

## 1. å®Œæ•´æ•°æ®æµè¯¦è§£

### 1.1 æ•°æ®æµæ¦‚è§ˆå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         åŸå§‹æ•°æ®æº                               â”‚
â”‚  LeRobot Dataset: /robot/data/task1/lerobot/                   â”‚
â”‚    â”œâ”€ episode_0000.parquet                                     â”‚
â”‚    â”œâ”€ episode_0001.parquet                                     â”‚
â”‚    â””â”€ ...                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Dataset åŠ è½½                                â”‚
â”‚  LeRobotDataset.__getitem__() è¿”å›:                             â”‚
â”‚    {                                                            â”‚
â”‚      'observation.state': [2, 16],          # [seq_len, dim]   â”‚
â”‚      'observation.images.head_cam_h': [2, 3, 480, 640],        â”‚
â”‚      'observation.images.wrist_cam_l': [2, 3, 480, 640],       â”‚
â”‚      'observation.images.wrist_cam_r': [2, 3, 480, 640],       â”‚
â”‚      'observation.depth.depth_h': [2, 1, 480, 640],            â”‚
â”‚      'observation.depth.depth_l': [2, 1, 480, 640],            â”‚
â”‚      'observation.depth.depth_r': [2, 1, 480, 640],            â”‚
â”‚      'action': [16, 16]                     # [horizon, dim]   â”‚
â”‚    }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DataLoader æ‰¹å¤„ç†                           â”‚
â”‚  Batch (batch_size=64):                                         â”‚
â”‚    {                                                            â”‚
â”‚      'observation.state': [64, 2, 16],                         â”‚
â”‚      'observation.images.head_cam_h': [64, 2, 3, 480, 640],    â”‚
â”‚      ... (å…¶ä»–ç›¸æœº)                                              â”‚
â”‚      'action': [64, 16, 16]                                    â”‚
â”‚    }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å›¾åƒé¢„å¤„ç† (Policy._preprocess_batch)           â”‚
â”‚  1. è£å‰ª: crop_image(target_range=[420, 560])                  â”‚
â”‚  2. ç¼©æ”¾: resize_image(target_size=[210, 280])                 â”‚
â”‚  3. å †å : torch.stack([...], dim=-4)                           â”‚
â”‚                                                                 â”‚
â”‚  è¾“å‡º:                                                          â”‚
â”‚    batch[OBS_IMAGES]: [64, 2, 6, 210, 280]  # 6ç›¸æœº (3RGB+3D)  â”‚
â”‚    batch[OBS_DEPTH]: [64, 2, 3, 210, 280]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å½’ä¸€åŒ– (normalize_inputs/targets)             â”‚
â”‚  observation.state: (x - mean) / std                           â”‚
â”‚  observation.images: (x - mean) / std                          â”‚
â”‚  action: (x - mean) / std                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HierarchicalScheduler åˆ†å±‚å¤„ç†                      â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Layer 1: SafetyReflexLayer                          â”‚       â”‚
â”‚  â”‚   è¾“å…¥: observation.state [64, 2, 16]               â”‚       â”‚
â”‚  â”‚   è¾“å‡º: {                                           â”‚       â”‚
â”‚  â”‚     'emergency': [64] bool,                         â”‚       â”‚
â”‚  â”‚     'balance_action': [64, 16],                     â”‚       â”‚
â”‚  â”‚     'action': [64, 16]                              â”‚       â”‚
â”‚  â”‚   }                                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼ (æ›´æ–°context)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Layer 2: GaitControlLayer                           â”‚       â”‚
â”‚  â”‚   è¾“å…¥: observation.state [64, 2, 16] + context     â”‚       â”‚
â”‚  â”‚   è¾“å‡º: {                                           â”‚       â”‚
â”‚  â”‚     'gait_features': [64, 2, 128],                  â”‚       â”‚
â”‚  â”‚     'action': [64, 16]                              â”‚       â”‚
â”‚  â”‚   }                                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼ (æ›´æ–°context)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Layer 3: ManipulationLayer                          â”‚       â”‚
â”‚  â”‚   è¾“å…¥: observation.* [64, ...] + context           â”‚       â”‚
â”‚  â”‚   å¤„ç†: å¤šæ¨¡æ€ç‰¹å¾èåˆ + Transformer                 â”‚       â”‚
â”‚  â”‚   è¾“å‡º: {                                           â”‚       â”‚
â”‚  â”‚     'manipulation_features': [64, 1, 512],          â”‚       â”‚
â”‚  â”‚     'action': [64, 16]                              â”‚       â”‚
â”‚  â”‚   }                                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼ (æ›´æ–°context)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Layer 4: GlobalPlanningLayer (å¯é€‰æ¿€æ´»)             â”‚       â”‚
â”‚  â”‚   è¾“å…¥: observation.* [64, ...] + context           â”‚       â”‚
â”‚  â”‚   å¤„ç†: é•¿æœŸè®°å¿† + ä»»åŠ¡åˆ†è§£                         â”‚       â”‚
â”‚  â”‚   è¾“å‡º: {                                           â”‚       â”‚
â”‚  â”‚     'global_plan': [64, 64],                        â”‚       â”‚
â”‚  â”‚     'action': [64, 16]                              â”‚       â”‚
â”‚  â”‚   }                                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                 â”‚
â”‚  æ±‡æ€»è¾“å‡º: layer_outputs = {                                    â”‚
â”‚    'safety': {...},                                             â”‚
â”‚    'gait': {...},                                               â”‚
â”‚    'manipulation': {...},                                       â”‚
â”‚    'planning': {...}                                            â”‚
â”‚  }                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            HierarchicalDiffusionModel æŸå¤±è®¡ç®—                   â”‚
â”‚  diffusion_loss = compute_loss(batch)                          â”‚
â”‚  # Diffusionæ¨¡å‹çš„æ ‡å‡†æŸå¤±ï¼ˆä¸èåˆlayer_outputsï¼‰               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              åˆ†å±‚æŸå¤±èšåˆ (_aggregate_hierarchical_loss)         â”‚
â”‚                                                                 â”‚
â”‚  total_loss = diffusion_loss                                   â”‚
â”‚                                                                 â”‚
â”‚  for layer_name in enabled_layers:                             â”‚
â”‚    if 'loss' in layer_outputs[layer_name]:                     â”‚
â”‚      layer_weight = task_layer_weights[layer_name]             â”‚
â”‚      layer_loss = layer_outputs[layer_name]['loss']            â”‚
â”‚      total_loss += layer_weight * layer_loss                   â”‚
â”‚                                                                 â”‚
â”‚  ä¾‹å¦‚:                                                          â”‚
â”‚    total_loss = diffusion_loss                                 â”‚
â”‚                + 2.0 * safety_loss                             â”‚
â”‚                + 1.0 * gait_loss                               â”‚
â”‚                + 2.0 * manipulation_loss                       â”‚
â”‚                + 0.5 * planning_loss                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åå‘ä¼ æ’­ & ä¼˜åŒ–å™¨æ­¥éª¤                         â”‚
â”‚  scaled_loss = total_loss / accumulation_steps                 â”‚
â”‚  scaler.scale(scaled_loss).backward()                          â”‚
â”‚  scaler.step(optimizer)                                        â”‚
â”‚  lr_scheduler.step()                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å¼ é‡ç»´åº¦è¿½è¸ª

#### è®­ç»ƒæ—¶ (batch_size=64)

| é˜¶æ®µ | Key | Shape | è¯´æ˜ |
|---|---|---|---|
| Dataset | observation.state | [2, 16] | 2ä¸ªæ—¶é—´æ­¥ï¼Œ16ç»´çŠ¶æ€ |
| DataLoader | observation.state | [64, 2, 16] | æ‰¹å¤„ç† |
| Dataset | observation.images.head_cam_h | [2, 3, 480, 640] | 2æ­¥ï¼ŒRGB |
| DataLoader | observation.images.head_cam_h | [64, 2, 3, 480, 640] | æ‰¹å¤„ç† |
| Preprocess | observation.images.head_cam_h | [64, 2, 3, 210, 280] | è£å‰ªç¼©æ”¾ |
| Preprocess | OBS_IMAGES | [64, 2, 6, 210, 280] | å †å 6ç›¸æœº |
| SafetyLayer | robot_state (input) | [64, 2, 16] | è¾“å…¥ |
| SafetyLayer | gru_output | [64, 2, 64] | GRUè¾“å‡º |
| SafetyLayer | action (output) | [64, 16] | åŠ¨ä½œè¾“å‡º |
| ManipulationLayer | state_features | [64, 1, 16] | çŠ¶æ€ |
| ManipulationLayer | combined_visual | [64, 1, 1280] | è§†è§‰ |
| ManipulationLayer | projected_features | [64, 1, 512] | æŠ•å½± |
| ManipulationLayer | manipulation_features | [64, 1, 512] | Transformerè¾“å‡º |
| ManipulationLayer | action (output) | [64, 16] | åŠ¨ä½œè¾“å‡º |

#### æ¨ç†æ—¶ (batch_size=1)

| é˜¶æ®µ | Key | Shape | è¯´æ˜ |
|---|---|---|---|
| Env | observation.state | [16] | å•ä¸ªæ—¶é—´æ­¥ |
| Policy | observation.state | [1, 1, 16] | æ·»åŠ batchå’Œseqç»´åº¦ |
| SafetyLayer | action | [1, 16] | å•æ ·æœ¬è¾“å‡º |
| Final | action | [16] | ç§»é™¤batchç»´åº¦ |

---

## 2. è®­ç»ƒæ•°æ®æµ

### 2.1 å•ä¸ªè®­ç»ƒæ­¥éª¤

```python
# ä¼ªä»£ç ï¼Œå±•ç¤ºå®Œæ•´æµç¨‹
for epoch in range(max_epoch):
    for batch in dataloader:
        # 1. æ•°æ®ç§»åˆ°GPU
        batch = {k: v.to(device) for k, v in batch.items()}
        # batch['observation.state']: [64, 2, 16]
        # batch['action']: [64, 16, 16]

        # 2. å‰å‘ä¼ æ’­
        with autocast(amp_enabled):
            loss, hierarchical_info = policy.forward(batch, curriculum_info)

        # å†…éƒ¨æµç¨‹:
        #   a. _preprocess_batch(batch)
        #      - crop_image, resize_image
        #      - å †å å›¾åƒ -> OBS_IMAGES

        #   b. normalize_inputs(batch)
        #      - å½’ä¸€åŒ–è§‚æµ‹

        #   c. normalize_targets(batch)
        #      - å½’ä¸€åŒ–åŠ¨ä½œ

        #   d. _identify_task(batch, curriculum_info)
        #      - æ¨æ–­ä»»åŠ¡ç±»å‹

        #   e. scheduler(batch, task_info)
        #      - æŒ‰ä¼˜å…ˆçº§è°ƒç”¨å„å±‚
        #      - è¿”å› layer_outputs

        #   f. diffusion.compute_loss(batch, layer_outputs)
        #      - è®¡ç®—DiffusionæŸå¤±

        #   g. _aggregate_hierarchical_loss(diffusion_loss, layer_outputs)
        #      - èšåˆæ‰€æœ‰å±‚çš„æŸå¤±

        # 3. åå‘ä¼ æ’­
        scaled_loss = loss / accumulation_steps
        scaler.scale(scaled_loss).backward()

        # 4. ä¼˜åŒ–å™¨æ­¥éª¤
        if steps % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            lr_scheduler.step()

        # 5. è®°å½•æ—¥å¿—
        if steps % log_freq == 0:
            writer.add_scalar("train/loss", scaled_loss.item(), steps)
            # è®°å½•åˆ†å±‚ä¿¡æ¯
            for key, value in hierarchical_info.items():
                writer.add_scalar(f"hierarchical/{key}", value, steps)
```

### 2.2 è¯¾ç¨‹å­¦ä¹ æ•°æ®æµ

```python
# é˜¶æ®µ1: åªè®­ç»ƒ manipulation å±‚
curriculum_info = {
    'stage': 'manipulation_first',
    'enabled_layers': ['manipulation']
}

# Policyå†…éƒ¨:
self.enabled_layers = ['manipulation']

# æŸå¤±èšåˆ:
total_loss = diffusion_loss + 2.0 * manipulation_loss
# safety_loss, gait_loss, planning_loss ä¸è®¡ç®—

# --------------------

# é˜¶æ®µ2: è®­ç»ƒ manipulation + safety
curriculum_info = {
    'stage': 'manipulation_safety',
    'enabled_layers': ['manipulation', 'safety']
}

self.enabled_layers = ['manipulation', 'safety']

total_loss = diffusion_loss + 2.0 * manipulation_loss + 2.0 * safety_loss
# gait_loss, planning_loss ä¸è®¡ç®—
```

### 2.3 ä»»åŠ¡ç‰¹å®šè®­ç»ƒæ•°æ®æµ

```python
# å¤šä»»åŠ¡DataLoader
datasets = {
    1: task1_dataset,  # 300 episodes
    2: task2_dataset,  # 200 episodes
}

# åŠ æƒé‡‡æ ·
task_weights = {1: 0.6, 2: 0.4}  # åŸºäºå¤æ‚åº¦å’Œæ•°æ®é‡
sample_weights = [0.6] * 300 + [0.4] * 200
sampler = WeightedRandomSampler(sample_weights, ...)

dataloader = DataLoader(combined_dataset, sampler=sampler, ...)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    # batchä¸­å¯èƒ½åŒ…å«æ¥è‡ªä¸åŒä»»åŠ¡çš„æ ·æœ¬
    # æ ¹æ®task_idè·å–ç‰¹å®šæƒé‡
    task_loss_weights = task_manager.get_task_specific_loss_weights(batch)

    loss, info = policy.forward(batch, task_weights=task_loss_weights)
```

---

## 3. æ¨ç†æ•°æ®æµ

### 3.1 åœ¨çº¿æ¨ç† (Real-time)

```python
# åˆå§‹åŒ–
policy = HumanoidDiffusionPolicy.from_pretrained(checkpoint_path)
policy.eval()
policy.to(device)
policy.reset()  # æ¸…ç©ºobservation queue

# æ¨ç†å¾ªç¯
for step in range(max_steps):
    # 1. è·å–è§‚æµ‹
    obs = env.get_observation()
    # obs = {
    #   'observation.state': [16],  # æ³¨æ„ï¼šæ²¡æœ‰batchç»´åº¦
    #   'observation.images.head_cam_h': [3, 480, 640],
    #   ...
    # }

    # 2. ç§»åˆ°GPUå¹¶æ·»åŠ batchç»´åº¦
    obs = {k: torch.from_numpy(v).unsqueeze(0).to(device)
           for k, v in obs.items()}
    # obs = {
    #   'observation.state': [1, 16],
    #   'observation.images.head_cam_h': [1, 3, 480, 640],
    #   ...
    # }

    # 3. é€‰æ‹©åŠ¨ä½œ
    with torch.no_grad():
        action = policy.select_action(obs)
    # action: [1, 16]

    # 4. ç§»é™¤batchç»´åº¦ï¼Œç§»åˆ°CPU
    action = action.squeeze(0).cpu().numpy()
    # action: [16]

    # 5. åå½’ä¸€åŒ–
    action = denormalize_action(action, dataset_stats)

    # 6. æ‰§è¡ŒåŠ¨ä½œ
    obs_next, reward, done, info = env.step(action)
```

### 3.2 æ¨ç†æ¨¡å¼ (Inference Mode with Latency Budget)

```python
policy = HumanoidDiffusionPolicy.from_pretrained(checkpoint_path)
policy.eval()

for step in range(max_steps):
    obs = env.get_observation()
    obs = preprocess_obs(obs)

    # è®¾ç½®å»¶è¿Ÿé¢„ç®—
    task_info = {
        'latency_budget_ms': 50.0,  # 50msé¢„ç®—
        'requires_locomotion': True,
        'requires_manipulation': True,
        'task_complexity': 'medium'
    }

    with torch.no_grad():
        # HierarchicalSchedulerä¼šæ ¹æ®é¢„ç®—è‡ªé€‚åº”æ¿€æ´»å±‚
        layer_outputs = policy.scheduler.inference_mode(
            obs, task_info, latency_budget_ms=50.0
        )

    # ä»å±‚è¾“å‡ºæå–åŠ¨ä½œ
    action = policy._extract_action_from_layers(layer_outputs, obs)

    # æ£€æŸ¥æ˜¯å¦åœ¨é¢„ç®—å†…
    inference_stats = layer_outputs['_inference_stats']
    if not inference_stats['within_budget']:
        print(f"âš ï¸  è¶…å‡ºé¢„ç®—: {inference_stats['total_time_ms']:.1f}ms")

    env.step(action)
```

### 3.3 ç¦»çº¿è¯„ä¼°

```python
# ç¦»çº¿è¯„ä¼°ï¼šåœ¨ä¿å­˜çš„episodeä¸Šè¯„ä¼°
from lerobot.datasets.lerobot_dataset import LeRobotDataset

dataset = LeRobotDataset(repoid, root=data_path)
policy = HumanoidDiffusionPolicy.from_pretrained(checkpoint_path)
policy.eval()

total_mse = 0.0
num_samples = 0

for i in range(len(dataset)):
    sample = dataset[i]

    # æå–è§‚æµ‹å’ŒçœŸå®åŠ¨ä½œ
    obs = {k: v for k, v in sample.items() if k.startswith('observation.')}
    true_action = sample['action'][0]  # ç¬¬ä¸€ä¸ªåŠ¨ä½œ

    # é¢„æµ‹åŠ¨ä½œ
    with torch.no_grad():
        pred_action = policy.select_action(obs)

    # è®¡ç®—è¯¯å·®
    mse = torch.mean((pred_action - true_action) ** 2)
    total_mse += mse.item()
    num_samples += 1

avg_mse = total_mse / num_samples
print(f"å¹³å‡MSE: {avg_mse:.4f}")
```

---

## 4. å®æˆ˜ç¤ºä¾‹

### 4.1 ä»é›¶å¼€å§‹è®­ç»ƒ

```bash
# 1. å‡†å¤‡æ•°æ®
# ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®ä½ç½®: /robot/data/task1/lerobot/

# 2. æ£€æŸ¥é…ç½®
# configs/policy/humanoid_diffusion_config.yaml
# - root: æ•°æ®è·¯å¾„
# - use_hierarchical: True
# - curriculum_learning.enable: True

# 3. å¼€å§‹è®­ç»ƒ
python kuavo_train/train_hierarchical_policy.py \
  --config-name=humanoid_diffusion_config

# 4. ç›‘æ§è®­ç»ƒ
tensorboard --logdir outputs/train/task_400_episodes/humanoid_hierarchical/

# 5. æ£€æŸ¥æ—¥å¿—
tail -f hierarchical_training.log
```

### 4.2 è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ

```python
# é…ç½®æ–‡ä»¶ä¸­å®šä¹‰è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
curriculum_learning:
  enable: True
  universal_stages:
    stage1:
      name: 'manipulation_first'
      layers: ['manipulation']
      epochs: 8
    stage2:
      name: 'manipulation_safety'
      layers: ['manipulation', 'safety']
      epochs: 7
    # ...

# è®­ç»ƒæ—¶è‡ªåŠ¨æ‰§è¡Œè¯¾ç¨‹å­¦ä¹ 
# è¾“å‡ºæ—¥å¿—:
# ğŸ“ å¼€å§‹è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: stage1_manipulation_first
#    æ¿€æ´»å±‚: ['manipulation']
#    è®­ç»ƒè½®æ¬¡: 8
# ...
# âœ… è¯¾ç¨‹é˜¶æ®µ stage1_manipulation_first å®Œæˆï¼Œæœ€ä½³æŸå¤±: 0.0523
```

### 4.3 æ¢å¤è®­ç»ƒ

```bash
# 1. æ‰¾åˆ°ä¹‹å‰çš„æ£€æŸ¥ç‚¹
ls outputs/train/task_400_episodes/humanoid_hierarchical/

# 2. ä¿®æ”¹é…ç½®
training:
  resume: True
  resume_timestamp: "run_20250110_123456"

# 3. æ¢å¤è®­ç»ƒ
python kuavo_train/train_hierarchical_policy.py \
  --config-name=humanoid_diffusion_config

# è¾“å‡ºæ—¥å¿—:
# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: outputs/.../run_20250110_123456
# âœ… Policy ä¿å­˜æˆåŠŸ
# å·²æ¢å¤è®­ç»ƒä»epoch 150, step 12000
```

### 4.4 éƒ¨ç½²åˆ°çœŸå®æœºå™¨äºº

```python
# eval_kuavo.py
from kuavo_train.wrapper.policy.humanoid.HumanoidDiffusionPolicy import HumanoidDiffusionPolicy

# 1. åŠ è½½æ¨¡å‹
checkpoint_path = "outputs/train/.../best"
policy = HumanoidDiffusionPolicy.from_pretrained(
    checkpoint_path,
    use_hierarchical=True,
    hierarchical=hierarchical_config  # éœ€è¦æä¾›hierarchicalé…ç½®
)
policy.eval()
policy.to('cuda')

# 2. åˆå§‹åŒ–ç¯å¢ƒ
from kuavo_deploy.kuavo_env.kuavo_real_env import KuavoRealEnv
env = KuavoRealEnv(config)

# 3. æ¨ç†å¾ªç¯
obs, info = env.reset()
policy.reset()

for step in range(1000):
    # é€‰æ‹©åŠ¨ä½œ
    action = policy.select_action(obs)

    # æ‰§è¡ŒåŠ¨ä½œ
    obs, reward, terminated, truncated, info = env.step(action.numpy())

    # è®°å½•å±‚è¾“å‡ºï¼ˆå¯é€‰ï¼‰
    layer_outputs = policy.get_last_layer_outputs()
    if layer_outputs and 'safety' in layer_outputs:
        if layer_outputs['safety']['emergency']:
            print("ğŸš¨ æ£€æµ‹åˆ°ç´§æ€¥æƒ…å†µï¼")
            break

    if terminated or truncated:
        break

env.close()
```

---

## 5. è°ƒè¯•æŠ€å·§

### 5.1 æŸ¥çœ‹å±‚æ¿€æ´»æƒ…å†µ

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
loss, hierarchical_info = policy.forward(batch, curriculum_info)

# hierarchical_info åŒ…å«:
# {
#   'curriculum_stage': 'stage2_manipulation_safety',
#   'enabled_layers': ['manipulation', 'safety'],
#   'task_weights': {'safety': 2.0, 'manipulation': 2.0, ...},
#   'layer_performance': {
#     'safety': {'loss': 0.05, 'execution_time': 8.2, 'active': True},
#     'manipulation': {'loss': 0.08, 'execution_time': 95.3, 'active': True},
#     'gait': {'active': False},
#     'planning': {'active': False}
#   }
# }

# æ‰“å°æ¿€æ´»å±‚
active_layers = [k for k, v in hierarchical_info['layer_performance'].items()
                 if v.get('active', False)]
print(f"æ¿€æ´»å±‚: {active_layers}")
```

### 5.2 å¯è§†åŒ–æŸå¤±åˆ†è§£

```python
# åœ¨TensorBoardä¸­æŸ¥çœ‹
writer.add_scalar("hierarchical/total_loss", total_loss.item(), steps)
writer.add_scalar("hierarchical/diffusion_loss", diffusion_loss.item(), steps)

for layer_name, layer_output in layer_outputs.items():
    if 'loss' in layer_output:
        writer.add_scalar(f"hierarchical/{layer_name}_loss",
                         layer_output['loss'].item(), steps)
```

### 5.3 æ£€æŸ¥æ•°æ®ç»´åº¦

```python
def check_batch_shapes(batch, stage=""):
    """æ£€æŸ¥batchä¸­æ‰€æœ‰å¼ é‡çš„ç»´åº¦"""
    print(f"\n{'='*60}")
    print(f"Batch shapes at stage: {stage}")
    print(f"{'='*60}")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"{key:40s}: {list(value.shape)}")
    print(f"{'='*60}\n")

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
check_batch_shapes(batch, "After DataLoader")
batch = policy._preprocess_batch(batch)
check_batch_shapes(batch, "After Preprocess")
```

### 5.4 å•æ­¥è°ƒè¯•

```python
# æµ‹è¯•å•ä¸ªå±‚
layer = policy.scheduler.layers['safety']
layer.eval()

# æ„é€ æµ‹è¯•è¾“å…¥
test_input = {
    'observation.state': torch.randn(1, 1, 16).to(device)
}

# å‰å‘ä¼ æ’­
with torch.no_grad():
    output = layer.forward(test_input)

print(f"Emergency: {output['emergency']}")
print(f"Balance action: {output['balance_action'].shape}")
```

### 5.5 æ€§èƒ½åˆ†æ

```python
# ä½¿ç”¨PyTorch Profiler
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True
) as prof:
    for i, batch in enumerate(dataloader):
        if i >= 10:  # åªåˆ†æ10ä¸ªbatch
            break
        loss, info = policy.forward(batch)
        loss.backward()

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# å¯¼å‡ºChrome trace
prof.export_chrome_trace("trace.json")
# åœ¨ chrome://tracing ä¸­æŸ¥çœ‹
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 å†…å­˜ä¼˜åŒ–

#### æ¢¯åº¦ç´¯ç§¯

```python
# åœ¨é…ç½®ä¸­è®¾ç½®
training:
  batch_size: 32            # å‡å°å®é™…batch size
  accumulation_steps: 2     # ç´¯ç§¯2æ­¥ï¼Œç­‰æ•ˆbatch_size=64

# è®­ç»ƒå¾ªç¯ä¼šè‡ªåŠ¨å¤„ç†
scaled_loss = loss / accumulation_steps
scaled_loss.backward()

if steps % accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()
```

#### æ··åˆç²¾åº¦è®­ç»ƒ

```python
# åœ¨é…ç½®ä¸­å¯ç”¨
policy:
  use_amp: True

# è‡ªåŠ¨ä½¿ç”¨AMP
scaler = torch.amp.GradScaler(enabled=True)

with autocast(amp_enabled=True):
    loss, info = policy.forward(batch)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

#### å†…å­˜é‡Šæ”¾

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­å®šæœŸæ¸…ç†
if steps % 100 == 0:
    torch.cuda.empty_cache()
```

### 6.2 é€Ÿåº¦ä¼˜åŒ–

#### DataLoaderä¼˜åŒ–

```python
dataloader = DataLoader(
    dataset,
    batch_size=64,
    num_workers=25,           # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True,          # å›ºå®šå†…å­˜ï¼ŒåŠ é€Ÿä¼ è¾“
    prefetch_factor=2,        # é¢„å–2ä¸ªbatch
    persistent_workers=True   # ä¿æŒworkerè¿›ç¨‹
)
```

#### ç¼–è¯‘æ¨¡å‹ (PyTorch 2.0+)

```python
# ç¼–è¯‘Policyæ¨¡å‹
policy = torch.compile(policy, mode="reduce-overhead")

# æˆ–è€…åªç¼–è¯‘ç‰¹å®šå±‚
policy.scheduler.layers['manipulation'] = torch.compile(
    policy.scheduler.layers['manipulation']
)
```

#### å‡å°‘ä¸å¿…è¦çš„è®¡ç®—

```python
# æ¨ç†æ—¶å…³é—­ä¸å¿…è¦çš„å±‚
policy.scheduler.set_layer_enabled('planning', False)

# æˆ–è€…ä½¿ç”¨inference_mode
layer_outputs = policy.scheduler.inference_mode(
    batch, task_info, latency_budget_ms=50.0
)
```

### 6.3 åˆ†å¸ƒå¼è®­ç»ƒ

```python
# ä½¿ç”¨PyTorch DDP
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼
dist.init_process_group(backend='nccl')
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)

# åŒ…è£…æ¨¡å‹
policy = DDP(policy.to(local_rank), device_ids=[local_rank])

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    batch = {k: v.to(local_rank) for k, v in batch.items()}
    loss, info = policy(batch)
    loss.backward()
    optimizer.step()
```

---

## 7. å¸¸è§é—®é¢˜

### 7.1 ç»´åº¦ä¸åŒ¹é…

**é—®é¢˜**: `RuntimeError: size mismatch`

**åŸå› **: è¾“å…¥ç»´åº¦ä¸å±‚æœŸæœ›çš„ä¸ä¸€è‡´

**è§£å†³**:
```python
# æ£€æŸ¥é…ç½®
# humanoid_diffusion_config.yaml
hierarchical:
  layers:
    safety:
      input_dim: 16   # ç¡®ä¿ä¸å®é™…çŠ¶æ€ç»´åº¦ä¸€è‡´
      output_dim: 16

# å¦‚æœæœºå™¨äººé…ç½®æ”¹å˜ï¼ˆå¦‚å¯ç”¨åŒè…¿ï¼‰ï¼Œéœ€è¦æ›´æ–°input_dim
```

### 7.2 OOM (Out of Memory)

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³**:
1. å‡å°batch_size
2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯
3. å¯ç”¨AMP
4. å‡å°‘å›¾åƒåˆ†è¾¨ç‡
5. ç¦ç”¨ä¸å¿…è¦çš„å±‚

```python
# é…ç½®è°ƒæ•´
training:
  batch_size: 32             # ä»64å‡å°åˆ°32
  accumulation_steps: 2      # ç­‰æ•ˆbatch_size=64

policy:
  use_amp: True              # å¯ç”¨æ··åˆç²¾åº¦

  custom:
    resize_shape: [180, 240]  # ä»[210, 280]å‡å°
```

### 7.3 è®­ç»ƒä¸æ”¶æ•›

**é—®é¢˜**: Lossä¸ä¸‹é™æˆ–å‘æ•£

**å¯èƒ½åŸå› åŠè§£å†³**:

1. **å­¦ä¹ ç‡è¿‡é«˜**
```python
policy:
  optimizer_lr: 0.00005  # ä»0.0001å‡å°
```

2. **å±‚æƒé‡ä¸å¹³è¡¡**
```python
hierarchical:
  layer_weights:
    safety: 1.0      # ä»2.0å‡å°
    manipulation: 1.0
    # é¿å…æŸå±‚æƒé‡è¿‡å¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®š
```

3. **è¯¾ç¨‹å­¦ä¹ é˜¶æ®µepochsä¸è¶³**
```python
universal_stages:
  stage1:
    epochs: 15  # ä»8å¢åŠ åˆ°15
```

4. **æ•°æ®å½’ä¸€åŒ–é—®é¢˜**
```python
# æ£€æŸ¥dataset_statsæ˜¯å¦æ­£ç¡®
print(dataset_stats['observation.state']['mean'])
print(dataset_stats['observation.state']['std'])
```

### 7.4 æ¨ç†é€Ÿåº¦æ…¢

**é—®é¢˜**: æ¨ç†å»¶è¿Ÿè¶…è¿‡é¢„ç®—

**è§£å†³**:

1. **ä½¿ç”¨æ¨ç†æ¨¡å¼**
```python
layer_outputs = policy.scheduler.inference_mode(
    batch, task_info, latency_budget_ms=50.0
)
```

2. **ç¦ç”¨å¤æ‚å±‚**
```python
policy.scheduler.set_layer_enabled('planning', False)
```

3. **å‡å°æ¨¡å‹å¤§å°**
```python
# ä½¿ç”¨æ›´å°çš„é…ç½®
hierarchical:
  layers:
    manipulation:
      hidden_size: 256  # ä»512å‡å°
      layers: 2         # ä»3å‡å°
```

4. **ä½¿ç”¨TensorRTä¼˜åŒ–**
```python
import torch_tensorrt

# ç¼–è¯‘ä¸ºTensorRT
trt_model = torch_tensorrt.compile(
    policy,
    inputs=[torch_tensorrt.Input((1, 1, 16))],
    enabled_precisions={torch.float16}
)
```

### 7.5 å±‚ä¸æ¿€æ´»

**é—®é¢˜**: æŸäº›å±‚ä»ä¸æ¿€æ´»

**æ£€æŸ¥**:
```python
# æŸ¥çœ‹should_activateé€»è¾‘
layer = policy.scheduler.layers['planning']
should_activate = layer.should_activate(batch, context)
print(f"Planning layer should activate: {should_activate}")

# æ£€æŸ¥context
print(f"Task complexity: {context.get('task_complexity')}")
# planningå±‚åªåœ¨task_complexity='high'æˆ–'very_high'æ—¶æ¿€æ´»

# è§£å†³ï¼šåœ¨task_infoä¸­è®¾ç½®æ­£ç¡®çš„å¤æ‚åº¦
task_info = {
    'task_complexity': 'high'  # æ”¹ä¸º'high'
}
```

### 7.6 è¯¾ç¨‹å­¦ä¹ è·³è¿‡

**é—®é¢˜**: è¯¾ç¨‹å­¦ä¹ é˜¶æ®µè¢«è·³è¿‡

**æ£€æŸ¥**:
```python
# 1. ç¡®è®¤é…ç½®å¯ç”¨
curriculum_learning:
  enable: True  # å¿…é¡»ä¸ºTrue

# 2. æ£€æŸ¥stagesé…ç½®
universal_stages:
  stage1:  # å¿…é¡»æœ‰å®šä¹‰
    name: '...'
    layers: [...]
    epochs: 10

# 3. æ£€æŸ¥è®­ç»ƒæ—¥å¿—
# åº”è¯¥çœ‹åˆ°: ğŸ“ å¯åŠ¨è¯¾ç¨‹å­¦ä¹ ï¼Œå…±Xä¸ªé˜¶æ®µ
```

---

## æ€»ç»“

æœ¬æ–‡è¯¦ç»†è¯´æ˜äº†åˆ†å±‚æ¶æ„ä¸­çš„æ•°æ®æµåŠ¨ï¼š

1. **è®­ç»ƒæ•°æ®æµ**: Dataset â†’ DataLoader â†’ Preprocess â†’ Normalize â†’ HierarchicalScheduler â†’ Loss â†’ Backward
2. **æ¨ç†æ•°æ®æµ**: Env â†’ Observation â†’ Policy â†’ HierarchicalScheduler â†’ Action â†’ Env
3. **å®æˆ˜ç¤ºä¾‹**: ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹
4. **è°ƒè¯•æŠ€å·§**: å¦‚ä½•å®šä½å’Œè§£å†³é—®é¢˜
5. **æ€§èƒ½ä¼˜åŒ–**: å†…å­˜ã€é€Ÿåº¦ã€åˆ†å¸ƒå¼è®­ç»ƒ
6. **å¸¸è§é—®é¢˜**: å…¸å‹é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

---

**ç›¸å…³æ–‡æ¡£**:
- [ä¸»æ¶æ„æ–‡æ¡£](hierarchical_policy_architecture.md)
- [å±‚è¯¦ç»†è®¾è®¡](hierarchical_layers_detailed.md)
- [è®­ç»ƒè„šæœ¬](../kuavo_train/train_hierarchical_policy.py)

