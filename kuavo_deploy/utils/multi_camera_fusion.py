"""
å¤šç›¸æœºæ·±åº¦èåˆé¢„å¤„ç†æ¨¡å—

ä¸ºSmolVLAæä¾›å¤šç›¸æœºRGB+æ·±åº¦ä¿¡æ¯çš„èåˆé¢„å¤„ç†åŠŸèƒ½
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Union
import time
from .depth_conversion import depth_to_rgb_for_smolvla


class MultiCameraDepthFusion:
    """
    å¤šç›¸æœºæ·±åº¦èåˆå¤„ç†å™¨

    æ”¯æŒ3ä¸ªRGBç›¸æœº + 3ä¸ªæ·±åº¦ç›¸æœºçš„èåˆå¤„ç†
    """

    def __init__(self,
                 target_size: Tuple[int, int] = (512, 512),
                 depth_range: Tuple[float, float] = (0, 1000),
                 device: str = 'cpu',
                 enable_depth: bool = True):
        """
        åˆå§‹åŒ–å¤šç›¸æœºæ·±åº¦èåˆå™¨

        Args:
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸ (height, width)
            depth_range: æ·±åº¦å€¼èŒƒå›´ (min_depth, max_depth)
            device: è®¾å¤‡
            enable_depth: æ˜¯å¦å¯ç”¨æ·±åº¦å¤„ç†
        """
        self.target_size = target_size
        self.depth_range = depth_range
        self.device = device
        self.enable_depth = enable_depth

        # ç›¸æœºé…å¯¹æ˜ å°„
        self.camera_pairs = {
            'head_cam_h': 'depth_h',      # å¤´éƒ¨RGB + å¤´éƒ¨æ·±åº¦
            'wrist_cam_l': 'depth_l',     # å·¦æ‰‹RGB + å·¦æ‰‹æ·±åº¦
            'wrist_cam_r': 'depth_r',     # å³æ‰‹RGB + å³æ‰‹æ·±åº¦
        }

        # RGBç›¸æœºåˆ—è¡¨
        self.rgb_cameras = ['head_cam_h', 'wrist_cam_l', 'wrist_cam_r']

        # æ·±åº¦ç›¸æœºåˆ—è¡¨
        self.depth_cameras = ['depth_h', 'depth_l', 'depth_r']

        print(f"âœ… MultiCameraDepthFusion initialized")
        print(f"   Target size: {target_size}")
        print(f"   Depth range: {depth_range}")
        print(f"   Device: {device}")
        print(f"   Enable depth: {enable_depth}")
        print(f"   Camera pairs: {self.camera_pairs}")

    def img_preprocess_smolvla(self, image: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:
        """
        SmolVLA RGBå›¾åƒé¢„å¤„ç†

        Args:
            image: è¾“å…¥å›¾åƒ [H, W, 3] æˆ– [3, H, W]

        Returns:
            processed_tensor: é¢„å¤„ç†åçš„å¼ é‡ [1, 3, H, W]
        """
        from torchvision.transforms import functional as F
        from torchvision.transforms import InterpolationMode

        # è½¬æ¢ä¸ºtensor
        if isinstance(image, np.ndarray):
            tensor_img = torch.from_numpy(
                image).permute(2, 0, 1).float() / 255.0
        else:
            tensor_img = image.float()

        # ç¡®ä¿æ˜¯3é€šé“
        if tensor_img.shape[0] != 3:
            tensor_img = tensor_img.unsqueeze(0).repeat(3, 1, 1)

        h, w = tensor_img.shape[-2:]
        target_h, target_w = self.target_size

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆä¿æŒé•¿å®½æ¯”ï¼‰
        scale = min(target_h / h, target_w / w)
        new_h, new_w = int(h * scale), int(w * scale)

        # Resize
        tensor_img = F.resize(
            tensor_img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)

        # Padåˆ°ç›®æ ‡å°ºå¯¸
        pad_h = target_h - new_h
        pad_w = target_w - new_w
        pad_top = pad_h // 2
        pad_bottom = pad_h - pad_top
        pad_left = pad_w // 2
        pad_right = pad_w - pad_left

        tensor_img = torch.nn.functional.pad(
            tensor_img,
            (pad_left, pad_right, pad_top, pad_bottom),
            mode='constant',
            value=0
        )

        # æ·»åŠ batchç»´åº¦
        return tensor_img.unsqueeze(0).to(self.device, non_blocking=True)

    def process_single_camera_pair(self,
                                   rgb_key: str,
                                   depth_key: str,
                                   obs: Dict) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†å•ä¸ªç›¸æœºå¯¹ï¼ˆRGB + æ·±åº¦ï¼‰

        Args:
            rgb_key: RGBç›¸æœºé”®å
            depth_key: æ·±åº¦ç›¸æœºé”®å
            obs: è§‚æµ‹æ•°æ®

        Returns:
            processed_data: å¤„ç†åçš„æ•°æ®
        """
        processed_data = {}

        # å¤„ç†RGBå›¾åƒ
        if rgb_key in obs:
            rgb_image = obs[rgb_key]
            processed_data[f"observation.{rgb_key}"] = self.img_preprocess_smolvla(
                rgb_image)

        # å¤„ç†æ·±åº¦å›¾åƒ
        if self.enable_depth and depth_key in obs:
            depth_image = obs[depth_key]
            depth_rgb = depth_to_rgb_for_smolvla(
                depth_image,
                target_size=self.target_size,
                depth_range=self.depth_range,
                device=self.device
            )
            processed_data[f"observation.{depth_key}"] = depth_rgb

        return processed_data

    def process_observations(self, obs: Dict) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†å¤šç›¸æœºè§‚æµ‹æ•°æ®

        Args:
            obs: åŸå§‹è§‚æµ‹æ•°æ®

        Returns:
            observation: å¤„ç†åçš„è§‚æµ‹æ•°æ®
        """
        observation = {}

        # å¤„ç†æ‰€æœ‰ç›¸æœºå¯¹
        for rgb_key, depth_key in self.camera_pairs.items():
            pair_data = self.process_single_camera_pair(
                rgb_key, depth_key, obs)
            observation.update(pair_data)

        # å¤„ç†å…¶ä»–è§‚æµ‹æ•°æ®ï¼ˆå¦‚çŠ¶æ€ä¿¡æ¯ï¼‰
        for key, value in obs.items():
            if key not in self.rgb_cameras and key not in self.depth_cameras:
                if 'state' in key.lower():
                    observation[f"observation.{key}"] = torch.tensor(
                        value, dtype=torch.float32
                    ).unsqueeze(0).to(self.device)

        return observation

    def process_observations_simple(self, obs: Dict) -> Dict[str, torch.Tensor]:
        """
        ç®€å•çš„å¤šç›¸æœºå¤„ç†ï¼ˆç‹¬ç«‹å¤„ç†æ¯ä¸ªç›¸æœºï¼‰

        Args:
            obs: åŸå§‹è§‚æµ‹æ•°æ®

        Returns:
            observation: å¤„ç†åçš„è§‚æµ‹æ•°æ®
        """
        observation = {}

        # å¤„ç†æ‰€æœ‰RGBç›¸æœº
        for camera in self.rgb_cameras:
            if camera in obs:
                observation[f"observation.{camera}"] = self.img_preprocess_smolvla(
                    obs[camera])

        # å¤„ç†æ‰€æœ‰æ·±åº¦ç›¸æœºï¼ˆè½¬æ¢ä¸ºRGBä¼ªå½©è‰²ï¼‰
        if self.enable_depth:
            for camera in self.depth_cameras:
                if camera in obs:
                    depth_rgb = depth_to_rgb_for_smolvla(
                        obs[camera],
                        target_size=self.target_size,
                        depth_range=self.depth_range,
                        device=self.device
                    )
                    observation[f"observation.{camera}"] = depth_rgb

        # å¤„ç†çŠ¶æ€ä¿¡æ¯
        for key, value in obs.items():
            if key not in self.rgb_cameras and key not in self.depth_cameras:
                if 'state' in key.lower():
                    observation[f"observation.{key}"] = torch.tensor(
                        value, dtype=torch.float32
                    ).unsqueeze(0).to(self.device)

        return observation

    def get_processing_stats(self) -> Dict[str, any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            stats: ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            'target_size': self.target_size,
            'depth_range': self.depth_range,
            'device': self.device,
            'enable_depth': self.enable_depth,
            'rgb_cameras': self.rgb_cameras,
            'depth_cameras': self.depth_cameras,
            'camera_pairs': self.camera_pairs,
        }


def create_multi_camera_fusion(target_size: Tuple[int, int] = (512, 512),
                               depth_range: Tuple[float, float] = (0, 1000),
                               device: str = 'cpu',
                               enable_depth: bool = True) -> MultiCameraDepthFusion:
    """
    åˆ›å»ºå¤šç›¸æœºæ·±åº¦èåˆå™¨

    Args:
        target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
        depth_range: æ·±åº¦å€¼èŒƒå›´
        device: è®¾å¤‡
        enable_depth: æ˜¯å¦å¯ç”¨æ·±åº¦å¤„ç†

    Returns:
        fusion_processor: æ·±åº¦èåˆå¤„ç†å™¨
    """
    return MultiCameraDepthFusion(
        target_size=target_size,
        depth_range=depth_range,
        device=device,
        enable_depth=enable_depth
    )


def benchmark_multi_camera_fusion():
    """å¤šç›¸æœºèåˆæ€§èƒ½æµ‹è¯•"""
    print("ğŸ” å¤šç›¸æœºèåˆæ€§èƒ½æµ‹è¯•")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    obs = {}
    for camera in ['head_cam_h', 'wrist_cam_l', 'wrist_cam_r']:
        obs[camera] = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

    for camera in ['depth_h', 'depth_l', 'depth_r']:
        obs[camera] = np.random.randint(0, 1000, (480, 640), dtype=np.uint16)

    obs['state'] = np.random.randn(16)

    # åˆ›å»ºèåˆå™¨
    fusion_processor = create_multi_camera_fusion(device='cpu')

    # æµ‹è¯•å¤„ç†æ—¶é—´
    times = []
    for _ in range(50):
        start_time = time.time()
        processed_obs = fusion_processor.process_observations_simple(obs)
        processing_time = (time.time() - start_time) * 1000
        times.append(processing_time)

    print(f"å¤šç›¸æœºèåˆå¤„ç†:")
    print(f"  å¹³å‡æ—¶é—´: {np.mean(times):.2f}ms")
    print(f"  æ ‡å‡†å·®: {np.std(times):.2f}ms")
    print(f"  æœ€å¤§æ—¶é—´: {np.max(times):.2f}ms")

    print(f"\nå¤„ç†ç»“æœ:")
    for key, value in processed_obs.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")

    return fusion_processor


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    fusion_processor = benchmark_multi_camera_fusion()

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = fusion_processor.get_processing_stats()
    print(f"\nèåˆå™¨ç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
