"""
æ·±åº¦å›¾åƒåˆ°RGBé¢œè‰²æ˜ å°„å·¥å…·

å°†æ·±åº¦å›¾åƒè½¬æ¢ä¸ºRGBä¼ªå½©è‰²å›¾åƒï¼Œä½¿SmolVLAèƒ½å¤Ÿå¤„ç†æ·±åº¦ä¿¡æ¯
"""

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from typing import Union, Tuple, List
import time


def depth_to_rgb_opencv(depth_image: np.ndarray,
                        colormap_type: int = cv2.COLORMAP_JET,
                        depth_range: Tuple[float, float] = (0, 1000)) -> np.ndarray:
    """
    ä½¿ç”¨OpenCVå°†æ·±åº¦å›¾åƒè½¬æ¢ä¸ºRGBä¼ªå½©è‰²å›¾åƒ

    Args:
        depth_image: æ·±åº¦å›¾åƒ [H, W] æˆ– [H, W, 1]
        colormap_type: OpenCVé¢œè‰²æ˜ å°„ç±»å‹ (cv2.COLORMAP_JET, cv2.COLORMAP_RAINBOWç­‰)
        depth_range: æ·±åº¦å€¼èŒƒå›´ (min_depth, max_depth)

    Returns:
        rgb_image: RGBä¼ªå½©è‰²å›¾åƒ [H, W, 3]
    """
    # ç¡®ä¿è¾“å…¥æ˜¯å•é€šé“
    if len(depth_image.shape) == 3:
        depth_image = depth_image.squeeze()

    # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´
    depth_clipped = np.clip(depth_image, depth_range[0], depth_range[1])

    # å½’ä¸€åŒ–åˆ° [0, 255]
    depth_normalized = cv2.normalize(
        depth_clipped, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U
    )

    # åº”ç”¨é¢œè‰²æ˜ å°„
    rgb_image = cv2.applyColorMap(depth_normalized, colormap_type)

    return rgb_image


def depth_to_rgb_torch(depth_tensor: torch.Tensor,
                       colormap_lut: torch.Tensor,
                       depth_range: Tuple[float, float] = (0, 1000)) -> torch.Tensor:
    """
    ä½¿ç”¨PyTorchå°†æ·±åº¦å¼ é‡è½¬æ¢ä¸ºRGBä¼ªå½©è‰²å¼ é‡

    Args:
        depth_tensor: æ·±åº¦å¼ é‡ [B, 1, H, W] æˆ– [B, H, W]
        colormap_lut: é¢œè‰²æ˜ å°„æŸ¥æ‰¾è¡¨ [256, 3]
        depth_range: æ·±åº¦å€¼èŒƒå›´ (min_depth, max_depth)

    Returns:
        rgb_tensor: RGBä¼ªå½©è‰²å¼ é‡ [B, 3, H, W]
    """
    # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
    if depth_tensor.dim() == 3:
        depth_tensor = depth_tensor.unsqueeze(1)  # [B, H, W] -> [B, 1, H, W]

    batch_size, channels, height, width = depth_tensor.shape

    # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´
    depth_clipped = torch.clamp(depth_tensor, depth_range[0], depth_range[1])

    # å½’ä¸€åŒ–åˆ° [0, 1]
    depth_min, depth_max = depth_range
    depth_normalized = (depth_clipped - depth_min) / (depth_max - depth_min)

    # æ˜ å°„åˆ°é¢œè‰²ç´¢å¼• [0, 255]
    indices = (depth_normalized * (colormap_lut.shape[0] - 1)).long()
    indices = torch.clamp(indices, 0, colormap_lut.shape[0] - 1)

    # åº”ç”¨é¢œè‰²æ˜ å°„
    rgb_tensor = colormap_lut[indices].permute(
        0, 4, 1, 2, 3).squeeze(4)  # [B, 3, H, W]

    return rgb_tensor


def create_jet_colormap_lut(device: str = 'cpu') -> torch.Tensor:
    """
    åˆ›å»ºJeté¢œè‰²æ˜ å°„æŸ¥æ‰¾è¡¨

    Args:
        device: è®¾å¤‡ ('cpu' æˆ– 'cuda')

    Returns:
        colormap_lut: Jeté¢œè‰²æ˜ å°„æŸ¥æ‰¾è¡¨ [256, 3]
    """
    lut = torch.zeros(256, 3, device=device)

    for i in range(256):
        value = i / 255.0
        r, g, b = jet_colormap(value)
        lut[i] = torch.tensor([r, g, b], device=device) / 255.0

    return lut


def jet_colormap(value: float) -> Tuple[float, float, float]:
    """
    Jeté¢œè‰²æ˜ å°„å‡½æ•°

    Args:
        value: å½’ä¸€åŒ–å€¼ [0, 1]

    Returns:
        (r, g, b): RGBé¢œè‰²å€¼ [0, 1]
    """
    # Jeté¢œè‰²æ˜ å°„çš„æ•°å­¦å®šä¹‰
    if value < 0.125:
        # æ·±è“åˆ°è“
        r = 0
        g = 0
        b = 0.5 + 4 * value
    elif value < 0.375:
        # è“åˆ°é’
        r = 0
        g = 4 * (value - 0.125)
        b = 1
    elif value < 0.625:
        # é’åˆ°ç»¿
        r = 0
        g = 1
        b = 1 - 4 * (value - 0.375)
    elif value < 0.875:
        # ç»¿åˆ°é»„
        r = 4 * (value - 0.625)
        g = 1
        b = 0
    else:
        # é»„åˆ°çº¢
        r = 1
        g = 1 - 4 * (value - 0.875)
        b = 0

    return r, g, b


def depth_to_rgb_for_smolvla(depth_image: Union[np.ndarray, torch.Tensor],
                             target_size: Tuple[int, int] = (512, 512),
                             depth_range: Tuple[float, float] = (0, 1000),
                             device: str = 'cpu',
                             use_padding: bool = True) -> torch.Tensor:
    """
    ä¸ºSmolVLAå°†æ·±åº¦å›¾åƒè½¬æ¢ä¸ºRGBä¼ªå½©è‰²å¼ é‡

    æ”¯æŒä¸¤ç§å¤„ç†æ–¹å¼ï¼š
    1. use_padding=True: ä¿æŒé•¿å®½æ¯”ï¼Œç”¨paddingå¡«å…… (æ¨èç”¨äºé«˜ç²¾åº¦ä»»åŠ¡)
    2. use_padding=False: ç›´æ¥resizeåˆ°ç›®æ ‡å°ºå¯¸ (å¿«é€Ÿå¤„ç†)

    Args:
        depth_image: æ·±åº¦å›¾åƒ [H, W] æˆ– [H, W, 1]
        target_size: ç›®æ ‡å°ºå¯¸ (height, width)
        depth_range: æ·±åº¦å€¼èŒƒå›´ (min_depth, max_depth)
        device: è®¾å¤‡
        use_padding: æ˜¯å¦ä½¿ç”¨paddingæ–¹å¼ä¿æŒé•¿å®½æ¯”

    Returns:
        rgb_tensor: RGBå¼ é‡ [1, 3, H, W]
    """
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(depth_image, torch.Tensor):
        depth_np = depth_image.cpu().numpy()
    else:
        depth_np = depth_image

    # ç¡®ä¿æ˜¯å•é€šé“
    if len(depth_np.shape) == 3:
        depth_np = depth_np.squeeze()

    # è½¬æ¢ä¸ºRGBä¼ªå½©è‰²
    rgb_image = depth_to_rgb_opencv(depth_np, cv2.COLORMAP_JET, depth_range)

    if use_padding:
        # ä½¿ç”¨paddingæ–¹å¼ä¿æŒé•¿å®½æ¯”
        rgb_tensor = _resize_with_padding(rgb_image, target_size, device)
    else:
        # ç›´æ¥resizeåˆ°ç›®æ ‡å°ºå¯¸
        if rgb_image.shape[:2] != target_size:
            rgb_image = cv2.resize(
                rgb_image, target_size[::-1], interpolation=cv2.INTER_LINEAR)

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        rgb_tensor = torch.from_numpy(
            rgb_image).permute(2, 0, 1).float() / 255.0
        rgb_tensor = rgb_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦ [1, 3, H, W]
        rgb_tensor = rgb_tensor.to(device)

    return rgb_tensor


def _resize_with_padding(rgb_image: np.ndarray,
                         target_size: Tuple[int, int],
                         device: str) -> torch.Tensor:
    """
    ä½¿ç”¨paddingæ–¹å¼è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œä¿æŒé•¿å®½æ¯”

    Args:
        rgb_image: RGBå›¾åƒ [H, W, 3]
        target_size: ç›®æ ‡å°ºå¯¸ (height, width)
        device: è®¾å¤‡

    Returns:
        rgb_tensor: RGBå¼ é‡ [1, 3, H, W]
    """
    from torchvision.transforms import functional as F
    from torchvision.transforms import InterpolationMode

    # è½¬æ¢ä¸ºtensor
    tensor_img = torch.from_numpy(rgb_image).permute(2, 0, 1).float() / 255.0

    h, w = tensor_img.shape[-2:]
    target_h, target_w = target_size

    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆä¿æŒé•¿å®½æ¯”ï¼‰
    scale = min(target_h / h, target_w / w)
    new_h, new_w = int(h * scale), int(w * scale)

    # Resize
    tensor_img = F.resize(
        tensor_img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)

    # Padåˆ°ç›®æ ‡å°ºå¯¸
    pad_h = target_h - new_h
    pad_w = target_w - new_w
    pad_top = pad_h // 2
    pad_bottom = pad_h - pad_top
    pad_left = pad_w // 2
    pad_right = pad_w - pad_left

    tensor_img = torch.nn.functional.pad(
        tensor_img,
        (pad_left, pad_right, pad_top, pad_bottom),
        mode='constant',
        value=0  # ç”¨0å¡«å……ï¼ˆæ·±åº¦=0è¡¨ç¤ºæ— æ•ˆåŒºåŸŸï¼‰
    )

    # æ·»åŠ batchç»´åº¦
    return tensor_img.unsqueeze(0).to(device, non_blocking=True)


def benchmark_depth_conversion():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸ” æ·±åº¦è½¬æ¢æ€§èƒ½æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•æ•°æ®
    depth_image = np.random.randint(0, 1000, (512, 512), dtype=np.uint16)

    # OpenCVå®ç°æµ‹è¯•
    times_opencv = []
    for _ in range(100):
        start_time = time.time()
        rgb_image = depth_to_rgb_opencv(depth_image)
        conversion_time = (time.time() - start_time) * 1000
        times_opencv.append(conversion_time)

    print(f"OpenCVå®ç°:")
    print(f"  å¹³å‡æ—¶é—´: {np.mean(times_opencv):.2f}ms")
    print(f"  æ ‡å‡†å·®: {np.std(times_opencv):.2f}ms")
    print(f"  æœ€å¤§æ—¶é—´: {np.max(times_opencv):.2f}ms")

    # PyTorchå®ç°æµ‹è¯•
    depth_tensor = torch.from_numpy(
        depth_image).unsqueeze(0).unsqueeze(0).float()
    colormap_lut = create_jet_colormap_lut()

    times_torch = []
    for _ in range(100):
        start_time = time.time()
        rgb_tensor = depth_to_rgb_torch(depth_tensor, colormap_lut)
        conversion_time = (time.time() - start_time) * 1000
        times_torch.append(conversion_time)

    print(f"\nPyTorchå®ç°:")
    print(f"  å¹³å‡æ—¶é—´: {np.mean(times_torch):.2f}ms")
    print(f"  æ ‡å‡†å·®: {np.std(times_torch):.2f}ms")
    print(f"  æœ€å¤§æ—¶é—´: {np.max(times_torch):.2f}ms")

    print(
        f"\næ¨èä½¿ç”¨: {'OpenCV' if np.mean(times_opencv) < np.mean(times_torch) else 'PyTorch'}")


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    benchmark_depth_conversion()
