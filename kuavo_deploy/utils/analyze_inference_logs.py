"""
æ¨ç†æ—¥å¿—åˆ†æå·¥å…·

ç”¨äºåˆ†ææ¨ç†æ—¥å¿—æ–‡ä»¶ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Any
import numpy as np


def read_jsonl(file_path: Path) -> List[Dict[str, Any]]:
    """è¯»å–JSONLæ–‡ä»¶"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def analyze_inference_time(log_dir: Path):
    """åˆ†ææ¨ç†æ—¶é—´ç»Ÿè®¡"""
    print("\n" + "="*60)
    print("ğŸ“Š æ¨ç†æ—¶é—´åˆ†æ")
    print("="*60)

    log_files = list(log_dir.glob("inference_episode_*.jsonl"))
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        return

    all_times = []
    episode_times = {}

    for log_file in log_files:
        episode_idx = int(log_file.stem.split('_')[-1])
        steps = read_jsonl(log_file)
        times = [step['inference_time_ms'] for step in steps]
        all_times.extend(times)
        episode_times[episode_idx] = {
            'mean': np.mean(times),
            'min': np.min(times),
            'max': np.max(times),
            'std': np.std(times),
        }

    print(f"\næ€»ä½“ç»Ÿè®¡ (åŸºäº {len(log_files)} ä¸ªå›åˆ, {len(all_times)} æ­¥):")
    print(f"  å¹³å‡æ¨ç†æ—¶é—´: {np.mean(all_times):.2f}ms")
    print(f"  æœ€å°æ¨ç†æ—¶é—´: {np.min(all_times):.2f}ms")
    print(f"  æœ€å¤§æ¨ç†æ—¶é—´: {np.max(all_times):.2f}ms")
    print(f"  æ ‡å‡†å·®: {np.std(all_times):.2f}ms")
    print(f"  ä¸­ä½æ•°: {np.median(all_times):.2f}ms")
    print(f"  95åˆ†ä½æ•°: {np.percentile(all_times, 95):.2f}ms")
    print(f"  99åˆ†ä½æ•°: {np.percentile(all_times, 99):.2f}ms")

    # æ˜¾ç¤ºæ¨ç†æ—¶é—´åˆ†å¸ƒ
    print(f"\næ¨ç†æ—¶é—´åˆ†å¸ƒ:")
    bins = [0, 10, 20, 30, 40, 50, 75, 100, 150, 200, float('inf')]
    bin_labels = ['0-10ms', '10-20ms', '20-30ms', '30-40ms', '40-50ms',
                  '50-75ms', '75-100ms', '100-150ms', '150-200ms', '>200ms']

    hist, _ = np.histogram(all_times, bins=bins)
    for label, count in zip(bin_labels, hist):
        percentage = count / len(all_times) * 100
        bar = 'â–ˆ' * int(percentage / 2)
        print(f"  {label:>12}: {bar} {count:5d} ({percentage:5.1f}%)")


def analyze_action_statistics(log_dir: Path):
    """åˆ†æåŠ¨ä½œç»Ÿè®¡"""
    print("\n" + "="*60)
    print("ğŸ¯ åŠ¨ä½œç»Ÿè®¡åˆ†æ")
    print("="*60)

    log_files = list(log_dir.glob("inference_episode_*.jsonl"))
    if not log_files:
        return

    all_action_means = []
    all_action_stds = []
    all_action_mins = []
    all_action_maxs = []

    for log_file in log_files[:5]:  # åªåˆ†æå‰5ä¸ªå›åˆä½œä¸ºç¤ºä¾‹
        steps = read_jsonl(log_file)
        for step in steps:
            action_info = step.get('action', {})
            all_action_means.append(action_info.get('mean', 0))
            all_action_stds.append(action_info.get('std', 0))
            all_action_mins.append(action_info.get('min', 0))
            all_action_maxs.append(action_info.get('max', 0))

    print(f"\nåŸºäºå‰ {min(5, len(log_files))} ä¸ªå›åˆçš„åŠ¨ä½œç»Ÿè®¡:")
    print(f"  åŠ¨ä½œå‡å€¼çš„å¹³å‡å€¼: {np.mean(all_action_means):.4f}")
    print(f"  åŠ¨ä½œæ ‡å‡†å·®çš„å¹³å‡å€¼: {np.mean(all_action_stds):.4f}")
    print(f"  åŠ¨ä½œæœ€å°å€¼çš„å¹³å‡å€¼: {np.min(all_action_mins):.4f}")
    print(f"  åŠ¨ä½œæœ€å¤§å€¼çš„å¹³å‡å€¼: {np.max(all_action_maxs):.4f}")


def analyze_layer_activation(log_dir: Path):
    """åˆ†æå±‚æ¿€æ´»æƒ…å†µ"""
    print("\n" + "="*60)
    print("ğŸ—ï¸  åˆ†å±‚æ¶æ„æ¿€æ´»åˆ†æ")
    print("="*60)

    # è¯»å–èšåˆæŠ¥å‘Š
    aggregated_file = log_dir / "aggregated_inference_report.json"
    if not aggregated_file.exists():
        print("âš ï¸  æœªæ‰¾åˆ°èšåˆæŠ¥å‘Šï¼Œè·³è¿‡å±‚æ¿€æ´»åˆ†æ")
        return

    with open(aggregated_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if 'hierarchical_aggregated_stats' not in data:
        print("â„¹ï¸  å½“å‰ç­–ç•¥ä¸æ˜¯åˆ†å±‚æ¶æ„ï¼Œæ— å±‚æ¿€æ´»ä¿¡æ¯")
        return

    h_stats = data['hierarchical_aggregated_stats']

    print(f"\nå±‚æ¿€æ´»ç»Ÿè®¡ (æ€»å›åˆæ•°: {data['total_episodes']}):")
    for layer, stats in h_stats.items():
        print(f"\n  ğŸ“ {layer.upper()} å±‚:")
        print(f"    æ€»æ¿€æ´»æ¬¡æ•°: {stats['total_activations']}")
        print(f"    å¹³å‡æ‰§è¡Œæ—¶é—´: {stats['avg_execution_time_ms']:.2f}ms")

        # è®¡ç®—æ¿€æ´»ç‡ï¼ˆç›¸å¯¹äºæ€»æ­¥æ•°ï¼‰
        total_steps = sum(ep['total_steps'] for ep in data['episodes'])
        activation_rate = stats['total_activations'] / total_steps * 100
        print(f"    æ¿€æ´»ç‡: {activation_rate:.1f}%")

    # åˆ†æå±‚æ‰§è¡Œæ—¶é—´å æ¯”
    print(f"\nå±‚æ‰§è¡Œæ—¶é—´å æ¯”:")
    total_layer_time = sum(stats['avg_execution_time_ms'] * stats['total_activations']
                           for stats in h_stats.values())

    for layer, stats in h_stats.items():
        layer_total_time = stats['avg_execution_time_ms'] * \
            stats['total_activations']
        percentage = layer_total_time / total_layer_time * 100
        bar = 'â–ˆ' * int(percentage / 5)
        print(f"  {layer:>12}: {bar} {percentage:5.1f}%")


def analyze_success_rate(log_dir: Path):
    """åˆ†ææˆåŠŸç‡"""
    print("\n" + "="*60)
    print("ğŸ¯ æˆåŠŸç‡åˆ†æ")
    print("="*60)

    summary_files = list(log_dir.glob("inference_episode_*_summary.json"))
    if not summary_files:
        print("âŒ æœªæ‰¾åˆ°æ€»ç»“æ–‡ä»¶")
        return

    successes = []
    episode_steps = []
    episode_durations = []

    for summary_file in sorted(summary_files):
        with open(summary_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            successes.append(data.get('success', False))
            episode_steps.append(data.get('total_steps', 0))
            episode_durations.append(data.get('episode_duration_sec', 0))

    success_count = sum(successes)
    total_episodes = len(successes)
    success_rate = success_count / total_episodes if total_episodes > 0 else 0

    print(f"\næ€»ä½“æˆåŠŸç‡: {success_rate:.2%} ({success_count}/{total_episodes})")
    print(f"å¹³å‡å›åˆæ­¥æ•°: {np.mean(episode_steps):.1f}")
    print(f"å¹³å‡å›åˆæ—¶é•¿: {np.mean(episode_durations):.1f}ç§’")

    # æˆåŠŸå’Œå¤±è´¥çš„å¯¹æ¯”
    if success_count > 0 and success_count < total_episodes:
        success_steps = [steps for success, steps in zip(
            successes, episode_steps) if success]
        fail_steps = [steps for success, steps in zip(
            successes, episode_steps) if not success]

        print(f"\næˆåŠŸå›åˆå¹³å‡æ­¥æ•°: {np.mean(success_steps):.1f}")
        print(f"å¤±è´¥å›åˆå¹³å‡æ­¥æ•°: {np.mean(fail_steps):.1f}")


def generate_report(log_dir: Path, output_file: Path = None):
    """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
    log_dir = Path(log_dir)

    if not log_dir.exists():
        print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return

    print("\n" + "="*60)
    print(f"ğŸ“ æ¨ç†æ—¥å¿—åˆ†ææŠ¥å‘Š")
    print(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
    print("="*60)

    # è¿è¡Œå„é¡¹åˆ†æ
    analyze_success_rate(log_dir)
    analyze_inference_time(log_dir)
    analyze_action_statistics(log_dir)
    analyze_layer_activation(log_dir)

    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆ!")
    print("="*60 + "\n")

    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜æŠ¥å‘Šï¼ˆè¿™é‡Œåªæ˜¯æ‰“å°ï¼Œå®é™…å¯ä»¥ä¿å­˜ä¸ºæ–‡ä»¶ï¼‰
    if output_file:
        print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="æ¨ç†æ—¥å¿—åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python analyze_inference_logs.py \\
    --log_dir outputs/eval/task_400_episodes/humanoid_hierarchical/task_specific_run_20251009_033145/epoch30/inference_logs

  python analyze_inference_logs.py \\
    --log_dir outputs/eval/.../inference_logs \\
    --output report.txt
        """
    )

    parser.add_argument(
        '--log_dir',
        type=str,
        required=True,
        help='æ¨ç†æ—¥å¿—ç›®å½•è·¯å¾„'
    )

    parser.add_argument(
        '--output',
        type=str,
        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰'
    )

    args = parser.parse_args()

    log_dir = Path(args.log_dir)
    output_file = Path(args.output) if args.output else None

    generate_report(log_dir, output_file)


if __name__ == "__main__":
    main()
