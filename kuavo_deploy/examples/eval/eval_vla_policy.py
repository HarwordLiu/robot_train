# -*- coding: utf-8 -*-
"""
VLA Transformer Policyéƒ¨ç½²æ¨¡å—

æ”¯æŒVLAç­–ç•¥çš„éƒ¨ç½²å’Œæ¨ç†
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

import lerobot_patches.custom_patches

from dataclasses import dataclass, field
import hydra
import gymnasium as gym
import imageio
import numpy
import torch
from tqdm import tqdm
import datetime
import time
import numpy as np
from omegaconf import DictConfig, ListConfig, OmegaConf
from torchvision.transforms.functional import to_tensor
from std_msgs.msg import Bool
import rospy
import threading

# å¯¼å…¥VLAæ¨¡å—
from kuavo_train.wrapper.policy.vla.VLAPolicyWrapper import VLAPolicyWrapper
from lerobot.utils.random_utils import set_seed

from configs.deploy.config_inference import load_inference_config
from kuavo_deploy.utils.logging_utils import setup_logger

log_model = setup_logger("model")
log_robot = setup_logger("robot")

def pause_callback(msg):
    if msg.data:
        pause_flag.set()
    else:
        pause_flag.clear()

def stop_callback(msg):
    if msg.data:
        stop_flag.set()

pause_sub = rospy.Subscriber('/kuavo/pause_state', Bool, pause_callback, queue_size=10)
stop_sub = rospy.Subscriber('/kuavo/stop_state', Bool, stop_callback, queue_size=10)
stop_flag = threading.Event()
pause_flag = threading.Event()

def img_preprocess(image, device="cpu"):
    return to_tensor(image).unsqueeze(0).to(device, non_blocking=True)

def depth_preprocess(depth, device="cpu", depth_range=[0, 1000]):
    """é¢„å¤„ç†æ·±åº¦å›¾åƒ"""
    depth = np.array(depth)
    depth = np.clip(depth, depth_range[0], depth_range[1])
    depth = (depth - depth_range[0]) / (depth_range[1] - depth_range[0])
    return torch.tensor(depth, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device, non_blocking=True)

def setup_vla_policy(pretrained_path, device=torch.device("cuda")):
    """
    è®¾ç½®å¹¶åŠ è½½VLAç­–ç•¥æ¨¡å‹

    Args:
        pretrained_path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡

    Returns:
        åŠ è½½çš„ç­–ç•¥æ¨¡å‹
    """

    if device.type == 'cpu':
        log_model.warning("Warning: Using CPU for inference, this may be slow.")
        time.sleep(3)

    log_model.info("ğŸ¤– Loading VLA Transformer Policy...")
    policy = VLAPolicyWrapper.from_pretrained(Path(pretrained_path), strict=True)

    policy.eval()
    policy.to(device)
    policy.reset()

    # Log model info
    log_model.info(f"âœ… Model loaded from {pretrained_path}")
    log_model.info(f"ğŸ“‹ Model n_obs_steps: {policy.config.n_obs_steps}")
    log_model.info(f"ğŸ–¥ï¸  Model device: {device}")
    log_model.info(f"ğŸ”§ Policy type: VLA Transformer")
    log_model.info(f"ğŸ“Š Token dim: {policy.config.token_embed_dim}")
    log_model.info(f"ğŸ“Š Transformer depth: {policy.config.transformer_depth}")

    return policy

def main(config_path: str, env: gym.Env):
    """VLAä¸»æ¨ç†å¾ªç¯"""

    # åŠ è½½é…ç½®
    cfg = load_inference_config(config_path)
    from omegaconf import OmegaConf
    full_cfg = OmegaConf.load(config_path)

    use_delta = cfg.use_delta
    eval_episodes = cfg.eval_episodes
    device = torch.device(cfg.device)

    # è®¾ç½®éšæœºç§å­
    set_seed(cfg.seed)

    # æ„å»ºæ¨¡å‹è·¯å¾„
    pretrained_path = f"outputs/train/{cfg.task}/{cfg.method}/{cfg.timestamp}/epoch{cfg.epoch}"

    # åŠ è½½VLAç­–ç•¥
    policy = setup_vla_policy(pretrained_path, device)

    # æ¨ç†å¾ªç¯
    results = []
    for episode in range(eval_episodes):

        log_model.info(f"ğŸ¯ Episode {episode + 1}/{eval_episodes}")

        # é‡ç½®ç¯å¢ƒå’Œç­–ç•¥
        obs, info = env.reset()
        policy.reset()

        episode_reward = 0
        episode_length = 0
        success = False

        # è®°å½•æ¨ç†æ—¶é—´
        inference_times = []

        while True:
            # æ£€æŸ¥æ§åˆ¶ä¿¡å·
            if stop_flag.is_set():
                log_model.info("ğŸ›‘ Stop signal received, terminating...")
                break

            while pause_flag.is_set():
                log_model.info("â¸ï¸  Paused, waiting for resume...")
                time.sleep(0.1)

            # é¢„å¤„ç†è§‚æµ‹
            observation = {}

            # å¤„ç†å›¾åƒè§‚æµ‹
            for key in obs.keys():
                if 'image' in key.lower() or 'cam' in key.lower():
                    observation[f"observation.{key}"] = img_preprocess(obs[key], device)
                elif 'depth' in key.lower():
                    observation[f"observation.{key}"] = depth_preprocess(obs[key], device, cfg.depth_range)
                elif 'state' in key.lower():
                    observation[f"observation.{key}"] = torch.tensor(obs[key], dtype=torch.float32).unsqueeze(0).to(device)

            # VLAæ¨ç†
            start_time = time.time()
            with torch.no_grad():
                action = policy.select_action(observation)
            inference_time = (time.time() - start_time) * 1000  # è½¬ä¸ºæ¯«ç§’

            inference_times.append(inference_time)

            # æ¯100æ­¥è®°å½•ä¸€æ¬¡æ¨ç†æ—¶é—´
            if episode_length % 100 == 0:
                avg_time = np.mean(inference_times[-100:]) if len(inference_times) >= 100 else np.mean(inference_times)
                log_model.info(f"Step {episode_length}: Avg inference time: {avg_time:.2f}ms")

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action.cpu().numpy())

            episode_reward += reward
            episode_length += 1

            # æ£€æŸ¥å›åˆç»“æŸ
            if terminated or truncated:
                success = info.get('is_success', False)
                break

        # è®°å½•å›åˆç»“æœ
        avg_inference_time = np.mean(inference_times)
        results.append({
            'episode': episode,
            'reward': episode_reward,
            'length': episode_length,
            'success': success,
            'avg_inference_time_ms': avg_inference_time
        })

        # è®°å½•å›åˆç»Ÿè®¡
        log_model.info(f"ğŸ“ˆ Episode {episode + 1} - Reward: {episode_reward:.3f}, Length: {episode_length}, Success: {success}")
        log_model.info(f"â±ï¸  Average inference time: {avg_inference_time:.2f}ms")

        if stop_flag.is_set():
            break

    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    if results:
        avg_reward = np.mean([r['reward'] for r in results])
        success_rate = np.mean([r['success'] for r in results])
        avg_length = np.mean([r['length'] for r in results])
        avg_inference = np.mean([r['avg_inference_time_ms'] for r in results])

        log_model.info(f"ğŸ† Final Results - Episodes: {len(results)}")
        log_model.info(f"ğŸ“Š Average Reward: {avg_reward:.3f}")
        log_model.info(f"âœ… Success Rate: {success_rate:.1%}")
        log_model.info(f"ğŸ“ Average Length: {avg_length:.1f}")
        log_model.info(f"â±ï¸  Average Inference Time: {avg_inference:.2f}ms")

    return results

# ä¸ºäº†ä¿æŒä¸åŸæœ‰æ¥å£çš„å…¼å®¹æ€§ï¼Œæä¾›setup_policyåˆ«å
def setup_policy(pretrained_path, policy_type, device=torch.device("cuda")):
    """å…¼å®¹æ€§æ¥å£"""
    if policy_type != 'vla_transformer':
        raise ValueError(f"This script only supports 'vla_transformer' policy, got '{policy_type}'")
    return setup_vla_policy(pretrained_path, device)
