# -*- coding: utf-8 -*-
"""
SmolVLA Policy Deployment Module (Enhanced)

å¢å¼ºç‰ˆSmolVLAéƒ¨ç½²è„šæœ¬ - é›†æˆæ¨ç†ä¼˜åŒ–ï¼ˆæ— éœ€é‡æ–°è®­ç»ƒï¼‰ï¼š
1. Actionåå¤„ç†ï¼šå¹³æ»‘æ»¤æ³¢ + ç²¾ç»†æ“ä½œå¢ç›Š
2. ç²¾ç¡®Language Instructionï¼šæ›´å‡†ç¡®çš„ä»»åŠ¡æè¿°
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

import lerobot_patches.custom_patches

from dataclasses import dataclass, field
import hydra
import gymnasium as gym
import imageio
import numpy
import torch
from tqdm import tqdm
import datetime
import time
import numpy as np
from omegaconf import DictConfig, ListConfig, OmegaConf
from torchvision.transforms.functional import to_tensor, resize
from torchvision.transforms import InterpolationMode
from std_msgs.msg import Bool
import rospy
import threading

# Import SmolVLA modules
from kuavo_train.wrapper.policy.smolvla.SmolVLAPolicyWrapper import SmolVLAPolicyWrapper
from lerobot.utils.random_utils import set_seed

# âœ¨ å¯¼å…¥æ¨ç†åå¤„ç†æ¨¡å—
from kuavo_deploy.utils.action_postprocessing import ActionPostProcessor

from configs.deploy.config_inference import load_inference_config
from kuavo_deploy.utils.logging_utils import setup_logger

log_model = setup_logger("model")
log_robot = setup_logger("robot")

def pause_callback(msg):
    if msg.data:
        pause_flag.set()
    else:
        pause_flag.clear()

def stop_callback(msg):
    if msg.data:
        stop_flag.set()

pause_sub = rospy.Subscriber('/kuavo/pause_state', Bool, pause_callback, queue_size=10)
stop_sub = rospy.Subscriber('/kuavo/stop_state', Bool, stop_callback, queue_size=10)
stop_flag = threading.Event()
pause_flag = threading.Event()

def img_preprocess_smolvla(image, target_size=(512, 512), device="cpu"):
    """Preprocess image for SmolVLA (512x512)"""
    tensor_img = to_tensor(image)
    h, w = tensor_img.shape[-2:]
    target_h, target_w = target_size

    scale = min(target_h / h, target_w / w)
    new_h, new_w = int(h * scale), int(w * scale)

    tensor_img = resize(tensor_img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)

    pad_h = target_h - new_h
    pad_w = target_w - new_w
    pad_top = pad_h // 2
    pad_bottom = pad_h - pad_top
    pad_left = pad_w // 2
    pad_right = pad_w - pad_left

    tensor_img = torch.nn.functional.pad(
        tensor_img,
        (pad_left, pad_right, pad_top, pad_bottom),
        mode='constant',
        value=0
    )

    return tensor_img.unsqueeze(0).to(device, non_blocking=True)

def depth_preprocess(depth, device="cpu", depth_range=[0, 1000]):
    """Preprocess depth image"""
    depth = np.array(depth)
    depth = np.clip(depth, depth_range[0], depth_range[1])
    depth = (depth - depth_range[0]) / (depth_range[1] - depth_range[0])
    return torch.tensor(depth, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device, non_blocking=True)

def setup_smolvla_policy(pretrained_path, language_instruction, device=torch.device("cuda")):
    """Setup and load SmolVLA policy model"""

    if device.type == 'cpu':
        log_model.warning("Warning: Using CPU for inference, this may be slow.")
        time.sleep(3)

    log_model.info("ğŸ¤– Loading SmolVLA Policy (Enhanced)...")
    log_model.info(f"ğŸ“ Task Instruction: {language_instruction}")

    policy = SmolVLAPolicyWrapper.from_pretrained(Path(pretrained_path))

    policy.eval()
    policy.to(device)
    policy.reset()

    log_model.info(f"âœ… Model loaded from {pretrained_path}")
    log_model.info(f"ğŸ“‹ Model n_obs_steps: {policy.config.n_obs_steps}")
    log_model.info(f"ğŸ–¥ï¸  Model device: {device}")
    log_model.info(f"ğŸ”§ Policy type: SmolVLA Sequential (Enhanced)")
    log_model.info(f"ğŸ“Š VLM: {policy.config.vlm_model_name}")
    log_model.info(f"ğŸ“Š Action dim: {policy.config.max_action_dim} (Kuavo uses first 16)")
    log_model.info(f"ğŸ“Š Chunk size: {policy.config.chunk_size}")
    log_model.info(f"ğŸ“Š Action steps: {policy.config.n_action_steps}")

    return policy

def main(config_path: str, env: gym.Env):
    """SmolVLA enhanced inference loop"""

    # Load config
    cfg = load_inference_config(config_path)
    from omegaconf import OmegaConf
    full_cfg = OmegaConf.load(config_path)

    use_delta = cfg.use_delta
    eval_episodes = cfg.eval_episodes
    device = torch.device(cfg.device)

    # âœ¨ ä½¿ç”¨æ›´ç²¾ç¡®çš„language instruction
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰ç²¾ç¡®æè¿°ï¼Œä½¿ç”¨é…ç½®ä¸­çš„ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤çš„
    language_instruction = cfg.get('language_instruction_enhanced', cfg.language_instruction)

    # é»˜è®¤çš„ç²¾ç¡®instructionï¼ˆå¦‚æœé…ç½®æ–‡ä»¶æ²¡æœ‰ï¼‰
    if language_instruction == cfg.language_instruction and 'push' in language_instruction:
        language_instruction = 'Pick up the moving object from the conveyor belt, place it precisely at the first target position on the table, then pick it up again and place it precisely at the second target position'
        log_model.info("ğŸ“ Using enhanced language instruction (precise placement)")

    # Set random seed
    set_seed(cfg.seed)

    # Build model path
    pretrained_path = f"outputs/train/{cfg.task}/{cfg.method}/{cfg.timestamp}/epoch{cfg.epoch}"

    # Load SmolVLA policy
    policy = setup_smolvla_policy(pretrained_path, language_instruction, device)

    # âœ¨ åˆå§‹åŒ–Actionåå¤„ç†å™¨
    enable_postprocessing = cfg.get('enable_postprocessing', True)
    if enable_postprocessing:
        postprocessor = ActionPostProcessor(
            action_dim=16,
            enable_smoothing=cfg.get('enable_smoothing', True),
            enable_fine_gain=cfg.get('enable_fine_gain', True),
            enable_workspace_limit=cfg.get('enable_workspace_limit', True),
            enable_velocity_limit=cfg.get('enable_velocity_limit', True),
            smooth_alpha=cfg.get('smooth_alpha', 0.3),
            fine_motion_gain=cfg.get('fine_motion_gain', 1.5),
            max_velocity=cfg.get('max_velocity', 0.2),
            control_frequency=cfg.get('control_frequency', 10.0)
        )
        log_model.info("âœ¨ Action Postprocessing Enabled:")
        log_model.info(f"   - Smoothing: {cfg.get('enable_smoothing', True)} (alpha={cfg.get('smooth_alpha', 0.3)})")
        log_model.info(f"   - Fine Gain: {cfg.get('enable_fine_gain', True)} (gain={cfg.get('fine_motion_gain', 1.5)}x)")
        log_model.info(f"   - Workspace Limit: {cfg.get('enable_workspace_limit', True)}")
        log_model.info(f"   - Velocity Limit: {cfg.get('enable_velocity_limit', True)} (max={cfg.get('max_velocity', 0.2)} rad/s)")
    else:
        postprocessor = None
        log_model.info("âš ï¸  Action Postprocessing Disabled")

    # Inference loop
    results = []
    for episode in range(eval_episodes):

        log_model.info(f"ğŸ¯ Episode {episode + 1}/{eval_episodes}")

        # Reset environment and policy
        obs, info = env.reset()
        policy.reset()

        # âœ¨ Reset postprocessor
        if postprocessor:
            postprocessor.reset()

        episode_reward = 0
        episode_length = 0
        success = False

        # Track inference times and action stats
        inference_times = []
        raw_action_magnitudes = []
        processed_action_magnitudes = []

        # è·å–åˆå§‹çŠ¶æ€
        current_state = obs.get('state', np.zeros(16))

        while True:
            # Check control signals
            if stop_flag.is_set():
                log_model.info("ğŸ›‘ Stop signal received, terminating...")
                break

            while pause_flag.is_set():
                log_model.info("â¸ï¸  Paused, waiting for resume...")
                time.sleep(0.1)

            # Preprocess observations
            observation = {}

            # Process image observations (resize to 512x512 for SmolVLA)
            for key in obs.keys():
                if 'image' in key.lower() or 'cam' in key.lower():
                    observation[f"observation.{key}"] = img_preprocess_smolvla(
                        obs[key], target_size=(512, 512), device=device
                    )
                elif 'depth' in key.lower():
                    observation[f"observation.{key}"] = depth_preprocess(
                        obs[key], device, cfg.depth_range
                    )
                elif 'state' in key.lower():
                    observation[f"observation.{key}"] = torch.tensor(
                        obs[key], dtype=torch.float32
                    ).unsqueeze(0).to(device)
                    current_state = obs[key]  # ä¿å­˜å½“å‰çŠ¶æ€ç”¨äºåå¤„ç†

            # Add language instruction to batch
            observation['task'] = [language_instruction]

            # SmolVLA inference
            start_time = time.time()
            with torch.no_grad():
                action_chunk = policy.select_action(observation)
                action = action_chunk[:, 0, :]
                raw_action = action.squeeze(0).cpu().numpy()[:16]

            inference_time = (time.time() - start_time) * 1000
            inference_times.append(inference_time)

            # âœ¨ Actionåå¤„ç†
            if postprocessor:
                processed_action = postprocessor.process(raw_action, current_state)

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                raw_magnitude = np.linalg.norm(raw_action[:14])
                processed_magnitude = np.linalg.norm(processed_action[:14])
                raw_action_magnitudes.append(raw_magnitude)
                processed_action_magnitudes.append(processed_magnitude)

                final_action = processed_action
            else:
                final_action = raw_action

            # Log inference time and action stats every 100 steps
            if episode_length % 100 == 0:
                avg_time = np.mean(inference_times[-100:]) if len(inference_times) >= 100 else np.mean(inference_times)
                log_model.info(f"Step {episode_length}: Avg inference time: {avg_time:.2f}ms")

                if postprocessor and len(raw_action_magnitudes) > 0:
                    avg_raw = np.mean(raw_action_magnitudes[-100:]) if len(raw_action_magnitudes) >= 100 else np.mean(raw_action_magnitudes)
                    avg_processed = np.mean(processed_action_magnitudes[-100:]) if len(processed_action_magnitudes) >= 100 else np.mean(processed_action_magnitudes)
                    gain = avg_processed / avg_raw if avg_raw > 1e-6 else 1.0
                    log_model.info(f"   Raw action: {avg_raw:.4f}, Processed: {avg_processed:.4f}, Gain: {gain:.2f}x")

            # Execute action
            obs, reward, terminated, truncated, info = env.step(final_action)

            episode_reward += reward
            episode_length += 1

            # Check episode end
            if terminated or truncated:
                success = info.get('is_success', False)
                break

        # Record episode results
        avg_inference_time = np.mean(inference_times)

        result = {
            'episode': episode,
            'reward': episode_reward,
            'length': episode_length,
            'success': success,
            'avg_inference_time_ms': avg_inference_time
        }

        if postprocessor and len(raw_action_magnitudes) > 0:
            result['avg_raw_action'] = np.mean(raw_action_magnitudes)
            result['avg_processed_action'] = np.mean(processed_action_magnitudes)
            result['avg_gain'] = result['avg_processed_action'] / result['avg_raw_action'] if result['avg_raw_action'] > 1e-6 else 1.0

        results.append(result)

        # Log episode statistics
        log_model.info(f"ğŸ“ˆ Episode {episode + 1} - Reward: {episode_reward:.3f}, Length: {episode_length}, Success: {success}")
        log_model.info(f"â±ï¸  Average inference time: {avg_inference_time:.2f}ms")

        if postprocessor and 'avg_gain' in result:
            log_model.info(f"ğŸ”§ Action postprocessing - Avg gain: {result['avg_gain']:.2f}x")

        if stop_flag.is_set():
            break

    # Calculate overall statistics
    if results:
        avg_reward = np.mean([r['reward'] for r in results])
        success_rate = np.mean([r['success'] for r in results])
        avg_length = np.mean([r['length'] for r in results])
        avg_inference = np.mean([r['avg_inference_time_ms'] for r in results])

        log_model.info(f"\n{'='*70}")
        log_model.info(f"ğŸ† Final Results - Episodes: {len(results)}")
        log_model.info(f"ğŸ“Š Average Reward: {avg_reward:.3f}")
        log_model.info(f"âœ… Success Rate: {success_rate:.1%}")
        log_model.info(f"ğŸ“ Average Length: {avg_length:.1f}")
        log_model.info(f"â±ï¸  Average Inference Time: {avg_inference:.2f}ms")

        if postprocessor and 'avg_gain' in results[0]:
            avg_gain = np.mean([r['avg_gain'] for r in results])
            log_model.info(f"ğŸ”§ Average Action Gain: {avg_gain:.2f}x")

        log_model.info(f"{'='*70}\n")

    return results

# Compatibility interface
def setup_policy(pretrained_path, policy_type, device=torch.device("cuda"), language_instruction=""):
    """Compatibility interface"""
    if policy_type != 'smolvla':
        raise ValueError(f"This script only supports 'smolvla' policy, got '{policy_type}'")
    return setup_smolvla_policy(pretrained_path, language_instruction, device)
