# -*- coding: utf-8 -*-
"""
åˆ†å±‚æ¶æ„éƒ¨ç½²æ¨¡å—

æ”¯æŒåˆ†å±‚äººå½¢æœºå™¨äººDiffusion Policyçš„éƒ¨ç½²å’Œæ¨ç†
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

import lerobot_patches.custom_patches

from dataclasses import dataclass, field
import hydra
import gymnasium as gym
import imageio
import numpy
import torch
from tqdm import tqdm
import datetime
import time
import numpy as np
from omegaconf import DictConfig, ListConfig, OmegaConf
from torchvision.transforms.functional import to_tensor
from std_msgs.msg import Bool
import rospy
import threading

# å¯¼å…¥åˆ†å±‚æ¶æ„æ¨¡å—
from kuavo_train.wrapper.policy.humanoid.HumanoidDiffusionPolicy import HumanoidDiffusionPolicy
from kuavo_train.wrapper.policy.diffusion.DiffusionPolicyWrapper import CustomDiffusionPolicyWrapper
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.utils.random_utils import set_seed

from configs.deploy.config_inference import load_inference_config
from kuavo_deploy.utils.logging_utils import setup_logger

log_model = setup_logger("model")
log_robot = setup_logger("robot")

def pause_callback(msg):
    if msg.data:
        pause_flag.set()
    else:
        pause_flag.clear()

def stop_callback(msg):
    if msg.data:
        stop_flag.set()

pause_sub = rospy.Subscriber('/kuavo/pause_state', Bool, pause_callback, queue_size=10)
stop_sub = rospy.Subscriber('/kuavo/stop_state', Bool, stop_callback, queue_size=10)
stop_flag = threading.Event()
pause_flag = threading.Event()

def check_control_signals():
    """æ£€æŸ¥æ§åˆ¶ä¿¡å·"""
    log_model.info(f"ğŸ” æ£€æŸ¥æ§åˆ¶ä¿¡å· - æš‚åœçŠ¶æ€: {pause_flag}, åœæ­¢çŠ¶æ€: {stop_flag}")

def img_preprocess(image, device="cpu"):
    return to_tensor(image).unsqueeze(0).to(device, non_blocking=True)

def depth_preprocess(depth, device="cpu", depth_range=[0, 1000]):
    """é¢„å¤„ç†æ·±åº¦å›¾åƒ"""
    depth = np.array(depth)
    depth = np.clip(depth, depth_range[0], depth_range[1])
    depth = (depth - depth_range[0]) / (depth_range[1] - depth_range[0])
    return torch.tensor(depth, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device, non_blocking=True)

def setup_hierarchical_policy(pretrained_path, policy_type, device=torch.device("cuda")):
    """
    è®¾ç½®å¹¶åŠ è½½åˆ†å±‚æ¶æ„ç­–ç•¥æ¨¡å‹

    Args:
        pretrained_path: æ£€æŸ¥ç‚¹è·¯å¾„
        policy_type: ç­–ç•¥ç±»å‹ ('diffusion', 'act', 'hierarchical_diffusion')
        device: è®¾å¤‡

    Returns:
        åŠ è½½çš„ç­–ç•¥æ¨¡å‹
    """

    if device.type == 'cpu':
        log_model.warning("Warning: Using CPU for inference, this may be slow.")
        time.sleep(3)

    if policy_type == 'hierarchical_diffusion':
        # åŠ è½½åˆ†å±‚æ¶æ„æ¨¡å‹
        log_model.info("ğŸ¤– Loading Hierarchical Diffusion Policy...")
        policy = HumanoidDiffusionPolicy.from_pretrained(Path(pretrained_path), strict=True)

        # æ‰“å°åˆ†å±‚æ¶æ„ä¿¡æ¯
        if hasattr(policy, 'print_architecture_summary'):
            policy.print_architecture_summary()

        # æ‰“å°å±‚æ€§èƒ½ç»Ÿè®¡
        if hasattr(policy, 'get_performance_stats'):
            stats = policy.get_performance_stats()
            log_model.info(f"ğŸ“Š Layer performance stats: {stats}")

    elif policy_type == 'diffusion':
        # ä¼ ç»Ÿdiffusionæ¨¡å‹
        log_model.info("ğŸ“ Loading Traditional Diffusion Policy...")
        policy = CustomDiffusionPolicyWrapper.from_pretrained(Path(pretrained_path), strict=True)

    elif policy_type == 'act':
        # ACTæ¨¡å‹
        log_model.info("ğŸ­ Loading ACT Policy...")
        policy = ACTPolicy.from_pretrained(Path(pretrained_path), strict=True)
    else:
        raise ValueError(f"Unsupported policy type: {policy_type}. Supported: 'diffusion', 'act', 'hierarchical_diffusion'")

    policy.eval()
    policy.to(device)
    policy.reset()

    # Log model info
    log_model.info(f"âœ… Model loaded from {pretrained_path}")
    log_model.info(f"ğŸ“‹ Model n_obs_steps: {policy.config.n_obs_steps}")
    log_model.info(f"ğŸ–¥ï¸  Model device: {device}")
    log_model.info(f"ğŸ”§ Policy type: {policy_type}")

    return policy

def hierarchical_inference_step(policy, observation, task_info=None):
    """
    åˆ†å±‚æ¶æ„æ¨ç†æ­¥éª¤

    Args:
        policy: åˆ†å±‚ç­–ç•¥æ¨¡å‹
        observation: è§‚æµ‹æ•°æ®
        task_info: ä»»åŠ¡ä¿¡æ¯ï¼ˆç”¨äºå±‚é€‰æ‹©ï¼‰

    Returns:
        action: åŠ¨ä½œè¾“å‡º
        hierarchical_info: åˆ†å±‚æ¶æ„ç‰¹æœ‰ä¿¡æ¯
    """

    if hasattr(policy, 'scheduler') and policy.scheduler:
        # åˆ†å±‚æ¶æ„æ¨ç†
        with torch.no_grad():
            # æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
            if task_info is None:
                task_info = {
                    'task_complexity': 'medium',  # é»˜è®¤ä¸­ç­‰å¤æ‚åº¦
                    'requires_locomotion': False,  # ä»…æ‰‹è‡‚ä»»åŠ¡
                    'requires_manipulation': True,
                    'safety_priority': True
                }

            # åˆ†å±‚æ¨ç†
            outputs = policy.scheduler(observation, task_info)

            # æå–æœ€ç»ˆåŠ¨ä½œ
            if 'final_action' in outputs:
                action = outputs['final_action']
            else:
                # å¦‚æœæ²¡æœ‰final_actionï¼Œä½¿ç”¨æœ€é«˜ä¼˜å…ˆçº§çš„å±‚è¾“å‡º
                for layer_name in ['safety', 'gait', 'manipulation', 'planning']:
                    if layer_name in outputs and 'action' in outputs[layer_name]:
                        action = outputs[layer_name]['action']
                        break
                else:
                    raise RuntimeError("No valid action output from hierarchical layers")

            # æ„å»ºåˆ†å±‚ä¿¡æ¯
            hierarchical_info = {
                'active_layers': list(outputs.keys()),
                'layer_outputs': {k: v for k, v in outputs.items() if k != 'final_action'},
                'inference_time': outputs.get('_inference_stats', {}).get('total_time', 0),
                'within_budget': outputs.get('_inference_stats', {}).get('within_budget', True)
            }

            return action, hierarchical_info
    else:
        # ä¼ ç»Ÿæ¨ç†æ¨¡å¼
        with torch.no_grad():
            action = policy.select_action(observation)
            hierarchical_info = {
                'mode': 'traditional',
                'active_layers': ['main'],
                'layer_outputs': {},
                'inference_time': 0,
                'within_budget': True
            }
            return action, hierarchical_info

def log_hierarchical_performance(hierarchical_info, step):
    """è®°å½•åˆ†å±‚æ¶æ„æ€§èƒ½ä¿¡æ¯"""

    if hierarchical_info['mode'] == 'hierarchical':
        active_layers = hierarchical_info.get('active_layers', [])
        inference_time = hierarchical_info.get('inference_time', 0)
        within_budget = hierarchical_info.get('within_budget', True)

        budget_status = "âœ…" if within_budget else "âŒ"
        log_model.info(f"Step {step}: {budget_status} Layers: {active_layers}, Time: {inference_time:.2f}ms")

        # æ¯100æ­¥è®°å½•è¯¦ç»†ç»Ÿè®¡
        if step % 100 == 0:
            layer_outputs = hierarchical_info.get('layer_outputs', {})
            for layer_name, layer_output in layer_outputs.items():
                log_model.info(f"  ğŸ“Š {layer_name}: {layer_output.get('execution_time', 0):.2f}ms")

def main(config_path: str, env: gym.Env):
    """åˆ†å±‚æ¶æ„ä¸»æ¨ç†å¾ªç¯"""

    # åŠ è½½é…ç½®
    cfg = load_inference_config(config_path)

    use_delta = cfg.use_delta
    eval_episodes = cfg.eval_episodes
    device = torch.device(cfg.device)

    # è®¾ç½®éšæœºç§å­
    set_seed(cfg.seed)

    # æ„å»ºæ¨¡å‹è·¯å¾„
    pretrained_path = f"outputs/train/{cfg.task}/{cfg.method}/{cfg.timestamp}/epoch{cfg.epoch}"

    # åŠ è½½åˆ†å±‚ç­–ç•¥
    policy = setup_hierarchical_policy(pretrained_path, cfg.policy_type, device)

    # æ£€æŸ¥æ¨¡å‹ç±»å‹
    is_hierarchical = cfg.policy_type == 'hierarchical_diffusion'
    log_model.info(f"ğŸ”„ Hierarchical mode: {is_hierarchical}")

    # æ¨ç†å¾ªç¯
    results = []
    for episode in range(eval_episodes):

        log_model.info(f"ğŸ¯ Episode {episode + 1}/{eval_episodes}")

        # é‡ç½®ç¯å¢ƒå’Œç­–ç•¥
        obs, info = env.reset()
        policy.reset()

        episode_reward = 0
        episode_length = 0
        success = False
        hierarchical_stats = {
            'total_inference_time': 0,
            'layer_activations': {},
            'budget_violations': 0
        }

        while True:
            # æ£€æŸ¥æ§åˆ¶ä¿¡å·
            if stop_flag.is_set():
                log_model.info("ğŸ›‘ Stop signal received, terminating...")
                break

            while pause_flag.is_set():
                log_model.info("â¸ï¸  Paused, waiting for resume...")
                time.sleep(0.1)

            # é¢„å¤„ç†è§‚æµ‹
            observation = {}

            # å¤„ç†å›¾åƒè§‚æµ‹
            for key in obs.keys():
                if 'image' in key.lower() or 'cam' in key.lower():
                    observation[f"observation.{key}"] = img_preprocess(obs[key], device)
                elif 'depth' in key.lower():
                    observation[f"observation.{key}"] = depth_preprocess(obs[key], device, cfg.depth_range)
                elif 'state' in key.lower():
                    observation[f"observation.{key}"] = torch.tensor(obs[key], dtype=torch.float32).unsqueeze(0).to(device)

            # æ„å»ºä»»åŠ¡ä¿¡æ¯ï¼ˆé’ˆå¯¹åˆ†å±‚æ¶æ„ï¼‰
            task_info = {
                'task_complexity': 'medium',
                'requires_locomotion': False,  # ä»…æ‰‹è‡‚æ“ä½œ
                'requires_manipulation': True,
                'safety_priority': True,
                'episode': episode,
                'step': episode_length
            }

            # åˆ†å±‚æ¨ç†
            if is_hierarchical:
                action, hierarchical_info = hierarchical_inference_step(policy, observation, task_info)

                # æ›´æ–°ç»Ÿè®¡
                hierarchical_stats['total_inference_time'] += hierarchical_info.get('inference_time', 0)
                if not hierarchical_info.get('within_budget', True):
                    hierarchical_stats['budget_violations'] += 1

                # ç»Ÿè®¡å±‚æ¿€æ´»
                for layer in hierarchical_info.get('active_layers', []):
                    hierarchical_stats['layer_activations'][layer] = hierarchical_stats['layer_activations'].get(layer, 0) + 1

                # è®°å½•æ€§èƒ½
                log_hierarchical_performance(hierarchical_info, episode_length)
            else:
                # ä¼ ç»Ÿæ¨ç†
                action = policy.select_action(observation)
                hierarchical_info = {'mode': 'traditional'}

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action.cpu().numpy())

            episode_reward += reward
            episode_length += 1

            # æ£€æŸ¥å›åˆç»“æŸ
            if terminated or truncated:
                success = info.get('is_success', False)
                break

        # è®°å½•å›åˆç»“æœ
        results.append({
            'episode': episode,
            'reward': episode_reward,
            'length': episode_length,
            'success': success,
            'hierarchical_stats': hierarchical_stats.copy()
        })

        # è®°å½•å›åˆç»Ÿè®¡
        log_model.info(f"ğŸ“ˆ Episode {episode + 1} - Reward: {episode_reward:.3f}, Length: {episode_length}, Success: {success}")

        if is_hierarchical:
            avg_inference_time = hierarchical_stats['total_inference_time'] / max(episode_length, 1)
            log_model.info(f"â±ï¸  Avg inference time: {avg_inference_time:.2f}ms")
            log_model.info(f"ğŸ¯ Layer activations: {hierarchical_stats['layer_activations']}")
            log_model.info(f"âš ï¸  Budget violations: {hierarchical_stats['budget_violations']}")

        if stop_flag.is_set():
            break

    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    if results:
        avg_reward = np.mean([r['reward'] for r in results])
        success_rate = np.mean([r['success'] for r in results])
        avg_length = np.mean([r['length'] for r in results])

        log_model.info(f"ğŸ† Final Results - Episodes: {len(results)}")
        log_model.info(f"ğŸ“Š Average Reward: {avg_reward:.3f}")
        log_model.info(f"âœ… Success Rate: {success_rate:.1%}")
        log_model.info(f"ğŸ“ Average Length: {avg_length:.1f}")

        if is_hierarchical:
            # åˆ†å±‚æ¶æ„ç‰¹æœ‰ç»Ÿè®¡
            total_budget_violations = sum(r['hierarchical_stats']['budget_violations'] for r in results)
            total_steps = sum(r['length'] for r in results)
            budget_violation_rate = total_budget_violations / max(total_steps, 1)

            log_model.info(f"âš¡ Hierarchical Performance:")
            log_model.info(f"   Budget violation rate: {budget_violation_rate:.1%}")

            # å±‚æ¿€æ´»ç»Ÿè®¡
            all_activations = {}
            for r in results:
                for layer, count in r['hierarchical_stats']['layer_activations'].items():
                    all_activations[layer] = all_activations.get(layer, 0) + count

            log_model.info(f"   Layer usage: {all_activations}")

    return results

# ä¸ºäº†ä¿æŒä¸åŸæœ‰æ¥å£çš„å…¼å®¹æ€§ï¼Œæä¾›setup_policyåˆ«å
def setup_policy(pretrained_path, policy_type, device=torch.device("cuda")):
    """å…¼å®¹æ€§æ¥å£"""
    return setup_hierarchical_policy(pretrained_path, policy_type, device)