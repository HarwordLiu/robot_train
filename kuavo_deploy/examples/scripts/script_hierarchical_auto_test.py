# -*- coding: utf-8 -*-
"""
åˆ†å±‚æ¶æ„è‡ªåŠ¨æµ‹è¯•è„šæœ¬

åŸºäºåˆ†å±‚æ¶æ„çš„ä»¿çœŸè‡ªåŠ¨æµ‹è¯•ï¼Œæ”¯æŒå¤šå›åˆè¯„ä¼°å’Œæ€§èƒ½åˆ†æ

ä½¿ç”¨ç¤ºä¾‹:
  python script_hierarchical_auto_test.py --task auto_test --config /path/to/hierarchical_config.yaml
"""

import rospy
import time
import argparse
from pathlib import Path
import gymnasium as gym
import numpy as np
import signal
import sys, os
import threading
import traceback
import json
from datetime import datetime

from kuavo_deploy.utils.logging_utils import setup_logger
from configs.deploy.config_inference import load_inference_config
from configs.deploy.config_kuavo_env import load_kuavo_env_config

# ä½¿ç”¨åˆ†å±‚æ¶æ„çš„è¯„ä¼°æ¨¡å—
from kuavo_deploy.examples.eval.eval_kuavo_hierarchical import main as hierarchical_main

# é…ç½®æ—¥å¿—
log_model = setup_logger("hierarchical_auto_test", "DEBUG")
log_robot = setup_logger("hierarchical_robot", "DEBUG")

class HierarchicalAutoTestController:
    """åˆ†å±‚æ¶æ„è‡ªåŠ¨æµ‹è¯•æ§åˆ¶å™¨"""

    def __init__(self):
        self.paused = False
        self.should_stop = False
        self.lock = threading.Lock()
        self.test_results = []
        self.current_episode = 0
        self.start_time = None

    def pause(self):
        with self.lock:
            self.paused = True
            log_model.info("â¸ï¸  Auto test paused")

    def resume(self):
        with self.lock:
            self.paused = False
            log_model.info("â–¶ï¸  Auto test resumed")

    def stop(self):
        with self.lock:
            self.should_stop = True
            log_model.info("ğŸ›‘ Auto test stopping")

    def is_paused(self):
        with self.lock:
            return self.paused

    def should_terminate(self):
        with self.lock:
            return self.should_stop

    def add_result(self, result):
        with self.lock:
            self.test_results.append(result)

    def get_results(self):
        with self.lock:
            return self.test_results.copy()

    def set_current_episode(self, episode):
        with self.lock:
            self.current_episode = episode

    def get_current_episode(self):
        with self.lock:
            return self.current_episode

    def start_timer(self):
        with self.lock:
            self.start_time = time.time()

    def get_elapsed_time(self):
        with self.lock:
            if self.start_time:
                return time.time() - self.start_time
            return 0

# å…¨å±€æ§åˆ¶å™¨
auto_test_controller = HierarchicalAutoTestController()

def signal_handler_pause(signum, frame):
    """æš‚åœ/æ¢å¤ä¿¡å·å¤„ç†"""
    if auto_test_controller.is_paused():
        auto_test_controller.resume()
    else:
        auto_test_controller.pause()

def signal_handler_stop(signum, frame):
    """åœæ­¢ä¿¡å·å¤„ç†"""
    auto_test_controller.stop()

# è®¾ç½®ä¿¡å·å¤„ç†
signal.signal(signal.SIGUSR1, signal_handler_pause)  # æš‚åœ/æ¢å¤
signal.signal(signal.SIGUSR2, signal_handler_stop)   # åœæ­¢

def analyze_hierarchical_performance(results):
    """åˆ†æåˆ†å±‚æ¶æ„æ€§èƒ½"""

    if not results:
        log_model.warning("âš ï¸  No results to analyze")
        return

    total_episodes = len(results)
    successful_episodes = sum(1 for r in results if r.get('success', False))
    success_rate = successful_episodes / total_episodes

    rewards = [r.get('reward', 0) for r in results]
    lengths = [r.get('length', 0) for r in results]

    avg_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    avg_length = np.mean(lengths)
    std_length = np.std(lengths)

    log_model.info("ğŸ† === Hierarchical Performance Analysis ===")
    log_model.info(f"ğŸ“Š Total Episodes: {total_episodes}")
    log_model.info(f"âœ… Success Rate: {success_rate:.1%} ({successful_episodes}/{total_episodes})")
    log_model.info(f"ğŸ¯ Average Reward: {avg_reward:.3f} Â± {std_reward:.3f}")
    log_model.info(f"ğŸ“ Average Length: {avg_length:.1f} Â± {std_length:.1f}")

    # åˆ†å±‚æ¶æ„ç‰¹æœ‰åˆ†æ
    hierarchical_stats = {
        'total_inference_time': 0,
        'total_budget_violations': 0,
        'total_steps': 0,
        'layer_activations': {},
        'layer_performance': {}
    }

    for result in results:
        h_stats = result.get('hierarchical_stats', {})
        hierarchical_stats['total_inference_time'] += h_stats.get('total_inference_time', 0)
        hierarchical_stats['total_budget_violations'] += h_stats.get('budget_violations', 0)
        hierarchical_stats['total_steps'] += result.get('length', 0)

        # ç´¯ç§¯å±‚æ¿€æ´»ç»Ÿè®¡
        layer_activations = h_stats.get('layer_activations', {})
        for layer, count in layer_activations.items():
            hierarchical_stats['layer_activations'][layer] = hierarchical_stats['layer_activations'].get(layer, 0) + count

    # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
    if hierarchical_stats['total_steps'] > 0:
        avg_inference_time = hierarchical_stats['total_inference_time'] / hierarchical_stats['total_steps']
        budget_violation_rate = hierarchical_stats['total_budget_violations'] / hierarchical_stats['total_steps']

        log_model.info("âš¡ === Hierarchical Architecture Analysis ===")
        log_model.info(f"â±ï¸  Average Inference Time: {avg_inference_time:.2f}ms")
        log_model.info(f"âš ï¸  Budget Violation Rate: {budget_violation_rate:.1%}")
        log_model.info(f"ğŸ—ï¸  Layer Usage Statistics:")

        total_activations = sum(hierarchical_stats['layer_activations'].values())
        for layer, count in hierarchical_stats['layer_activations'].items():
            usage_rate = count / total_activations if total_activations > 0 else 0
            log_model.info(f"   ğŸ“Š {layer}: {count} activations ({usage_rate:.1%})")

    return {
        'total_episodes': total_episodes,
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_length': avg_length,
        'hierarchical_stats': hierarchical_stats
    }

def save_test_results(results, analysis, config_path, output_dir="outputs/hierarchical_test_results"):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""

    try:
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"hierarchical_test_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)

        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'timestamp': timestamp,
            'config_path': config_path,
            'results': results,
            'analysis': analysis,
            'metadata': {
                'total_episodes': len(results),
                'test_duration': auto_test_controller.get_elapsed_time(),
                'hierarchical_architecture': True
            }
        }

        with open(filepath, 'w') as f:
            json.dump(save_data, f, indent=2, default=str)

        log_model.info(f"ğŸ’¾ Test results saved to: {filepath}")

        # åˆ›å»ºç®€åŒ–çš„æ‘˜è¦æ–‡ä»¶
        summary_filename = f"hierarchical_summary_{timestamp}.txt"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, 'w') as f:
            f.write("ğŸ¤– Hierarchical Architecture Test Summary\n")
            f.write("=" * 50 + "\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write(f"Config: {config_path}\n")
            f.write(f"Total Episodes: {analysis['total_episodes']}\n")
            f.write(f"Success Rate: {analysis['success_rate']:.1%}\n")
            f.write(f"Average Reward: {analysis['avg_reward']:.3f}\n")
            f.write(f"Average Length: {analysis['avg_length']:.1f}\n\n")

            f.write("ğŸ—ï¸  Hierarchical Performance:\n")
            h_stats = analysis['hierarchical_stats']
            if h_stats['total_steps'] > 0:
                avg_time = h_stats['total_inference_time'] / h_stats['total_steps']
                violation_rate = h_stats['total_budget_violations'] / h_stats['total_steps']
                f.write(f"Average Inference Time: {avg_time:.2f}ms\n")
                f.write(f"Budget Violation Rate: {violation_rate:.1%}\n")

            f.write("\nğŸ“Š Layer Usage:\n")
            total_activations = sum(h_stats['layer_activations'].values())
            for layer, count in h_stats['layer_activations'].items():
                usage_rate = count / total_activations if total_activations > 0 else 0
                f.write(f"{layer}: {count} ({usage_rate:.1%})\n")

        log_model.info(f"ğŸ“‹ Test summary saved to: {summary_filepath}")

    except Exception as e:
        log_model.error(f"âŒ Failed to save test results: {e}")

def progress_monitor():
    """è¿›åº¦ç›‘æ§çº¿ç¨‹"""

    while not auto_test_controller.should_terminate():
        time.sleep(10)  # æ¯10ç§’æ›´æ–°ä¸€æ¬¡

        if not auto_test_controller.is_paused():
            current_episode = auto_test_controller.get_current_episode()
            elapsed_time = auto_test_controller.get_elapsed_time()
            results = auto_test_controller.get_results()

            if results:
                recent_success_rate = np.mean([r.get('success', False) for r in results[-10:]])
                log_model.info(f"ğŸ“ˆ Progress - Episode: {current_episode}, "
                             f"Recent success rate: {recent_success_rate:.1%}, "
                             f"Elapsed: {elapsed_time:.1f}s")

def task_auto_test(config_path: str):
    """åˆ†å±‚æ¶æ„è‡ªåŠ¨æµ‹è¯•ä»»åŠ¡"""

    log_model.info("ğŸ§ª Hierarchical Task: AUTO_TEST - Starting automated testing")

    try:
        # åŠ è½½é…ç½®
        cfg = load_kuavo_env_config(config_path)
        inference_cfg = load_inference_config(config_path)

        log_model.info(f"ğŸ¯ Test Episodes: {inference_cfg.eval_episodes}")
        log_model.info(f"ğŸ¤– Policy Type: {inference_cfg.policy_type}")
        log_model.info(f"ğŸŒ Environment: {cfg.env_name}")

        # å¯åŠ¨è¿›åº¦ç›‘æ§
        auto_test_controller.start_timer()
        monitor_thread = threading.Thread(target=progress_monitor, daemon=True)
        monitor_thread.start()

        # åˆ›å»ºç¯å¢ƒ
        env = gym.make(cfg.env_name, config_path=config_path, render_mode="rgb_array")
        log_robot.info(f"âœ… Environment created: {cfg.env_name}")

        # è¿è¡Œåˆ†å±‚æ¶æ„æµ‹è¯•
        log_model.info("ğŸš€ Starting hierarchical inference testing...")
        results = hierarchical_main(config_path, env)

        env.close()

        if results:
            # åˆ†æç»“æœ
            analysis = analyze_hierarchical_performance(results)

            # ä¿å­˜ç»“æœ
            save_test_results(results, analysis, config_path)

            log_model.info("âœ… Hierarchical auto test completed successfully")
            return analysis
        else:
            log_model.error("âŒ No test results generated")
            return None

    except Exception as e:
        log_model.error(f"âŒ Auto test failed: {e}")
        traceback.print_exc()
        return None

def main():
    """åˆ†å±‚æ¶æ„è‡ªåŠ¨æµ‹è¯•ä¸»å‡½æ•°"""

    parser = argparse.ArgumentParser(description="Hierarchical Architecture Auto Test Script")
    parser.add_argument("--task", type=str, required=True,
                       choices=["auto_test"],
                       help="Task to execute (currently only auto_test)")
    parser.add_argument("--config", type=str, required=True,
                       help="Path to hierarchical configuration file")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose output")
    parser.add_argument("--save_results", action="store_true", default=True,
                       help="Save test results to file")

    args = parser.parse_args()

    if not os.path.exists(args.config):
        log_model.error(f"âŒ Config file not found: {args.config}")
        return

    # åˆå§‹åŒ–ROS
    rospy.init_node('hierarchical_kuavo_auto_test', anonymous=True)

    log_model.info("ğŸ§ª Hierarchical Kuavo Auto Test Starting...")
    log_model.info(f"ğŸ“ Task: {args.task}")
    log_model.info(f"âš™ï¸  Config: {args.config}")
    log_model.info(f"ğŸ“¢ Verbose: {args.verbose}")

    # è®¾ç½®è¯¦ç»†æ—¥å¿—
    if args.verbose:
        log_model.setLevel("DEBUG")
        log_robot.setLevel("DEBUG")

    try:
        if args.task == "auto_test":
            analysis = task_auto_test(args.config)

            if analysis:
                log_model.info("ğŸ‰ Hierarchical auto test completed successfully!")
                log_model.info(f"ğŸ“Š Final Success Rate: {analysis['success_rate']:.1%}")
                log_model.info(f"ğŸ¯ Final Average Reward: {analysis['avg_reward']:.3f}")
            else:
                log_model.error("âŒ Auto test failed to produce results")

    except KeyboardInterrupt:
        log_model.info("â¹ï¸  Auto test interrupted by user")
        auto_test_controller.stop()
    except Exception as e:
        log_model.error(f"âŒ Auto test failed: {e}")
        traceback.print_exc()
    finally:
        log_model.info("ğŸ Hierarchical auto test finished")

if __name__ == "__main__":
    main()