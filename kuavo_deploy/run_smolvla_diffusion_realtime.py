"""
SmolVLA Diffusion å®æ—¶éƒ¨ç½²è„šæœ¬

åœ¨ä»¿çœŸæˆ–çœŸå®æœºå™¨äººä¸Šå®æ—¶è¿è¡Œ SmolVLA Diffusion æ¨¡å‹
"""

import os
import sys
import time
import torch
import numpy as np
import threading
import queue
from pathlib import Path
from typing import Dict, Any, Optional, List
import argparse
from collections import deque

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from kuavo_train.wrapper.policy.smolvla import SmolVLADiffusionPolicyWrapper
from kuavo_sim_env.envs.kuavo_sim_env import KuavoSimEnv


class SmolVLADiffusionRealtime:
    """
    SmolVLA Diffusion å®æ—¶è¿è¡Œå™¨
    """

    def __init__(self, config_path: str, model_path: str, device: str = "cuda"):
        """
        åˆå§‹åŒ–
        """
        self.config_path = config_path
        self.model_path = model_path
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        # åŠ è½½é…ç½®
        self.cfg = self._load_config(config_path)

        # åˆå§‹åŒ–ç»„ä»¶
        self.env = None
        self.policy = None
        self.running = False

        # æ€§èƒ½ä¼˜åŒ–
        self.action_queue = queue.Queue(maxsize=1)
        self.obs_buffer = deque(maxlen=1)
        self.inference_times = deque(maxlen=100)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_steps': 0,
            'avg_inference_time': 0.0,
            'fps': 0.0,
        }

        print(f"ğŸš€ SmolVLA Diffusion å®æ—¶éƒ¨ç½²åˆå§‹åŒ–")
        print(f"   - é…ç½®: {config_path}")
        print(f"   - æ¨¡å‹: {model_path}")
        print(f"   - è®¾å¤‡: {self.device}")

    def _load_config(self, config_path: str):
        """åŠ è½½é…ç½®"""
        from omegaconf import OmegaConf
        return OmegaConf.load(config_path)

    def initialize(self):
        """åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹"""
        print("\nğŸ“¦ åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹...")

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = KuavoSimEnv(
            host=self.cfg.env.host,
            port=self.cfg.env.port
        )
        print("âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

        # åŠ è½½æ¨¡å‹
        self.policy = SmolVLADiffusionPolicyWrapper.from_pretrained(
            pretrained_name_or_path=self.model_path,
            apply_freezing=False
        )
        self.policy.to(self.device)
        self.policy.eval()

        # é¢„çƒ­æ¨¡å‹
        self._warmup()
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å’Œé¢„çƒ­å®Œæˆ")

    def _warmup(self):
        """æ¨¡å‹é¢„çƒ­"""
        print("ğŸ”¥ æ¨¡å‹é¢„çƒ­ä¸­...")
        with torch.no_grad():
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            dummy_batch = {
                'observation.images.h': torch.randn(1, 3, 512, 512).to(self.device),
                'observation.state': torch.randn(1, 32).to(self.device),
                'task': ['Warm up task'],
            }

            # é¢„çƒ­å‡ æ¬¡
            for _ in range(3):
                _ = self.policy.select_action(dummy_batch, num_inference_steps=5)

    def prepare_observation(self, obs: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """å‡†å¤‡è§‚æµ‹æ•°æ®"""
        batch = {}

        # å¤„ç†å›¾åƒ
        for key in self.cfg.observation.images:
            if key in obs:
                img = obs[key]
                if isinstance(img, np.ndarray):
                    img = torch.from_numpy(img).float()
                    if len(img.shape) == 3:
                        img = img.permute(2, 0, 1)  # HWC -> CHW
                    img = img / 255.0
                batch[key] = img.unsqueeze(0).to(self.device)

        # å¤„ç†æ·±åº¦
        for key in self.cfg.observation.depth:
            if key in obs:
                depth = obs[key]
                if isinstance(depth, np.ndarray):
                    depth = torch.from_numpy(depth).float()
                    if len(depth.shape) == 2:
                        depth = depth.unsqueeze(0)
                batch[key] = depth.unsqueeze(0).to(self.device)

        # å¤„ç†çŠ¶æ€
        if 'observation.state' in obs:
            state = obs['observation.state']
            if isinstance(state, np.ndarray):
                state = torch.from_numpy(state).float()

            # å¡«å……åˆ°32ç»´
            if len(state) < 32:
                padding = torch.zeros(32 - len(state))
                state = torch.cat([state, padding], dim=-1)

            batch['observation.state'] = state.unsqueeze(0).to(self.device)

        # æ·»åŠ è¯­è¨€æŒ‡ä»¤
        batch['task'] = [self.cfg.task.language_instruction]

        return batch

    def inference_worker(self):
        """æ¨ç†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                # è·å–æœ€æ–°è§‚æµ‹
                if not self.obs_buffer:
                    time.sleep(0.001)
                    continue

                obs = self.obs_buffer[-1]

                # å‡†å¤‡è¾“å…¥
                batch = self.prepare_observation(obs)

                # æ¨ç†
                start_time = time.time()
                with torch.no_grad():
                    if self.cfg.optimization.use_amp:
                        with torch.autocast(device_type='cuda', dtype=torch.float16):
                            actions = self.policy.select_action(
                                batch,
                                num_inference_steps=self.cfg.policy.inference.num_inference_steps
                            )
                    else:
                        actions = self.policy.select_action(
                            batch,
                            num_inference_steps=self.cfg.policy.inference.num_inference_steps
                        )

                inference_time = time.time() - start_time
                self.inference_times.append(inference_time)

                # æå–ç¬¬ä¸€ä¸ªåŠ¨ä½œ
                action = actions[0, 0].cpu().numpy()[:16]  # è£å‰ªåˆ°16ç»´

                # æ”¾å…¥é˜Ÿåˆ—
                try:
                    self.action_queue.put(action, timeout=0.01)
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡äº†ï¼Œä¸¢å¼ƒæ—§åŠ¨ä½œ
                    try:
                        self.action_queue.get_nowait()
                        self.action_queue.put(action, timeout=0.01)
                    except:
                        pass

            except Exception as e:
                print(f"âš ï¸ æ¨ç†é”™è¯¯: {e}")
                continue

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        print("\nğŸƒ å¼€å§‹å®æ—¶è¿è¡Œ...")
        print("æŒ‰ Ctrl+C åœæ­¢")

        # é‡ç½®ç¯å¢ƒ
        obs = self.env.reset()
        self.running = True

        # å¯åŠ¨æ¨ç†çº¿ç¨‹
        inference_thread = threading.Thread(target=self.inference_worker, daemon=True)
        inference_thread.start()

        # æ§åˆ¶å¾ªç¯
        control_freq = self.cfg.task.action.control_frequency
        control_period = 1.0 / control_freq

        last_print_time = time.time()
        step_count = 0

        try:
            while self.running:
                loop_start = time.time()

                # è·å–è§‚æµ‹
                obs = self.env.get_observation()
                self.obs_buffer.append(obs)

                # è·å–åŠ¨ä½œ
                action = None
                try:
                    action = self.action_queue.get(timeout=0.01)
                except queue.Empty:
                    # æ²¡æœ‰æ–°åŠ¨ä½œï¼Œä½¿ç”¨é›¶åŠ¨ä½œæˆ–ä¿æŒä¸Šä¸€åŠ¨ä½œ
                    action = np.zeros(16)  # Kuavo 16è‡ªç”±åº¦

                # æ‰§è¡ŒåŠ¨ä½œ
                self.env.step(action)

                # ç»Ÿè®¡
                step_count += 1
                self.stats['total_steps'] = step_count

                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯5ç§’ï¼‰
                current_time = time.time()
                if current_time - last_print_time > 5.0:
                    self._print_stats(current_time - last_print_time)
                    last_print_time = current_time

                # æ§åˆ¶é¢‘ç‡
                elapsed = time.time() - loop_start
                if elapsed < control_period:
                    time.sleep(control_period - elapsed)

        except KeyboardInterrupt:
            print("\nâ¹ï¸ åœæ­¢è¿è¡Œ")
        finally:
            self.running = False

    def _print_stats(self, duration: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—FPS
        fps = self.stats['total_steps'] / duration if duration > 0 else 0

        # è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´
        if self.inference_times:
            avg_inference_time = np.mean(list(self.inference_times)) * 1000
            min_inference_time = np.min(list(self.inference_times)) * 1000
            max_inference_time = np.max(list(self.inference_times)) * 1000
        else:
            avg_inference_time = min_inference_time = max_inference_time = 0

        print(f"\nğŸ“Š è¿è¡Œç»Ÿè®¡ (æœ€è¿‘{duration:.1f}ç§’):")
        print(f"   - FPS: {fps:.1f}")
        print(f"   - æ€»æ­¥æ•°: {self.stats['total_steps']}")
        print(f"   - æ¨ç†æ—¶é—´: å¹³å‡ {avg_inference_time:.1f}ms, "
              f"æœ€å¿« {min_inference_time:.1f}ms, "
              f"æœ€æ…¢ {max_inference_time:.1f}ms")
        print(f"   - é˜Ÿåˆ—å¤§å°: {self.action_queue.qsize()}")

        # é‡ç½®ç»Ÿè®¡
        self.stats['total_steps'] = 0
        self.inference_times.clear()


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="SmolVLA Diffusion å®æ—¶éƒ¨ç½²")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/deploy/kuavo_smolvla_diffusion_sim_env.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¾å¤‡ (cuda/cpu)"
    )

    args = parser.parse_args()

    # åˆ›å»ºè¿è¡Œå™¨
    runner = SmolVLADiffusionRealtime(
        config_path=args.config,
        model_path=args.model,
        device=args.device
    )

    # åˆå§‹åŒ–
    runner.initialize()

    # è¿è¡Œ
    runner.run()


if __name__ == "__main__":
    main()